page_titles,page_links,text_results
Discrimination in the United States,/wiki/Discrimination_in_the_United_States," Discrimination comprises ""base or the basis of class or category without regard to individual merit, especially to show prejudice on the basis of ethnicity, gender, or a similar social factor"".[1] This term is used to highlight the difference in treatment between members of different groups when one group is intentionally singled out and treated worse, or not given the same opportunities. Attitudes toward minorities have been marked by discrimination in the history of the United States. Many forms of discrimination have come to be recognized in American society, particularly on the basis of national origin,[2][3][4]race and ethnicity,[3][4][5]non-English languages,[3][4]religion,[6]gender,[7][8] and sexual orientation.[5][7][8] Colorism is a form of racially-based discrimination where people are treated unequally due to skin color. It initially came about in the United States during slavery. Lighter skinned slaves tended to work indoors, while dark skinned worked outdoors. In 1865, during the Reconstruction period after the American Civil War, the Thirteenth Amendment to the United States Constitution was passed and it abolished slavery. This was soon followed by the Fourteenth Amendment to the United States Constitution that granted citizenship to all persons ""born or naturalized in the United States"", and the Fifteenth Amendment to the United States Constitution that protected the rights to vote for everyone. These Amendments passed during the Reconstruction period extended protection to the newly emancipated slaves. However, in the 1870s Jim Crow laws were introduced in the Southeastern United States. These laws promoted the idea of ""Separate but equal""[9] which was first brought about from the Plessy v. Ferguson decision in 1896, meaning that all races were equal, but they had to have separate public facilities. The mixing of races was illegal in most places such as public schools, public transportation and restaurants.[10] These laws increased discrimination and segregation in the United States. Oftentimes, the products and sections designated for the ""Colored"" were inferior and not as nice for the ""White Only"".[11] Water fountains, bathrooms, and park benches were just a few of the areas segregated by Caucasians due to Jim Crow laws. Furthermore, the Jim Crow laws systematically made life harder for African-Americans and people of color. It made voting harder to accomplish, due to the fact that African-Americans had to do literacy tests and go through other obstacles before getting the chance to vote. In the modern United States, gay black men are extremely likely to experience intersectional discrimination. In the United States, the children of gay African-American men have a poverty rate of 52 percent, the highest in the country. Gay African-American men in partnerships are also six times more likely to live in poverty than gay white male couples.[12] Some have made the controversial claim that the racist treatment of African-Americans amounts to genocide, elaborated on in Black genocide with foci on slavery, Jim Crow, and other racist institutions in the U.S. In August 2020, the U.S. Justice Department argued that Yale University discriminated against Asian candidates on the basis of their race, a charge the university denied.[13] Major figures such as Martin Luther King Jr., Malcolm X, and Rosa Parks[14] were involved in the fight against the race-based discrimination of the Civil Rights Movement. Rosa Parks's refusal to give up her bus seat in 1955 sparked the Montgomery bus boycott—a large movement in Montgomery, Alabama, that was an integral period at the beginning of the Civil Rights Movement. The Bus Boycott lasted a total of 381 days before the Supreme Court of the United States deemed that segregated seating is unconstitutional. Dr. Martin Luther King Jr., a peaceful activist and pastor, led many such protests, advocating for the improvement of African-Americans in American's society. His role in the Montgomery Bus Boycott helped to launch his role in the Civil Rights Movement. King organized many protests attended not only by African-American, but also Caucasians. While King[15] organized peaceful protests, Malcolm X went a different route. His main supporters, The Nation of Islam, and he stressed the idea of black power, and black pride. Although Malcolm X's actions were radical, especially when they contradicted that of Dr. King, but he is still considered one of the pioneers in fighting back against racial discrimination in daily life and not just from a political standpoint. His ideas of black nationalism and the use of violence to fight back helped to spark the political group in the Black Panther Party for Self-Defense, which later became known as the Black Panther Party. Formed by Bobby Seale and Huey P. Newton, the organization was created in October 1966, in Oakland, California. Usually seen in all black and armed, as a group, the Black Panthers first started off patrolling Oakland Police Department activity, but soon grew to widespread support in cities like Los Angeles, and Chicago. Although they were seen as a violent gang and a danger to society, the Black Panthers brought numerous social programs such as free breakfast for school children and free clinics across numerous cities. What the Black Panthers were fighting for was outlined in their Ten-Point Program. They were ultimately taken down by the FBI, led by J. Edgar Hoover, in the early 1970s. Other factors such as internal tensions, and financial struggles also played into the demise of the Black Panther Party and by 1982 they were completely gone.[16] In the education system, the Civil Rights Movement further became huge after the ruling of Brown v. Board of Education in 1954. Oliver Brown challenged the Board of Education of Topeka, Kanas, when his daughter was not allowed to enroll in any of the all white schools claiming that ""separate but equal"" violates the protection clause of the Fourteenth Amendment. Ruby Bridges, along with the Little Rock Nine, dealt with discrimination from Caucasian peers, their parents, and the community in general during the desegregation of schools. The Little Rock Nine were a group of nine African-American students who volunteered to attend school at the Central High School in Little Rock, Arkansas. They continuously had problems with the public and faced harsh treatment from other students, parents, and even the Little Rock National Guard. However, a change occurred when President Dwight D. Eisenhower intervened by sending federal troops to escort the students.[17] For Ruby Bridges, she joined the Civil Rights Movement in the November 14, 1960, when she became enrolled in William Frantz Elementary School. Due to many parents not allowing their children in her class, Bridges was in class by herself, which was taught by Barbara Henry, and oftentimes ate alone and had recess alone. Ruby, along with her family, did face a lot of backlash throughout Louisiana from the desegregation; however, they did receive support from numerous people in the North and Bridges was able to finish the year.[18] Gender discrimination is another form of discrimination. Women are often seen as an expense to their employers because they take days off for children, need time off for maternity leave and may be stereotyped as ""more emotional"". It is known as the glass escalator[19] or the glass ceiling, which holds that while women are being held down in male-dominated professions, men often rise quickly to positions of authority in certain fields. Men are being pushed forward into management, even surpassing women who have been at the job longer and have more experience in the field in some cases. Discrimination against men has been described in the areas of family law, such as divorce and child custody, labor such as paternity leave, paternity fraud, health, education, conscription, and other areas of the law such as domestic violence, genital integrity, and allegations of rape.[citation needed] Immigrants to the United States are affected by a totally separate type of discrimination. Some people feel as though the large numbers of people being allowed into the country are cause for alarm, therefore discriminate against them.[20] Arizona passed a law that forces people to carry documents with them at all times to prove their citizenship. Some people claim that immigrants are taking up ""Americans"" jobs. Immigration restrictions are among the biggest government interventions in the economy. They prevent millions of immigrants from taking jobs, renting homes, and pursuing a wide range of opportunities that they could otherwise have.[21] Violent hate crimes have increased[22] drastically. Recent social psychological research suggests that this form of prejudice against migrants may be partly explained by some fairly basic cognitive processes.[23][24] According to Soylu,[25] some argue that immigrants constantly face being discriminated against because of the color of their skin, the sound of their voice, the way they look and their beliefs. Many immigrants are well educated, some argue that they are often blamed and persecuted for the ills in society such as overcrowding of schools, disease and unwanted changes in the host country's culture due to the beliefs of this ""unwelcomed"" group of people.[26] According to Soylu, there was an open immigration policy up until 1924 in America until the National Origins Act came into effect.[26] According to the Immigration Act of 1924 which is a United States federal law, it limited the annual number of immigrants who could be admitted from any country to 2% of the number of people from that country who were already living in the United States in 1890, down from the 3% cap set by the Immigration Restriction Act of 1921, according to the Census of 1890 It superseded the 1921 Emergency Quota Act. The law was primarily aimed at further restricting immigration of Southern Europeans and Eastern Europeans. According to Buchanan, later in the 1930s with the advent of opinion polling, immigration policy analysis was carried out by collecting public thoughts and opinions on the issue. These factors encouraged a heated debate on immigration policy. These debates continued even into the 2000s, and were intensified by George W. Bush's immigration proposal.[27] Some argue that the 9/11 terrorist attacks left the country in a state of paranoia and fear that strengthened the views in favor of having closed borders.[26] Immigration to the United States can be difficult due to immigrants' lack of access to legal documents and the expensive nature of immigration. The United States has historically been a major target destination for people seeking work and continues to be so today.. As Graciela, a 47-year-old married woman who had lived in the U.S. for 4 years, stated, “My husband,... he lost his job. Things were beginning to get tough... We came with the need to find work and better life possibilities.”[28][29] Worldwide, the workforce has become increasingly pluralistic and ethnically diverse as more and more people migrate across nations. Although race- or ethnicity-based discriminatory employment practices are prohibited in most developed countries, according to feminist scholar Mary Harcourt, actual discrimination is still widespread.[30] Sahagian Jacqueline, an author, argues that one example of this act of discrimination occurred at Macy's a department store. According to the U.S. Justice Department, Macy's used unfair documentation practices against legal immigrant employees who had proper work authorizations. During an eligibility re-verification process, Macy's broke immigration law that prohibits employers from discriminating against immigrant employees during re-verification by asking for more or different documents than other employees are required to submit based on a worker's immigration status or national origin. Some of the affected employees lost seniority, were suspended, or even let go due to the illegal re-verification.[31] While their opinions are controversial, researchers Moran, Tyler and Daranee argue that with immigrants' growing numbers and their expanding economic role in U.S. society, addressing challenges and creating opportunities for immigrants to succeed in the labor force are critical prerequisites to improve the economic security for all low-wage working families and ensure the future vitality of our economy.[32] Another type of discrimination is that against lesbian, gay, bisexual, transgender, and queer (LGBTQ+) individuals. For personal reasons such as religious beliefs, employers sometimes choose to not hire LGBTQ+ people. Late in 1979, a new religious revival among conservative Evangelical Protestants and Roman Catholics ushered in the Republican coalition politically aligned with the Christian right that would reign in the United States between the years 1970s and 1980s,[33][34][35][36] becoming another obstacle for the progress of the LGBTQ+ rights movement.[35] During the HIV/AIDS epidemic of the 1980s, LGBTQ+ communities were further stigmatized as they became the focus of mass hysteria, suffered isolation and marginalization, and were targeted with extreme acts of violence.[37] LGBTQ+ people and their rights have been discriminated against for various reasons; for example, one topic of controversy related to LGBTQ+ people is same-sex marriage, which was legalized in all the fifty states in June 2015 following the Supreme Court case Obergefell v. Hodges. On 15 June 2020, the Supreme Court ruled in Bostock v. Clayton County and two other cases, that workplace discrimination based on sexual orientation or gender identity is covered by Title VII of the Civil Rights Act of 1964.[38]"
Age discrimination in the United States,/wiki/Age_discrimination_in_the_United_States,"In the United States, all states have passed laws that restrict age discrimination,[1] and age discrimination is restricted under federal laws such as the Age Discrimination in Employment Act of 1967 (ADEA).[2] However, it is worthy of note that age discrimination is still an issue in employment as of 2019.[3] The Equal Credit Opportunity Act (ECOA) is a United States law (codified at 15 U.S.C. § 1691 et seq.), enacted 28 October 1974,[4] that makes it unlawful for any creditor to discriminate against any applicant, with respect to any aspect of a credit transaction, on the basis of (among other things) age, provided the applicant has the capacity to contract.[5] Some U.S. political offices have qualifications that discriminate on the basis of age. For example, pursuant to the Constitution of the United States the President of the United States must be at least 35 years old; a United States senator must be at least 30; and a member of the United States House of Representatives must be at least age 25. The Age Discrimination in Employment Act of 1967 (ADEA) (29 U.S.C. § 621 to 29 U.S.C. § 634) is a federal law that provides certain employment protections to workers who are over the age of forty, who work for an employer who has twenty or more employees. For protected workers, the ADEA prohibits discrimination at all levels of employment, from recruitment and hiring, through the employment relationship, and through decisions for layoffs or termination of the employment relationship.[6] An age limit may only be legally specified for protected workers in the circumstance where age has been shown to be a ""bona fide occupational qualification [BFOQ] reasonably necessary to the normal operation of the particular business"" (see 29 U.S.C. § 623(f)(1)). In practice, BFOQs for age are limited to the obvious (hiring a young actor to play a young character in a movie), when a job is physically demanding (police, firefighters, military service), or when public safety is a concern (for example, in the case of age limits for pilots, truck drivers, and bus drivers). Some states like New York and New Jersey including District of Columbia have laws that protect younger workers from reverse age discrimination, a practice not prohibited under the ADEA.[7] In these jurisdictions, employers are legally prohibited from discriminating against workers 18 and older for their age unless a bona fide occupational qualification exists (i.e., employers may require bartenders to be at least 21 to comply with the legal drinking age).[8][9][10] In 1968, the EEOC declared age restrictions on flight attendants' employment to be illegal sex discrimination under Title VII of the Civil Rights Act of 1964.[11] Mandatory retirement due to age is generally unlawful in the United States, except in certain industries and occupations that are regulated by law, and are often part of the government (such as military service and federal police agencies, such as the Federal Bureau of Investigation). Minnesota has statutorily established mandatory retirement for all judges at age 70 (more precisely, at the end of the month a judge reaches that age). The Minnesota Legislature has had the constitutional right to set judicial retirement ages since 1956, but did not do so until 1973, setting the age at 70.[12] The Federal Age Discrimination in Employment Act, which became law in 1986, ended mandatory age-related retirement at age 70 for many jobs, not including the Minnesota judiciary;[12] another exception was all postsecondary institutions (colleges, etc.) This exception ended on December 31, 1993.[13][14] The Fair Treatment for Experienced Pilots Act (Public Law 110-135) went into effect on December 13, 2007, raising the mandatory retirement age for pilots to 65 from the previous 60.[15] In 1986, the Fair Labor Standards Act was amended to allow the United States Secretary of Labor to provide special certificates to allow an employer to pay less than the minimum wage to individuals whose earning or productive capacity is impaired by age, physical or mental deficiency, or injury.[16] These employees must still be paid wages that are related to the individual's productivity and commensurate with those paid to similarly located and employed nonhandicapped workers.[16] Federal minimum wage laws allow for employers to pay lower wages to young workers. Many state and local minimum wage laws mirror such an age-based, tiered minimum wage.[17] In the United States, a person must generally be at least 14 years old to seek a job, and workers face additional restrictions on their work activities until they reach age 16.[18] Additional age restrictions for workers vary by state. For example, many states require workers under 18 years of age to have work permits and not fulfill occupations deemed hazardous.[19] In Western Air Lines, Inc v Criswell 472 US 400 (1985) the United States Supreme Court held it was lawful to require airline pilots to retire at 60, because the Federal Aviation Authority forbid using pilots over 60 in aviation. But the Court held that refusing to employ flight engineers over that age was unjustified as there were no such FAA requirements. (Note that The Fair Treatment for Experienced Pilots Act (Public Law 110-135) went into effect on December 13, 2007, raising the mandatory retirement age for pilots to 65 from the previous 60.)[15] DeMarco v. Holy Cross High School 4 F.3d 166 (2nd Cir. 1993) was an employment discrimination case brought under the ADEA (Age Discrimination in Employment Act of 1967). The appellant, Guy DeMarco, was released from employment prior to his eligibility for tenure at the age of forty-nine. Holy Cross High School argued that it was not subject to ADEA laws, and if it were that this case against it was in violation of the Free Exercise Clause and the Establishment Clause of the First Amendment. The defendant also argued that the plaintiff failed to utilize the administrative remedies available. The court noted that other anti-discrimination statutes were held to be applicable to religious organizations, with the exception of statutes that prohibited discrimination based on religious belief. Since statutes prohibiting discrimination by race, gender and national origin were already found applicable to religious organizations, it was logical (and a reasonable interpretation of the legislative history) to extend the prohibition against age discrimination to religious organizations as well.[20] The decision of the district court was reversed and the case remanded for further proceedings.[21] Hazen Paper Co. v. Biggins 507 U.S. 604 (1993)[22] was a United States Supreme Court case in which the court held that a disparate treatment claim cannot succeed unless the employee's protected trait had a determinative influence on the employer's decisionmaking.[23] This case concerned how Hazen Paper fired Biggins, 62, a few weeks before his service would have reached the required number of years for his pension to vest. Biggins sued Hazen Paper alleging a violation of the Age Discrimination in Employment Act of 1967.[24] In Kimel v. Florida Bd. of Regents, 528 U.S. 62 (2000), the United States Supreme Court held that state employees cannot sue states for monetary damages under the Age Discrimination in Employment Act of 1967 in federal court.[25] The EEOC may still enforce the ADEA against states, and state employees may still sue state officials for declaratory and injunctive relief.[26] In Gomez-Perez v. Potter (2008), the United States Supreme Court allowed federal workers who experience retaliation as a result of reporting age discrimination under the law to sue for damages.[27] The United States Supreme Court, in Meacham v. Knolls Atomic Power Lab, 554 U.S. 84 (2008), held that the employer, not the employee, bears the burden of proving that a layoff or other action that hurts older workers more than others was based not on age but on some other “reasonable factor.”[28] In 2009, the United States Supreme Court issued its opinion on Gross v. FBL Financial Services, Inc.. In a 5–4 opinion, the Court ruled that private-sector plaintiffs must prove that age was the ""but for"" cause of the adverse employment action they are suing over.[29][30] That is, the plaintiff must prove that age discrimination was the determining reason for the adverse employment action (e.g. the action would not have been taken 'but for' the plaintiff's age).[31] However, the Supreme Court's opinion did not explicitly mention public-sector workers.[32] A later opinion, University of Texas Southwestern Medical Center v. Nassar (2013) applied the same 'but for' standard to retaliation claims. In September 2016, California passed state bill AB-1687, an anti-ageism law taking effect on 1 January 2017, requiring ""commercial online entertainment employment"" services that allow paid subscribers to submit information and resumes (such as IMDbPro), to honor requests to have their ages and birthdays removed. The bill was supported by SAG-AFTRA's former and current presidents Ken Howard and Gabrielle Carteris, who felt that the law would help to reduce ageism in the entertainment industry.[33] On 23 February 2017, U.S. District Judge Vince Girdhari Chhabria issued a stay on the bill pending a further trial, claiming that it was ""difficult to imagine how AB 1687 could not violate the First Amendment"" because it inhibited the public consumption of factual information.[34] In February 2018, Girdhari ruled that the law was unconstitutional, arguing that the state of California ""[had] not shown that partially eliminating one source of age-related information will appreciably diminish the amount of age discrimination occurring in the entertainment industry."" The ruling was criticized by SAG-AFTRA, alleging that the court ""incorrectly concluded there were no material disputed factual issues, while precluding the parties from acquiring additional evidence or permitting the case to go to trial"". The ruling was eventually appealed, but the Ninth Circuit Court of Appeals upheld it in 2020.[35] Babb v. Wilkie, No. 18-882, , 589 U.S. ___ (2020), is a case of the United States Supreme Court in which the justices considered the scope of protections for federal employees in the Age Discrimination in Employment Act of 1967. Specifically, the Court ruled that plaintiffs only need to prove that age was a motivating factor in the decision in order to sue.[36] However, establishing but for causation is still necessary in determining the appropriate remedy. If a plaintiff can establish that the age was the determining factor in the employment outcome, they may be entitled to compensatory damages or other relief relating to the end result of the employment decision.[30][37] Our Lady of Guadalupe School v. Morrissey-Berru, 591 U.S. ___ (2020), is a United States Supreme Court case involving the ministerial exception of federal employment discrimination laws. The case extends from the Supreme Court's prior decision in Hosanna-Tabor Evangelical Lutheran Church & School v. Equal Employment Opportunity Commission (2012)[38] which created the ministerial exception based on the Establishment and Free Exercise Clauses of the United States Constitution, asserting that federal discrimination laws cannot be applied to leaders of religious organizations. The Supreme Court case Our Lady of Guadalupe School v. Morrissey-Berru, along with the consolidated St. James School v. Biel (Docket 19-348), both arose from rulings in the United States Court of Appeals for the Ninth Circuit that found that federal discrimination laws do apply to others within a religious organization that serve an important religious function but lack the title or training to be considered a religious leader under Hosanna-Tabor. One of those rulings in the United States Court of Appeals for the Ninth Circuit was the ruling in Morrissey-Berru v. Our Lady of Guadalupe School, in 2019, in which the United States Court of Appeals for the Ninth Circuit allowed a Catholic elementary school teacher's age discrimination suit to move forward.[39] The religious organization challenged that ruling on the basis of Hosanna-Tabor. The Supreme Court ruled in a 7–2 decision called Our Lady of Guadalupe School v. Morrissey-Berru on July 8, 2020 that reversed the Ninth Circuit's ruling, affirming that the principles of Hosanna-Tabor, that a person can be serving an important religious function even if not holding the title or training of a religious leader, satisfied the ministerial exception in employment discrimination.[40] The Older Americans Amendments of 1975 (Pub. L..mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px;white-space:nowrap}Tooltip Public Law (United States) 94–135) is an Act of the 94th U.S. Congress amending the Older Americans Act of 1965. It prohibits discrimination based on age in programs or activities that receive federal financial assistance, for instance, financial assistance to schools and colleges, provided by the U.S. Department of Education.[41][42] The District of Columbia and twelve states (California, Florida, Iowa, Hawaii, Kansas, Louisiana, Maine, Minnesota, Nebraska, New Mexico, New York, and Vermont) define age as a specific motivation for hate crimes.[43][44] The Twenty-sixth Amendment to the United States Constitution reads: Section 1. The right of citizens of the United States, who are eighteen years of age or older, to vote shall not be denied or abridged by the United States or by any State on account of age. Section 2. The Congress shall have power to enforce this article by appropriate legislation.[45] That amendment was ratified in 1971.[46] Prior to that: In 1943 and 1955 respectively, the Georgia and Kentucky legislatures approved measures to lower the voting age to 18.[47] On June 22, 1970, President Richard Nixon signed an extension of the Voting Rights Act of 1965 that required the voting age to be 18 in all federal, state, and local elections.[48] In his statement on signing the extension, Nixon said: Despite my misgivings about the constitutionality of this one provision, I have signed the bill. I have directed the Attorney General to cooperate fully in expediting a swift court test of the constitutionality of the 18-year-old provision.[49] Subsequently, Oregon and Texas challenged the law in court, and the case came before the Supreme Court in 1970 as Oregon v. Mitchell.[50] By this time, four states had a minimum voting age below 21: Georgia, Kentucky, Alaska and Hawaii.[51][52] In Oregon v. Mitchell (1970), the Supreme Court considered whether the voting-age provisions Congress added to the Voting Rights Act in 1970 were constitutional. The Court struck down the provisions that established 18 as the voting age in state and local elections. However, the Court upheld the provision establishing the voting age as 18 in federal elections. The Court was deeply divided in this case, and a majority of justices did not agree on a rationale for the holding.[53][54] The decision resulted in states being able to maintain 21 as the voting age in state and local elections, but being required to establish separate voter rolls so that voters between 18 and 21 years old could vote in federal elections.[55] The Newsboys Strike of 1899 fought ageist employment practices targeted against youth by large newspaper syndicates in the Northeast. The strikers demonstrated across the city for several days, effectively stopping circulation of the two papers, along with the news distribution for many New England cities. The strike lasted two weeks, causing Pulitzer's New York World to decrease its circulation from 360,000 papers sold per day to 125,000.[56] Although the price of papers was not lowered, the strike was successful in forcing the World and Journal to offer full buybacks to their sellers, thus increasing the amount of money that newsies received for their work.[57] The American Youth Congress, or AYC, was formed in 1935 to advocate for youth rights in U.S. politics.[58] It ended in 1940.[59] The AARP was founded in 1958 by Ethel Percy Andrus (a retired educator from California) and Leonard Davis (later the founder of the Colonial Penn Group of insurance companies).[60][61] Its stated mission is ""to empower people to choose how they live as they age"".[62] It is an influential lobbying group in the United States focusing largely on issues affecting the elderly.[63][64] The Gray Panthers was formed in 1970 by Maggie Kuhn, with a goal of eliminating mandatory retirement; they now work on many social justice issues including eliminating ageism.[65][66][67] Youth Liberation of Ann Arbor started in 1970 to promote youth and fight ageism. Three O'Clock Lobby formed in 1976 to promote youth participation throughout traditionally ageist government structures in Michigan. Senior Action in a Gay Environment (SAGE) was incorporated in 1978 by lesbian and gay activists and aging service professionals; it is now called Services & Advocacy for GLBT Elders. It works with LGBT older adults and aging service providers to address and overcome the challenges of discrimination in older adult service settings.[68] OWL - The Voice of Women 40+ was founded as the Older Women's League by Tish Sommers and Laurie Shields, following the White House Mini-Conference on Older Women in Des Moines, Iowa in October 1980. It advocated for women in the U.S. who were age 40 and over. In March 2017, it was reported that the national organization had decided to disband, but local chapters may continue to function under the OWL name or possibly another name.[69] Old Lesbians Organizing for Change was founded in 1987; the mission of the organization is to ""eliminate the oppression of ageism and to stand in solidarity against all oppressions"" through “[the] cooperative community of Old Lesbian feminist activists from many backgrounds working for justice and the well-being of all old lesbians.”[70] Their initial meeting was inspired by the publication of the book Look Me in the Eye: Old Women, Aging and Ageism by Barbara Macdonald and Cynthia Rich in 1983.[71] Americans for a Society Free from Age Restrictions formed in 1996 to advance the civil and human rights of young people through eliminating ageist laws targeted against young people, and to help youth counter ageism in America.[72] The National Youth Rights Association started in 1998 to promote awareness of the legal and human rights of young people in the United States.[73] The Freechild Project was formed in 2001 to identify, unify and promote diverse opportunities for youth engagement in social change by fighting ageism. In 2002 the Freechild Project created an information and training initiative to provide resources to youth organizations and schools focused on youth rights.[74]"
Affirmative action in the United States,/wiki/Affirmative_action_in_the_United_States," In the United States, affirmative action consists of government-mandated, government-approved, and voluntary private programs granting special consideration to historically excluded groups, specifically racial minorities and women.[1][2] These programs tend to focus on access to education and employment in order to redress the disadvantages[3][4][5][6][7] associated with past and present discrimination.[8] Another goal of affirmative action policies is to ensure that public institutions, such as universities, hospitals, and police forces, are more representative of the populations they serve.[9] U.S. president John F. Kennedy issued Executive Order 10925, which required government contractors to take ""affirmative action to ensure that applicants are employed, and that employees are treated during employment, without regard to their race, creed, color, or national origin"" but affirmative action eventually evolved into a complex system of group preferences which would face many legal challenges. Affirmative action included the use of racial quotas until the Supreme Court ruled that quotas were unconstitutional in 1978.[10] Affirmative action currently tends to emphasize not specific quotas but rather ""targeted goals"" to address past discrimination in a particular institution or in broader society through ""good-faith efforts ... to identify, select, and train potentially qualified minorities and women.""[2][11] For example, many higher education institutions have voluntarily adopted policies which seek to increase recruitment of racial minorities.[12][page needed]Outreach campaigns, targeted recruitment, employee and management development, and employee support programs are examples of affirmative action in employment.[13] Nine states in the United States have banned race-based affirmative action: California (1996), Washington (1998, rescinded 2022[14]), Florida (1999), Michigan (2006), Nebraska (2008), Arizona (2010), New Hampshire (2012), Oklahoma (2012), and Idaho (2020). Florida's ban was via an executive order and New Hampshire and Idaho's bans were passed by the legislature. The other six bans were approved at the ballot.[15] The 1996 Hopwood v. Texas decision effectively barred affirmative action in the three states within the United States Court of Appeals for the Fifth Circuit—Louisiana, Mississippi, and Texas—until Grutter v. Bollinger abrogated it in 2003.[16] Affirmative action policies were developed to address long histories of discrimination faced by minorities and women, which reports suggest produced corresponding unfair advantages for whites and males.[17][18] They first emerged from debates over non-discrimination policies in the 1940s and during the civil rights movement.[19] These debates led to federal executive orders requiring non-discrimination in the employment policies of some government agencies and contractors in the 1940s and onward, and to Title VII of the Civil Rights Act of 1964 which prohibited racial discrimination in firms with over 25 employees. The first federal policy of race-conscious affirmative action was the Revised Philadelphia Plan, implemented in 1969, which required certain government contractors to set ""goals and timetables"" for integrating and diversifying their workforce. Similar policies emerged through a mix of voluntary practices and federal and state policies in employment and education. Affirmative action as a practice was partially upheld by the Supreme Court in Grutter v. Bollinger (2003), while the use of racial quotas for college admissions was ruled unconstitutional in Regents of the University of California v. Bakke (1978).[a] In Students for Fair Admissions v. Harvard (2023), the Supreme Court majority ruled that race-based affirmative action in college admissions violated the Equal Protection Clause of the Fourteenth Amendment, with concurrences highlighting race-based affirmative action's violation of Title VI of the Civil Rights Act. Affirmative action remains controversial in American politics. Supporters claim that it promotes equality and representation for groups which are socioeconomically disadvantaged or have faced historical discrimination or oppression and counteracts continuing bias and prejudice against women and minorities. Supporters also point to contemporary examples of conscious and unconscious biases, such as the finding that job-seekers with African American sounding names may be less likely to get a callback than those with white-sounding names, as proof that affirmative action is not obsolete.[11][20][21] Coversely, opponents argue that these policies constitute racism and/or amount to discrimination against other racial and ethnic groups, such as Asian Americans and White Americans, which entails favoring one group over another based upon racial preference rather than achievement, and many believe that the diversity of current American society suggests that affirmative action policies succeeded and are no longer required.[20] Opponents also argue that it tends to benefit the most privileged within minority groups at the expense of the least fortunate within majority groups,[22][page needed] or that when applied to universities it can hinder minority students by placing them in courses too difficult for them.[23] The policy now called affirmative action came as early as the Reconstruction Era (1863–1877) in which a former slave population lacked the skills and resources for independent living.[24] In 1865, General William Tecumseh Sherman proposed, for practical reasons, to divide the land and goods from Georgia and grant it to black families, which became the ""Forty acres and a mule"" policy.[24] The proposal was never adopted due to strong political opposition, and Sherman's orders were soon revoked by President Andrew Johnson. Nearly a century later (1950s–1960s), the discussion of policies to assist classes of individuals reemerged during the Civil Rights Movement. Civil rights guarantees that came through the interpretation of the Equal Protection Clause of the 14th Amendment affirmed the civil rights of people of color.[25] The first appearance of the term 'affirmative action' was in the National Labor Relations Act, better known as the Wagner Act, of 1935.[12]: 15  Proposed and championed by U.S. Senator Robert F. Wagner of New York, the Wagner Act was in line with President Roosevelt's goal of providing economic security to workers and other low-income groups.[26] During this time period it was not uncommon for employers to blacklist or fire employees associated with unions. The Wagner Act allowed workers to unionize without fear of being discriminated against, and empowered a National Labor Relations Board to review potential cases of worker discrimination. In the event of discrimination, employees were to be restored to an appropriate status in the company through 'affirmative action'.[27] While the Wagner Act protected workers and unions it did not protect minorities, who, exempting the Congress of Industrial Organizations, were often barred from union ranks.[12]: 11  This original coining of the term therefore has little to do with affirmative action policy as it is seen today, but helped set the stage for all policy meant to compensate or address an individual's unjust treatment.[28] FDR's New Deal programs often contained equal opportunity clauses stating ""no discrimination shall be made on account of race, color or creed"",[12]: 11  but the true forerunner to affirmative action was the Interior Secretary of the time, Harold L. Ickes. Ickes prohibited discrimination in hiring for Public Works Administration funded projects and oversaw not only the institution of a quota system, where contractors were required to employ a fixed percentage of Black workers, by Robert C. Weaver and Clark Foreman,[12]: 12  but also the equal pay of women proposed by Harry Hopkins.[12]: 14  FDR's largest contribution to affirmative action, however, lay in his Executive Order 8802 of 1941 which prohibited discrimination in the defense industry or government.[12]: 22  The executive order promoted the idea that if taxpayer funds were accepted through a government contract, then all taxpayers should have an equal opportunity to work through the contractor.[12]: 23–4  To enforce this idea, Roosevelt created the Fair Employment Practices Committee (FEPC) with the power to investigate hiring practices by government contractors.[12]: 22  Following the Sergeant Isaac Woodard incident, President Harry S. Truman, himself a combat veteran of World War I, issued Executive Order 9808[29] establishing the President's Committee on Civil Rights to examine the violence and recommend appropriate federal legislation. Hearing of the incident, Truman turned to NAACP leader Walter Francis White and declared, ""My God! I had no idea it was as terrible as that. We've got to do something."" In 1947 the committee published its findings, To Secure These Rights. The book was widely read, influential, and considered utopian for the times: ""In our land men are equal, but they are free to be different. From these very differences among our people has come the great human and national strength of America."" The report discussed and demonstrated racial discrimination in basic freedoms, education, public facilities, personal safety, and employment opportunities. The committee was disturbed by the state of race relations, and included the evacuation of Americans of Japanese descent during the war ""made without a trial or any sort of hearing...Fundamental to our whole system of law is the belief that guilt is personal and not a matter of heredity or association."" The recommendations were radical, calling for federal policies and laws to end racial discrimination and bring about equality: ""We can tolerate no restrictions upon the individual which depend upon irrelevant factors such as his race, his color, his religion, or the social position to which he is born."" To Secure These Rights set the liberal legislative agenda for the next generation that eventually would be signed into law by Lyndon B. Johnson.[12]: 35–36  To Secure These Rights also called for desegregation of the Armed Forces. ""Prejudice in any area is an ugly, undemocratic phenomenon, but in the armed services, where all men run the risk of death, it is especially repugnant."" The rationale was fairness: ""When an individual enters the service of the country, he necessarily surrenders some of the rights and privileges which are inherent in American citizenship."" In return, the government ""undertakes to protect his integrity as an individual."" Yet that was not possible in the segregated Army, since ""any discrimination which...prevents members of the minority groups from rendering full military service in defense of their country is for them a humiliating badge of inferiority."" The report called for an end to ""all discrimination and segregation based on race, color, creed, or national origins in...all branches of the Armed Services.""[12]: 38–39  In 1947 Truman and his advisors came up with a plan for a large standing military, called Universal Military Training, and presented it to Congress. The plan opposed all segregation in the new post-war Armed Forces: ""Nothing could be more tragic for the future attitude of our people, and for the unity of our nation"" than a citizens' military that emphasized ""class or racial difference.""[12]: 39–40  On February 2, 1948, President Truman delivered a special message to Congress. It consisted of ten objectives that Congress should focus on when enacting legislation. Truman concluded by saying, ""If we wish to inspire the peoples of the world whose freedom is in jeopardy, if we wish to restore hope to those who have already lost their civil liberties, if we wish to fulfill the promise that is ours, we must correct the remaining imperfections in our practice of democracy.""[30] In June, Truman became the first president to address the NAACP. His speech was a significant departure from traditional race relations in the United States. In front of 10,000 people at the Lincoln Memorial, the president left no doubt where he stood on civil rights. According to his speech, America had ""reached a turning point in the long history of our country's efforts to guarantee freedom and equality to all our citizens...Each man must be guaranteed equality of opportunity."" He proposed what black citizens had been calling for – an enhanced role of federal authority through the states. ""We must make the Federal government a friendly, vigilant defender of the rights and equalities of all Americans. And again I mean all Americans.""[12]: 40  On July 26, Truman mandated the end of hiring and employment discrimination in the federal government, reaffirming FDR's order of 1941.[12]: 40  He issued two executive orders on July 26, 1948: Executive Order 9980 and Executive Order 9981. Executive Order 9980, named Regulations Governing for Employment Practices within the Federal Establishment, instituted fair employment practices in the civilian agencies of the federal government. The order created the position of Fair Employment Officer. The order ""established in the Civil Service Commission a Fair Employment Board of not less than seven persons.""[29] Executive Order 9981, named Establishing the President's Committee on Equality of Treatment and Opportunity in the Armed Services, called for the integration of the Armed Forces and the creation of the National Military Establishment to carry out the executive order.[31] On December 3, 1951, Truman issued Executive Order 10308, named Improving the Means for Obtaining Compliance with the Nondiscrimination Provisions of Federal Contracts,[32] which established an anti-discrimination committee on government contract compliance responsible for ensuring that employers doing business with the federal government comply with all laws and regulations enacted by Congress and the committee on the grounds of discriminatory practices.[32] When Eisenhower was elected president in 1952 after defeating Democratic candidate Adlai Stevenson, he believed hiring practices and anti-discrimination laws should be decided by the states, although the administration gradually continued to desegregate the Armed Forces and the federal government.[12]: 50  The President also established the Government Contract Committee in 1953, which ""conducted surveys of the racial composition of federal employees and tax-supported contractors"".[12]: 50–51  The committee, chaired by Vice President Richard Nixon, had minimal outcomes in that they imposed the contractors with the primary responsibility of desegregation within their own companies and corporations.[12]: 51  In the 1960 presidential election, Democratic candidate and eventual winner John F. Kennedy ""criticized President Eisenhower for not ending discrimination in federally supported housing"" and ""advocated a permanent Fair Employment Practices Commission"".[12]: 59  Shortly after taking office, Kennedy issued Executive Order 10925 in March 1961, requiring government contractors to ""consider and recommend additional affirmative steps which should be taken by executive departments and agencies to realize more fully the national policy of nondiscrimination.... The contractor will take affirmative action to ensure that applicants are employed, and that employees are treated during employment, without regard to their race, creed, color, or national origin"".[12]: 60  The order also established the President's Committee on Equal Employment Opportunity (PCEEO), chaired by Vice President Lyndon B. Johnson. Federal contractors who failed to comply or violated the executive order were punished by contract cancellation and the possible debarment from future government contracts. The administration was ""not demanding any special preference or treatment or quotas for minorities"" but was rather ""advocating racially neutral hiring to end job discrimination"".[12]: 61  Turning to issues of women's rights, Kennedy initiated a Commission on the Status of Women in December 1961. The commission was charged with ""examining employment policies and practices of the government and of contractors"" with regard to sex.[12]: 66  In June 1963, President Kennedy continued his policy of affirmative action by issuing another mandate, Executive Order 11114. The order supplemented to his previous 1961 executive order declaring it was the ""policy of the United States to encourage by affirmative action the elimination of discrimination in employment"".[12]: 72  Through this order, all federal funds, such as ""grants, loans, unions and employers who accepted taxpayer funds, and other forms of financial assistance to state and local governments,"" were forced to comply to the government's policies on affirmative action in employment practices.[12]: 72  Lyndon B. Johnson, the Texas Democrat and Senate Majority Leader from 1955 to 1961, began to consider running for high office, and in doing so showed how his racial views differed from those held by many White Americans in the traditional South. In 1957, Johnson brokered a civil rights act through Congress. The bill established a Civil Rights Division and Commission in the Justice Department. The commission was empowered to investigate allegations of minority deprivation of rights.[12]: 57  The first time ""affirmative action"" is used by the federal government concerning race is in President John F. Kennedy's Executive Order 10925, which was chaired by Vice President Johnson. At Johnson's inaugural ball in Texas, he met with a young black lawyer, Hobart Taylor, Jr., and gave him the task to co-author the executive order. ""Affirmative action"" was chosen due to its alliterative quality. The term ""active recruitment"" started to be used as well. This order, albeit heavily worked up as a significant piece of legislation, in reality carried little actual power. The scope was limited to a couple hundred defense contractors, leaving nearly $7.5 billion in federal grants and loans unsupervised.[12]: 60  NAACP had many problems with JFK's ""token"" proposal. They wanted jobs. One day after the order took effect, NAACP labor secretary Herbert Hill filed complaints against the hiring and promoting practices of Lockheed Aircraft Corporation. Lockheed was doing business with the Defense Department on the first billion-dollar contract. Due to taxpayer-funding being 90% of Lockheed's business, along with disproportionate hiring practices, black workers charged Lockheed with ""overt discrimination."" Lockheed signed an agreement with Vice President Johnson that pledged an ""aggressive seeking out for more qualified minority candidates for technical and skill positions.[12]: 63–64  This agreement was the administration's model for a ""plan of progress."" Johnson and his assistants soon pressured other defense contractors, including Boeing and General Electric, to sign similar voluntary agreements indicating plans for progress. However, these plans were just that, voluntary. Many corporations in the South, still afflicted with Jim Crow laws, largely ignored the federal recommendations.[12]: 63–64  This eventually led to LBJ's Civil Rights Act, which came shortly after President Kennedy's assassination. This document was more holistic than any President Kennedy had offered, and therefore more controversial. It aimed not only to integrate public facilities, but also private businesses that sold to the public, such as motels, restaurants, theaters, and gas stations. Public schools, hospitals, libraries, parks, among other things, were included in the bill as well. It also worked with JFK's executive order 11114 by prohibiting discrimination in the awarding of federal contracts and holding the authority of the government to deny contracts to businesses who discriminate. Maybe most significant of all, Title VII of the Civil Rights Act aimed to end discrimination in all firms with 25 or more employees. Another provision established the Equal Employment Opportunity Commission as the agency charged with ending discrimination in the nation's workplace.[12]: 74  Conservatives said that Title VII of the bill advocated a de facto quota system, and asserted unconstitutionality as it attempts to regulate the workplace. Minnesota Senator Hubert Humphrey corrected this notion: ""there is nothing in [Title VII] that will give power to the Commission to require hiring, firing, and promotion to meet a racial 'quota.' [. . .] Title VII is designed to encourage the hiring on basis of ability and qualifications, not race or religion."" Title VII prohibits discrimination. Humphrey was the silent hero of the bill's passing through Congress. He pledged that the bill required no quotas, just nondiscrimination. Doing so, he convinced many pro-business Republicans, including Senate Minority Leader Everett Dirksen (IL) to support Title VII.[12]: 78–80  On July 2, 1964, the Act was signed into law by President Johnson. A Harris poll that spring showed 70% citizen approval of the Act.[12]: 82  The strides that the Johnson presidency made in ensuring equal opportunity in the workforce were built upon by his successor Richard Nixon. In 1969, the Nixon administration initiated the ""Philadelphia Order"". It was regarded as the most forceful plan thus far to guarantee fair hiring practices in construction jobs. Philadelphia was selected as the test case because, as Assistant Secretary of Labor Arthur Fletcher explained, ""The craft unions and the construction industry are among the most egregious offenders against equal opportunity laws . . . openly hostile toward letting blacks into their closed circle."" The order included definite ""goals and timetables."" As President Nixon asserted, ""We would not impose quotas, but would require federal contractors to show 'affirmative action' to meet the goals of increasing minority employment.""[33] It was through the Philadelphia Plan that the Nixon administration formed their adapted definition of affirmative action and became the official policy of the US government. The plan was defined as ""racial goals and timetables, not quotas"".[12]: 124  After the Nixon administration, advancements in affirmative action became less prevalent. ""During the brief Ford administration, affirmative action took a back seat, while enforcement stumbled along.""[12]: 145  Equal rights was still an important subject to many Americans, yet the world was changing and new issues were being raised. People began to look at affirmative action as a glorified issue of the past and now there were other areas that needed focus. ""Of all the triumphs that have marked this as America's Century –...none is more inspiring, if incomplete, than our pursuit of racial justice.""[34] In the first half of the 20th century segregation was considered fair and normal. Due to changes made in American society and governmental policies the United States is past the traditional assumptions of race relations.[12]: 275  ""Affirmative action is a national policy that concerns the way Americans feel about race, past discrimination, preferences, merit – and about themselves. This is why it is an American dilemma, and that is why we must understand how it developed and how its rationale and definition have changed since the 1960s.""[12]: 283  In 1983, Reagan signed Executive Order 12432, which instructed government agencies to create a development plan for Minority Business Enterprises. While the Reagan administration opposed discriminatory practices, it did not support the implementation of quotas and goals (Executive Order 11246).[35] Bi-partisan opposition in Congress and other government officials blocked the repeal of this Executive Order[clarification needed]. Reagan was particularly known for his opposition to affirmative action programs. He reduced funding for the Equal Employment Opportunity Commission, arguing that ""reverse discrimination"" resulted from these policies.[36] However, the courts reaffirmed affirmative action policies such as quotas. In 1986, the Supreme Court ruled that courts could order race-based quotas to fight discrimination in worker unions in Sheet Metal Workers' International Association v. EEOC, 478 U.S. 42. In 1987, in Johnson v. Transportation Agency, Santa Clara County, California, 480 U.S. 616, the Supreme Court ruled that sex or race was a factor that could be considered in a pool of qualified candidates by employers.[37] After the election and inauguration of Barack Obama in the 2008 election, a huge excitement swept the nation for the first African-American president of the United States. Many supporters and citizens began to hope for a future with affirmative action that would be secure under a black president. However, progress was not as apparent within the first few years of president Obama's administration. In 2009, education statistics denote the problems of college admissions in the US: ""The College Board recently released the average 2009 SAT scores by race and ethnicity. They found that the gap between Black and Latino student versus White and Asian students has widened, despite the College Board's recent efforts to change questions to eliminate cultural biases.""[38] To the administration, it was apparent that more work was needed to better the situation. The following year in 2010, Obama presented his plan regarding the past administration's policy, under George W. Bush, called the ""No Child Left Behind Act."" Unlike the No Child Left Behind Act, president Obama's policy would instead reward schools and institutions for working with minorities and oppressed students. Additionally, in an indirect manner, the Obama administration aimed to garner support for more federal money and funds to be allocated to financial aid and scholarships to universities and colleges within the United States.[38] They also have endorsed the decision of Fisher vs. University of Texas where the Supreme Court decision which endorses ""the use of affirmative action to achieve a diverse student body so long as programs are narrowly tailored to advance this goal.""[39] The Trump administration supported rolling back Obama-era policies on affirmative action,[40] and Trump advocated that institutions, including universities, colleges, and schools, should use ""race-neutral alternatives"" concerning admissions. The guidelines the administration set were aimed to curb the Supreme Court decision's in Fisher v. University of Texas.[39][41][42] In 2019, the United States District Court for the District of Massachusetts ruled in Students for Fair Admissions v. President and Fellows of Harvard College, a lawsuit alleging discrimination in admission against Asian Americans by the college, that Harvard's system, while imperfect, nonetheless passed constitutional muster.[43][44] The case was appealed, and in January 2022, the Supreme Court agreed to hear the case together with a similar case related to admissions practices at the University of North Carolina.[45][46] The case was argued on October 31, 2022.[47] After the court rejected affirmative action at U.S. colleges and universities on June 29, 2023, President Joe Biden said he ""strongly"" disagreed with the decision. In a televised address, he urged the nation to make sure the decision did not become ""the last word"" on affirmative action. ""Discrimination still exists in America,"" he said.[48] In 2010, Arizona voters passed a constitutional ban on government-sponsored affirmative action known as Proposition 107.[62] As of January 1, 2012 (House Bill 623), affirmative action is not allowed in college admissions and employment.[70] During the November 6, 2012 election, a majority of Oklahoma voters voted to pass Oklahoma State Question 759, which ended affirmative action in college admissions and public employment.[71] President Kennedy stated in Executive Order 10925 that ""discrimination because of race, creed, color, or national origin is contrary to the Constitutional principles and policies of the United States""; that ""it is the plain and positive obligation of the United States Government to promote and ensure equal opportunity for all qualified persons, without regard to race, creed, color, or national origin, employed or seeking employment with the Federal Government and on government contracts""; that ""it is the policy of the executive branch of the Government to encourage by positive measures equal opportunity for all qualified persons within the Government""; and that ""it is in the general interest and welfare of the United States to promote its economy, security, and national defense through the most efficient and effective utilization of all available manpower"".[49] Some individual American states also have orders that prohibit discrimination and outline affirmative action requirements with regard to race, creed, color, religion, sexual orientation, national origin, gender, age, and disability status.[81] Proponents of affirmative action argue that by nature the system is not only race based, but also class and gender based. To eliminate two of its key components would undermine the purpose of the entire system. The African American Policy Forum believes that the class based argument is based on the idea that non-poor minorities do not experience racial and gender based discrimination. The AAPF believes that ""Race-conscious affirmative action remains necessary to address race-based obstacles that block the path to success of countless people of color of all classes"". The group goes on to say that affirmative action is responsible for creating the African American middle class, so it does not make sense to say that the system only benefits the middle and upper classes.[82] Researchers told ABC News in 2023 that economic inequality, segregation and academic inequity in K-12 schools, as well as the lasting effect of past exclusion from colleges and universities have led to the continued underrepresentation of Black and brown students in four-year institutions.[83] Supporters of affirmative action point out the benefits women gained from the policy as evidence of its ability to assist historically marginalized groups. In the fifty years that disenfranchised groups have been the subject of affirmative action laws, their representation has risen dramatically[84] in the workforce, but some research suggests the increase in white women is due to their decision to enter their workforce rather than affirmative action.[85] According to anti-racism activist Tim Wise: Thanks in large measure to affirmative action and civil rights protections that opened up previously restricted opportunities to women of all colors, from 1972 to 1993: – The percentage of women architects increased from 3% to nearly 19% of the total; – The percentage of women doctors more than doubled from 10% to 22% of all doctors; – The percentage of women lawyers grew from 4% to 23% of the national total; – The percentage of female engineers went from less than 1% to nearly 9%; – The percentage of female chemists grew from 10% to 30% of all chemists; and, – The percentage of female college faculty went from 28% to 42% of all faculty. (Moseley-Braun 1995, 8) Furthermore, since only 1983, the percentage of women business managers and professionals grew from 41% of all such persons, to 48%, while the number of female police officers more than doubled, from 6% to 13% (U.S. Department of Commerce, Bureau of the Census 1995, Table 649). According to a 1995 study, there are at least six million women — the overwhelming majority of them white — who simply wouldn't have the jobs they have today, but for the inroads made by affirmative action (Cose 1997, 171).[86] For the first 250 years of America's recorded history, Africans were traded as commodities and forced to work without pay, first as indentured servants then as slaves. In much of the United States at this time, they were barred from all levels of education, from basic reading to higher-level skills useful outside of the plantation setting.[87] After slavery's abolition in 1865, Black-Americans saw the educational gap between themselves and whites compounded by segregation. They were forced to attend separate, under-funded schools due to Plessy v. Ferguson. Though de jure school segregation ended with Brown v. Board of Education, de facto segregation continues in education into the present day.[88] Following the end of World War II the educational gap between White and Black Americans was widened by Franklin D. Roosevelt's GI Bill. This piece of legislation paved the way for white GIs to attend college. Despite their veteran status returning black servicemen were not afforded loans at the same rate as whites. Furthermore, at the time of its introduction, segregation was still the law of the land barring blacks from the best institutions. Overall, ""Nearly 8 million servicemen and servicewomen were educated under the provisions of the GI Bill after World War II. But for blacks, higher educational opportunities were so few that the promise of the GI Bill went largely unfulfilled.""[89] According to a study by Dr. Paul Brest, Hispanics or ""Latinos"" include immigrants who are descendants of immigrants from the countries comprising Central and South America.[90] In 1991, Mexican Americans, Puerto Ricans, and Cuban Americans made up 80% of the Latino population in the United States. Latinos are disadvantaged compared to White Americans and are more likely to live in poverty.[90] They are the least well-educated major ethnic group and suffered a 3% drop in high school completion rate while African Americans experienced a 12% increase between 1975 and 1990.[90] In 1990, they constituted 9% of the population, but only received 3.1% of the bachelors's degrees awarded. At times when it was favorable to lawmakers, Latinos were considered ""white"" under Jim Crow laws during Reconstruction.[90] In other cases, according to Paul Brest, Latinos have been classified as an inferior race and a threat to white purity. Latinos have encountered considerable discrimination in areas such as employment, housing, and education.[90] Brest finds that stereotypes continue to be largely negative and many perceive Latinos as ""lazy, unproductive, and on the dole.""[90] Furthermore, native-born Latino-Americans and recent immigrants are seen as identical since outsiders tend not to differentiate between Latino groups.[90] The category of Native American applies to the diverse group of people who lived in North America before European settlement.[90] During the U.S. government's westward expansion, Native Americans were displaced from their land which had been their home for centuries. Instead, they were forced onto reservations which were far smaller and less productive.[90] According to Brest, land belonging to Native Americans was reduced from 138 million acres in 1887 to 52 million acres in 1934.[90] In 1990, the poverty rate for Native Americans was more than triple that of the whites and only 9.4% of Native Americans have completed a bachelor's degree as opposed to 25.2% of whites and 12.2% of African Americans.[90] Early Asian immigrants experienced prejudice and discrimination in the forms of not having the ability to become naturalized citizens. They also struggled with many of the same school segregation laws that African Americans faced.[90] Particularly, during World War II, Japanese Americans were interned in camps and lost their property, homes, and businesses.[90] Discrimination against Asians began with the Chinese Exclusion Act of 1882 and then continued with the Scott Act of 1888 and the Geary Act of 1892. At the beginning of the 20th century, the United States passed the Immigration Act of 1924 to prevent Asian immigration out of fear that Asians were stealing white jobs and lowering the standard for wages.[90] In addition, whites and non-Asians do not differentiate among the different Asian groups and perpetuate the ""model minority"" stereotype. According to a 2010 article by Professor Qin Zhang of Fairfield University, Asians are characterized as one dimensional in having great work ethic and valuing education, but lacking in communication skills and personality.[90][91] A negative outcome of this incorrect stereotype is that Asians have been portrayed as having poor leadership and interpersonal skills. This has contributed to the ""glass ceiling"" phenomenon in which although there are many qualified Asian Americans, they occupy a disproportionately small number of executive positions in businesses;[90] although this has recently changed with the many successes of Asian billionaires,[92] pop-culture icons including Bruce Lee, sports figures such as Jeremy Lin, gold-medal figure skater Michelle Kwan, and free-style skier Eileen Gu. Many proponents of affirmative action recognize that the policy is inherently unequal; however, minding the inescapable fact that historic inequalities exist in America, they believe the policy is much more fair than one in which these circumstances are not taken into account. Furthermore, those in favor of affirmative action see it as an effort towards inclusion rather than a discriminatory practice. ""Job discrimination is grounded in prejudice and exclusion, whereas affirmative action is an effort to overcome prejudicial treatment through inclusion. The most effective way to cure society of exclusionary practices is to make special efforts at inclusion, which is exactly what affirmative action does.""[93] The National Conference of State Legislatures held in Washington D.C. stated in a 2014 overview that many supporters for affirmative action argue that policies stemming from affirmative action help to open doors for historically excluded groups in workplace settings and higher education.[1] Workplace diversity has become a business management concept in which employers actively seek to promote an inclusive workplace.[94] By valuing diversity, employers possess the capacity to create an environment in which there is a culture of respect for individual differences as well as the ability to draw in talent and ideas from all segments of the population.[95] By creating this diverse workforce, these employers and companies gain a competitive advantage in an increasingly global economy.[95] According to the U.S. Equal Employment Opportunity Commission, many private sector employers have concluded that a diverse workforce makes a ""company stronger, more profitable, and a better place to work."" Therefore, these diversity promoting policies are implemented for competitive reasons rather than as a response to discrimination, but have shown the value in having diversity.[94] In the year 2000, according to a study by American Association of University Professors (AAUP), affirmative action promoted diversity within colleges and universities.[96][need quotation to verify] This has been shown to have positive effects on the educational outcomes and experiences of college students as well as the teaching of faculty members.[96] According to a study by Geoffrey Maruyama and José F. Moreno, the results showed that faculty members believed diversity helps students to reach the essential goals of a college education, Caucasian students suffer no detrimental effects from classroom diversity, and that attention to multicultural learning improves the ability of colleges and universities to accomplish their missions.[96] Furthermore, a diverse population of students offers unique perspectives in order to challenge preconceived notions through exposure to the experiences and ideas of others.[97] According to Professor Gurin of the University of Michigan, skills such as ""perspective-taking, acceptance of differences, a willingness and capacity to find commonalities among differences, acceptance of conflict as normal, conflict resolution, participation in democracy, and interest in the wider social world"" can potentially be developed in college while being exposed to heterogeneous group of students.[96] In addition, broadening perspectives helps students confront personal and substantive stereotypes and fosters discussion about racial and ethnic issues in a classroom setting.[97] Furthermore, the 2000 AAUP study states that having a diversity of views leads to a better discussion and greater understanding among the students on issues of race, tolerance, fairness, etc.[96] Fidan Ana Kurtulus, an economics professor at the University of Massachusetts Amherst,[98] found that during the 1970s and early 1980s, affirmative action led to an increase in the share of women and minorities working in federal U.S. contractors compared to firms that were not required to follow affirmative action guidelines.[99][100] There are a multitude of supporters as well as opponents to the policy of affirmative action. Many presidents throughout the last century have failed to take a very firm stance on the policy, and the public has had to discern the president's opinion for themselves. Bill Clinton, however, made his stance on affirmative action very clear in a speech on July 19, 1995, nearly two and a half years after his inauguration. In his speech, he discussed the history in the United States that brought the policy into fruition: slavery, Jim Crow, and segregation. Clinton also mentioned a point similar to President Lyndon B. Johnson's ""Freedom is not Enough"" speech, and declared that just outlawing discrimination in the country would not be enough to give everyone in America equality. He addressed the arguments that affirmative action hurt the white middle class and said that the policy was not the source of their problems. Clinton plainly outlined his stance on affirmative action, saying: Let me be clear about what affirmative action must not mean and what I won't allow it to be. It does not mean – and I don't favor – the unjustified preference of the unqualified over the qualified of any race or gender. It doesn't mean – and I don't favor – numerical quotas. It doesn't mean – and I don't favor – rejection or selection of any employee or student solely on the basis of race or gender without regard to merit... In the end, Clinton stated that all the evidence shows that, even though affirmative action should be a temporary policy, the time had not come for it to be ended. He felt it was still a relevant practice and overall, the goal of the nation should be to ""mend it, but don't end it."" Clinton's words became a slogan for many Americans on the topic of affirmative action.[12] Affirmative action has been the subject of numerous court cases, where it is often contested on constitutional grounds. Some states specifically prohibit affirmative action, such as California (Proposition 209), Washington (Initiative 200), Michigan (Michigan Civil Rights Initiative), and Nebraska (Nebraska Civil Rights Initiative). A 2005 study by Princeton sociologists Thomas J. Espenshade and Chang Y. Chung compared the effects of affirmative action on racial and special groups at three highly selective private research universities. The data from the study represent admissions disadvantage and advantage in terms of SAT points (on the old 1600-point scale): In 2009, Princeton sociologist Thomas Espenshade and researcher Alexandria Walton Radford, in their book No Longer Separate, Not Yet Equal, examined data on students applying to college in 1997 and calculated that Asian-Americans needed nearly perfect SAT scores of 1550 to have the same chance of being accepted at a top private university as whites who scored 1410 and African Americans who got 1100.[103] After controlling for grades, test scores, family background (legacy status), and athletic status (whether or not the student was a recruited athlete), Espenshade and Radford found that whites were three times, Hispanics six times, and blacks more than 15 times as likely to be accepted at a US university as Asian Americans.[105] Thomas Espenshade cautions though, ""I stop short of saying that Asian-American students are being discriminated against in the college application process because we don't have sufficient empirical evidence to support that claim.""[106] According to Richard Sander, artificially elevating minority students into schools they otherwise would not be capable of attending discourages them and tends to engender failure and high dropout rates for these students. For example, about half of Black college students rank in the bottom 20 percent of their classes,[107] Black law school graduates are four times as likely to fail bar exams as are whites, and interracial friendships are more likely to form among students with relatively similar levels of academic preparation; thus, Black and Hispanic people are more socially integrated on campuses where they are less academically mismatched.[108] He states that the supposed ""beneficiaries"" of affirmative action – minorities – do not actually benefit and rather are harmed by the policy.[109] Sander's ideas have been disputed, and his empirical analyses have been subject to substantial criticism.[110] A group including some of the country's lead statistical methodologists told the Supreme Court that Sander's analyses were sufficiently flawed that the Court would be wise to ignore them entirely.[111] A 2008 study by Jesse Rothstein and Albert H. Yoon confirmed Sander's mismatch findings, but also found that eliminating affirmative action would ""lead to a 63 percent decline in Black matriculants at all law schools and a 90 percent decline at elite law schools.""[112] These high numbers predictions were doubted in a review of previous studies by Peter Arcidiacono and Michael Lovenheim. Their 2016 article found a strong indication that racial preference results in a mismatch effect. However, they argued that the attendance by some African-American law students to less-selective schools would significantly improve the low first attempt rate at passing the state bar, but they cautioned that such improvements could be outweighed by decreases in law school attendance.[113] A 2021 study in the Quarterly Journal of Economics found that the 1998 ban on race-based affirmative action in California public universities led to lower wages for minority applicants and deterred qualified students from applying, which it stated was inconsistent with the mismatch effect.[114] The controversy surrounding affirmative action's effectiveness is based on the idea of class inequality. Opponents of racial affirmative action argue that the program actually benefits middle- and upper-class African Americans and Hispanic Americans at the expense of lower-class European Americans and Asian Americans. This argument supports the idea of class-based affirmative action. America's poor population is disproportionately made up of people of color, so class-based affirmative action would disproportionately help people of color. This would eliminate the need for race-based affirmative action as well as reducing any disproportionate benefits for middle- and upper-class people of color.[115] In 1976, a group of Italian American professors at City University of New York successfully advocated to be added as an affirmative action category for promotion and hiring. Italian Americans are usually considered white in the US and would not be covered under affirmative action policies, but statistical evidence suggested that Italian Americans were underrepresented relative to the proportion of Italian American residents in New York City.[116][better source needed] Libertarian economist Thomas Sowell wrote in his book, Affirmative Action Around the World: An Empirical Study, that affirmative action policies encourage non-preferred groups to designate themselves as members of preferred groups [i.e., primary beneficiaries of affirmative action] to take advantage of group preference policies.[117] Critics of affirmative action assert that while supporters define diversity as ""heterogeneous in meaningful ways, for example, in skill set, education, work experiences, perspectives on a problem, cultural orientation, and so forth"", the implementation is often solely based on superficial factors including gender, race and country of origin.[118] Supreme Court Justice Clarence Thomas opposes affirmative action. He believes the Equal Protection Clause of the Fourteenth Amendment forbids consideration of race, such as in race-based affirmative action or preferential treatment. He also believes it creates ""a cult of victimization"" and implies blacks require ""special treatment in order to succeed."" Thomas also cites his own experiences of affirmative action programs as a reason for his criticism.[119][120] Frederick Lynch, the author of Invisible Victims: White Males and the Crisis of Affirmative Action, did a study on white males that said they were victims of reverse discrimination.[121] Lynch explains that these white men felt frustrated and unfairly victimized by affirmative action.[122]Shelby Steele, another author against affirmative action, wanted to see affirmative action go back to its original meaning of enforcing equal opportunity. He argued that blacks had to take full responsibility in their education and in maintaining a job. Steele believes that there is still a long way to go in America to reach the goal of eradicating discrimination.[122] Libertarian economist Thomas Sowell identified what he says are negative results of affirmative action in his book, Affirmative Action Around the World: An Empirical Study.[117] Sowell writes that affirmative action policies encourage non-preferred groups to designate themselves as members of preferred groups [i.e., primary beneficiaries of affirmative action] to take advantage of group preference policies; that they tend to benefit primarily the most fortunate among the preferred group (e.g., upper and middle class blacks), often to the detriment of the least fortunate among the non-preferred groups (e.g., poor white or Asian); that they reduce the incentives of both the preferred and non-preferred to perform at their best – the former because doing so is unnecessary and the latter because it can prove futile – thereby resulting in net losses for society as a whole; and that they engender animosity toward preferred groups as well.[117]: 115–147  In the United States, a prominent form of racial preferences relates to access to education, particularly admission to universities and other forms of higher education. Race, ethnicity, native language, social class, geographical origin, parental attendance of the university in question (legacy admissions), and/or gender are sometimes taken into account when the university assesses an applicant's grades and test scores. Individuals can also be awarded scholarships and have fees paid on the basis of criteria listed above. In the early 1970s, Walter J. Leonard, an administrator at Harvard University, invented the Harvard Plan, ""one of the country's earliest and most effective affirmative-action programs, which became a model for other universities around the country.""[124] In 1978, the Supreme Court ruled in Regents of the University of California v. Bakke that public universities (and other government institutions) could not set specific numerical targets based on race for admissions or employment; the Court said that ""goals"" and ""timetables"" for diversity could be set instead.[125][better source needed] Dean of Yale Law School Louis Pollak wrote in 1969 that for the previous 15 years Yale ""customarily gave less weight to the LSAT and the rest of the standard academic apparatus in assessing black applicants"". He wrote that while most black students had ""not achieved academic distinction"", ""very few have failed to graduate"" and that ""many black alumni have ... speedily demonstrated professional accomplishments of a high order"". Pollak justified the university's plans to increase the number of minority students admitted with lowered standards ""in the fact ... that the country needs far more—and especially far more well-trained—black lawyers, bearing in mind that today only 2 or 3 per cent of the American bar is black"", and that if Yale could help ""in meeting this important national need, it ought to try to do so"". He believed that the ""minor fraction of the student body""—up to two dozen in the class entering that year—with ""prior educational deficiencies"" was not likely to damage the school, and expected that the number of ""well prepared"" black applicants would greatly increase in the future.[126] Scholars such as Ronald Dworkin have asserted that no college applicant has a right to expect that a university will design its admissions policies in a way that prizes any particular set of qualities.[127] In this view, admission is not an honor bestowed to reward superior merit but rather a way to advance the mission as each university defines it. If diversity is a goal of the university and their racial preferences do not discriminate against applicants based on hatred or contempt, then affirmative action can be judged acceptable based on the criteria related to the mission the university sets for itself.[128] Consistent with this view, admissions officers often claim to select students not based on academic record alone but also on commitment, enthusiasm, motivation, and potential.[129] Highly selective institutions of higher learning do not simply select only the highest SAT performers to populate their undergraduate courses, but high performers, with scores of 2250 to 2400 points, are extraordinarily well-represented at these institutions.[130] To accommodate the ruling in Hopwood v. Texas banning any use of race in school admissions, the State of Texas passed a law guaranteeing entry to any state university if a student finished in the top 10% of their graduating class. Florida and California also have similar college admission guarantees. Class rank tends to benefit top students at less competitive high schools, to the detriment of students at more competitive high schools. This effect, however, may be intentional since less-funded, less competitive schools are more likely to be schools where minority enrollment is high. Critics argue that class rank is more a measure of one's peers than of one's self. The top 10% rule adds racial diversity only because schools are still highly racially segregated because of residential patterns.[131] To some extent, the class rank rule has the same effect as traditional affirmative action.[131] From 1996 to 1998, Texas did not practice affirmative action in public college admissions, and minority enrollment dropped. The state's adoption of the ""top 10 percent"" rule has helped return minority enrollment to pre-1996 levels.[131] Race-conscious admissions continue to be practiced in Texas following Fisher v. University of Texas. Professor Cornel West estimated that when he attended Harvard College in the early-1970's, 95% of black students were descended from American black families dating back to the Jim Crow era.[132] But during a panel discussion at Harvard University's reunion for black alumni during the 2003–04 academic year, two prominent black professors at the institution—Lani Guinier and Henry Louis Gates—pointed out that one unintended effect of affirmative-action policies at Harvard designed to increase the number of black students had been the replacement of American black students with black immigrants. Guinier and Gates claimed that only about a third of black Harvard undergraduates were from families in which all four grandparents were born into the African American community, and that the majority of black students at Harvard were Caribbean and African immigrants or their children, and/or mixed-race children of biracial couples.[133] By 2007, 41% of black students at Ivy League colleges were reportedly first- or second-generation immigrants, a group which made up only 13% of the US black population. In 2020, Harvard students whose families had been in the US for generations began referring to themselves as ""Generational African-Americans"", who hypothesized that their numbers were vanishingly small.[132] The subject is alleged to be ""taboo"" among admissions officers, and black Harvard students have claimed the university has discouraged them from collecting demographic information about the backgrounds of the black student population.[133] UCLA professor Richard H. Sander published an article in the November 2004 issue of the Stanford Law Review that questioned the effectiveness of racial preferences in law schools. He noted that, prior to his article, there had been no comprehensive study on the effects of affirmative action.[109] The article presents a study that shows that half of all black law students rank near the bottom of their class after the first year of law school and that black law students are more likely to drop out of law school and to fail the bar exam.[109] The article offers a tentative estimate that the production of new black lawyers in the United States would grow by eight percent if affirmative action programs at all law schools were ended. Less qualified black students would attend less prestigious schools where they would be more closely matched in abilities with their classmates and thus perform relatively better.[109] Sander helped to develop a socioeconomically based affirmative action plan for the UCLA School of Law after the passage of Proposition 209 in 1996, which prohibited the use of racial preferences by public universities in California. This change occurred after studies showed that the graduation rate of blacks at UCLA was 41%, compared to 73% for whites. A 2007 study by Mark Long, an economics professor at the University of Washington, demonstrated that when state referendums and court decisions forced flagship public universities in California, Texas, and Washington to abandon their large, race-based affirmative-action preferences in admissions, so-called ""Top-X"" alternatives to racial preferences—in which the highest-graded students at all public high schools in the state were guaranteed admission to public colleges—were unable to make up for the losses in black and Hispanic enrollment. Specifically, apparent rebounds of black and Hispanic enrollment were, in fact, explained by increasing minority enrollment in high schools of those states, and the primary beneficiaries of these ""class-based"" affirmative action policies appeared to be white students. On the other hand, Long noted that affirmative action itself has both moral and material costs, including the unpopularity of race-based affirmative action in college admissions; the high costs associated with full-file reviews of applicants; and the specter of litigation.[134] A 2020 study by UC Berkeley Center Studies in Higher Education researcher Zachary Bleemer on the impact of California's ban on affirmative action on student outcomes using a difference-in-difference research design and a newly constructed longitudinal database linking all 1994–2002 University of California applicants to their college enrollment, course performance, major choice, degree attainment, and wages into their mid-30s found ""the first causal evidence that banning affirmative action exacerbates socioeconomic inequities.""[135] According to the study, the ban on affirmative action decreased Black and Latino student enrollment within the University of California system, reduced their likelihood of graduating and attending graduate school, and resulted in a decline in wages. At the same time, the policy did not significantly impact white and Asian American students. Dean Pollak wrote of the Yale quota for black students in response to a letter from Judge Macklin Fleming of the California Court of Appeal. Fleming criticized the Yale system as ""a long step toward the practice of apartheid and the maintenance of two law schools under one roof"", with consequent ""damage to the standards of Yale Law School"". He warned that such an admission policy ""will serve to perpetuate the very ideas and prejudices it is designed to combat. If in a given class the great majority of the black students are at the bottom of the class"", it would result in racial stratification between students, demands by black students to weaken academic standards, and other racially based ""aggressive conduct"". Fleming noted that racial quotas were a zero-sum game, as ""discrimination in favor of X is automatic discrimination against Y""; Asians in California, for example, were overrepresented in engineering schools and would suffer if black and Mexican applicants received preferential treatment. He stated that a quota system violated ""the American creed, one that Yale has proudly espoused ... that an American should be judged as an individual and not as a member of a group"".[126] In 2006, Jian Li, a Chinese undergraduate at Yale University, filed a civil rights complaint with the Office for Civil Rights against Princeton University, claiming that his race played a role in their decision to reject his application for admission and seeking the suspension of federal financial assistance to the university until it ""discontinues discrimination against Asian Americans in all forms"" by eliminating race and legacy preferences. Princeton Dean of Admissions Janet Rapelye responded to the claims in the November 30, 2006, issue of the Daily Princetonian by stating that ""the numbers don't indicate [discrimination]."" She said that Li was not admitted because ""many others had far better qualifications."" Li's extracurricular activities were described as ""not all that outstanding"".[136] Li countered in an email, saying that his placement on the waitlist undermines Rapelye's claim. ""Princeton had initially waitlisted my application,"" Li said. ""So if it were not for a yield which was higher than expected, the admissions office very well may have admitted a candidate whose ""outside activities were not all that outstanding"".[137] In September 2015, the Department of Justice concluded its nine-year investigation into alleged anti-Asian bias at Princeton and cleared Princeton of charges that it discriminated against Asian American applicants.[138] Furthermore, the department found that a number of Asian American students benefitted from race-conscious admissions.[139] In 2012, Abigail Fisher, an undergraduate student at Louisiana State University, and Rachel Multer Michalewicz, a law student at Southern Methodist University, filed a lawsuit to challenge the University of Texas admissions policy, asserting it had a ""race-conscious policy"" that ""violated their civil and constitutional rights"".[140] The University of Texas employs the ""Top Ten Percent Law"", under which admission to any public college or university in Texas is guaranteed to high school students who graduate in the top ten percent of their high school class.[141] Fisher has brought the admissions policy to court because she believes that she was denied acceptance to the University of Texas based on her race, and thus, her right to equal protection according to the 14th Amendment was violated.[142] The Supreme Court heard oral arguments in Fisher on October 10, 2012, and rendered an ambiguous ruling in 2013 that sent the case back to the lower court, stipulating only that the university must demonstrate that it could not achieve diversity through other, non-race sensitive means. In July 2014, the US Court of Appeals for the Fifth Circuit concluded that UT maintained a ""holistic"" approach in its application of affirmative action, and could continue the practice. On February 10, 2015, lawyers for Fisher filed a new case in the Supreme Court. It is a renewed complaint that the U.S. Court of Appeals for the Fifth Circuit got the issue wrong—on the second try as well as on the first.[143] The Supreme Court agreed in June 2015 to hear the case a second time. In July 2016 a majority of the Court found in favor of the University of Texas at Austin, with Justice Kennedy finding for the Court that the university's affirmative action policies were constitutional, despite the requirement of strict scrutiny. On November 17, 2014, Students for Fair Admissions, an offshoot of the Project on Fair Representation, filed lawsuits in federal district court challenging the admissions practices of Harvard University and the University of North Carolina at Chapel Hill. The UNC-Chapel Hill lawsuit alleges discrimination against white and Asian students, while the Harvard lawsuit focuses on discrimination against Asian applicants. Both universities requested the court to halt the lawsuits until the U.S. Supreme Court provides clarification of relevant law by ruling in Fisher v. University of Texas at Austin for the second time.[144] In May 2015, a coalition of more than 60 Asian-American organizations filed federal complaints with the Education and Justice Departments against Harvard University. The coalition asked for a civil rights investigation into what they described as Harvard's discriminatory admission practices against Asian-American applicants.[145][146][147] The complaint asserts that recent studies indicate that Harvard has engaged in systematic and continuous discrimination against Asian Americans in its ""holistic"" admissions process. Asian-American applicants with near-perfect test scores, top-one-percent grade point averages, academic awards, and leadership positions are allegedly rejected by Harvard because the university uses racial stereotypes, racially differentiated standards, and de facto racial quotas.[148] Harvard denies engaging in discrimination and said its admissions philosophy complies with the law. The school said the percentage of Asian-American students admitted has grown from 17% to 21% in a decade while Asian-Americans represent around 6% of the U.S. population.[149] The lawsuit against Harvard was heard in Boston federal court in October 2018. On October 1, 2019, Judge Allison D. Burroughs rejected the plaintiffs' claims, ruling that Harvard's admissions practices meet constitutional requirements and do not discriminate against Asian Americans.[150] SFFA filed an appeal in the First Circuit Court of Appeals, with oral argument scheduled for September 2020.[25][151] In June 2023 a majority of the Court found in favor of SFFA. In August 2020, the US Department of Justice notified Yale University of its findings that Yale illegally discriminates against Asian American and white applicants and demanded Yale cease using race or national origin in its upcoming 2020–2021 undergraduate admissions cycles.[152] Yale has issued a statement viewing the allegation as ""baseless"" and ""rushed"" and ""will not change its admissions processes in response to today's letter because the DOJ is seeking to impose a standard that is inconsistent with existing law"".[153] Numerous myths and misperceptions regarding affirmative actions shape public opinion on the issue.[154] These misperceptions often shape public opinion on specific cases as well. For example, in Students for Fair Admissions, the conflation of two separate issues—Harvard University's affirmative action policy and specific claims of discrimination by Harvard University – colors individuals judgements on affirmative action as a whole.[144] Such conflation allows ""longstanding myths about affirmative action and socially salient racial stereotypes concerning who does, and does not, belong in elite institutions of higher education"" to prosper.[144] Hence, it is often difficult for public opinion polls on cases, let alone the general issue of affirmative action, to be unaffected by such myths. The use of affirmative action in higher education has been debated countless of times during college admissions seasons, especially due to the mismatch effect. Though it creates opportunities for people of color and for people of minority groups to access higher education, many public universities, have been attacked for depending on racial quotas, implemented through affirmative action policies, to reach diversity goals. This has prompted for high-profile lawsuits and Supreme Court rulings based on arguments of reverse racism or discrimination that prevents admissions to ""more qualified"" white students to take place.[155] Consequently, these cases have constantly reshaped the view on affirmative action policies by referring to it, in its original sense, a race-conscious policy, which ultimately obliges the inclusion of people of minority groups in higher education.[156] Most importantly, it has questioned whether or not affirmative action is effective in achieving numerical goals while avoiding preferential treatment, where many[who?] have deemed it a form of ""reverse discrimination.""[157] In 1974, the California Supreme Court ruled that UC Davis violated the Equal Protection Clause and the Civil Rights Act because they were relying on racial quotas heavily.[158] Allen Bakke was a thirty-five-year-old man who applied to UC Davis medical school in two consecutive years, but was rejected both times.[158] This was because UC Davis had a special admission affirmative action program that reserved 16 spots for minority students, out of 100 admission slots, which Bakke argued was the reason he was rejected twice from the medical program despite having a high GPA and MCAT score.[158] In this special program, mostly run by members from minority groups, applicants who were considered disadvantaged did not have to meet the 2.5 grade point average minimum that the general admissions program implemented, and were only rated against other applicants from minority groups.[159] Since UC Davis was not able to prove that Bakke wouldn't have been admitted even if the special admission programs didn't exist, it was concluded that he was being discriminated by the color of his skin and was not being treated equally due to the racial quota.[159] Thus, Bakke was admitted to the school, as 8 out of 9 judges declared that the heavy reliance on the racial quota violated the Equal Protection Clause on the Fourteenth Amendment.[160] This ruling, however, did not prohibit the use of race as a factor in college admissions decisions,[160] it only prohibited its use for non-competitive admissions that favored a small demographic of minority group students.[157] Public opinion polls on affirmative action have varied significantly. It is likely that survey design, the framing of the survey question itself, and other factors may have significant effects on the survey results. The following polls only discuss affirmative action in higher education. In general, ""affirmative action"" is supported by the general public, but ""considerations based on race"" are opposed. In a survey conducted by Gallup in 2013,[161] 67% of U.S. adults believed college admission should be solely based on merit. According to Gallup: ""One of the clearest examples of affirmative action in practice is colleges' taking into account a person's racial or ethnic background when deciding which applicants will be admitted. Americans seem reluctant to endorse such a practice, and even blacks, who have historically been helped by such programs, are divided on the matter. Aside from African Americans, a majority of all other major subgroups believe colleges should determine admissions solely on merit."" In a national survey conducted by the Pew Research Center in 2014, among 3,335 Americans, 63% felt that affirmative action programs designed to increase the number of African American and minority students on college campuses are a good thing.[162] In February 2019, Gallup published the results of a November and December 2018 survey and found that support for affirmative action programs was growing.[163] They polled 6,502 Americans. Of survey respondents, 65% favored affirmative action programs for women and 61% favored affirmative action programs for minorities. Also in February 2019, the Pew Research Center published the results of a January and February 2019 survey and found that 73% of its respondents said that race or ethnicity should not be a factor in college admissions decisions.[164] According to this survey's results, majorities across racial and ethnic groups agree that race should not be a factor in college admissions decisions. White adults are particularly likely to hold this view: 78% say this, compared with 65% of Hispanics, 62% of African Americans, and 58% of Asians."
All In (Levs book),/wiki/All_In_(Levs_book),"All In: How Our Work-First Culture Fails Dads, Families, and Businesses—And How We Can Fix It Together is a 2015 book by journalist Josh Levs urging changes in employment practices, government policy, and societal attitudes concerning fathers and family care. When Levs, a CNN journalist, requested extended paid parental leave from CNN’s parent company Time Warner in August 2013, he was denied anything more than the two weeks paid leave for biological fathers—much less than 10 weeks paid leave that were provided for women and for men who had babies through adoption or surrogacy.[1] Levs used his two paid weeks, and additionally vacation and sick days as he cared for his three children and wife, who had developed severe preeclampsia.[1] Levs filed a charge with the Equal Employment Opportunity Commission (EEOC)[2][3] against Time Warner demanding equitable paid paternity leave, the claim essentially prevailing a year later.[1][4] Though Time Warner changed its family leave policies, the changes were not retroactive to benefit Levs himself.[5] Kirkus Reviews wrote that All In provides ""well-documented and easy-to-comprehend data on why men need more paid time off to be with their newborn children.""[6] Levs' analysis was said to show how the workplace has not kept pace with the significant changes in ""male-female dynamics at home"" over the past 50 years, and that, in addition to his scrutiny and evaluation of paid paternity leave, Levs also considered issues relating to absentee fathers, lack of intimacy for new parents, and finding the mental and spiritual balance needed for parenting during times of stress.[6] Publishers Weekly described All In as a ""call for men to fight against the laws, policies, and stigmas preventing them from fully participating in their families’ lives,"" more specifically, discussing parental leave, the tax system, paid family leave, the ""doofus dad"" stereotype, fear of men as predators, the stigma against men taking time off work for family, and a plea for men and women to work together,[7] as well as providing action plans for family-supportive work environments.[8]The Daily Beast's Andy Hinds added that All In argues that the ""pop culture image of dads as lazy and uninvolved"" is both false and damaging, pigeonholing both men and women.[9] All In characterizes the workplace as forcing men to place career before family, and penalizing them if they don't.[8][10] The book urges that men be accurately portrayed and more strongly supported in their fatherhood roles, such as with federally mandated paternity leave, and flextime and remote work opportunities.[10] Business Lexington's Paul Sanders wrote that All In reads like ""a series of thematically linked essays"" in a broad-brush approach that attempts to touch many facets of the problem, noting the book's extensive fact-checking approach.[11] The book is said to spotlight gender discrimination in an American workplace that trails much of the world on the issue, noting that in industry's ""war for talent"" companies can attract and retain employees by offering paid paternity leave.[11] The title All In is a play on the title of Sheryl Sandberg’s 2013 women's empowerment book, Lean In.[8][11][12][13] Levs was said to expand and detail Sandberg’s argument that advancing the American economy depends on changing workplace structures, not only for women but for men.[11]Time's Charlotte Alter summarized the gist of All In as being that ""men should lean in just as much as women—they should just do it in a different direction.""[13]"
Anti-Korean sentiment,/wiki/Anti-Korean_sentiment,"Anti-Korean sentiment or Koryophobia describes negative feelings towards Korean people, Korean culture, or the countries of North Korea or South Korea. It can even refer to feelings between Korean groups, especially between North and South Korea, or between Koreans on the Korean peninsula and the Korean diaspora. Anti-Korean sentiment has varied by location and time. Major historical events that impacted it include the Japanese occupation of Korea, the Korean War, and the Vietnam War. In recent years, sentiment has largely been impacted by politics, territorial disputes, disputes over claims of historical revisionism, economic competition, and culture. Anti-Korean sentiment is particularly prevalent in China, Japan, the United States, and between the two Korean states. The Korean Wave has also sparked a pushback in some countries. In other countries, Koreans have either been the target of racism because they are members of a specific ethnic group or they have been the target of racism against Asians in general. Various slurs for Koreans also exist, including slurs which Koreans describe other Koreans with. Korea and China have historically maintained complicated ties.[2][3] When Korea was annexed by Imperial Japan in 1910, it fell under Japanese influence. In China it is believed that some ethnic Koreans served in the Imperial Japanese Army whose invasion of China launched the Second Sino-Japanese War in July 1937.[4] Adding to this sentiment is the allegation that some Koreans reportedly operated the Burma-Siam Death Railway.[5][6] The Chinese referred to Koreans using the slur er guizi (Chinese: 二鬼子; pinyin: èr guǐzi).[7][better source needed] At the end of World War II, North Korea, which was aligned with the Soviet bloc, became an ally of the People's Republic of China (PRC), while the PRC and the Republic of Korea did not recognize each other. During the Korean War, when China was engaged in war with South Korea and its United Nations allies, propaganda was used to indoctrinate people into hating South Korea, which was called a ""puppet state"" of the United States by the PRC government of the time.[8] From 1992 onward, after South Korea's normalization of relations with China, the relationship between the two nations gradually improved. From 2000 onward, Korean popular culture became popular within China.[citation needed] A February 2021 survey conducted by scholars from Rice University, the University of British Columbia, and the Lee Kuan Yew School of Public Policy had 43% of Chinese respondents expressing an unfavorable view of South Korea, compared to 49% expressing a favorable view.[9] In the Kantō Massacre shortly after the 1923 Great Kantō earthquake, ethnic Koreans in Japan were scapegoated and killed by mobs of Japanese vigilantes.[10] During the 2002 FIFA World Cup, Japanese and Korean supporters clashed with one another. Both sides were also known to post racist messages against each other on online bulletins. There were also disputes regarding how the event was to be hosted, as a result of the rivalry between the two nations. The territorial dispute over Liancourt Rocks also fuels outrage. Manga Kenkanryu (often referred to as Hating the Korean Wave) by Sharin Yamano discusses these issues while making many other arguments and claims against Korea. Zainichi Koreans in Japan are also publicly perceived to be a nuisance[11] and are seen as likely to cause trouble and start riots, a view shared by former Tokyo Governor Shintaro Ishihara. A Zainichi organisation which has strong ties to the DPRK, Chongryon, is commonly accused of providing funding and material to North Korea and indoctrinating the Zainichi Korean population to actively hate Japan.[citation needed] Some right-wing groups in Japan today have targeted ethnic Koreans living within Japan. One such group, known as Zaitokukai, is organized by members on the Internet, and has led street demonstrations against Korean schools.[12] There is also much concern in Japan regarding North Korea and its nuclear and long-range missile capabilities, as a result of missile tests in 1993, 1998 and 2006 and an underground nuclear test in 2006. There are also controversies regarding North Korean abductions of Japanese, where Japanese citizens were abducted by North Korean agents during the 1970s and 1980s.[13] The Korean Wave, or the exportation of South Korean pop culture, has created some negative feelings among pockets of Japanese society. Many Japanese citizens with conservative views and some right-wing nationalist groups have organized anti-Korean Wave demonstrations via 2channel. On 9 August 2011, more than 2,000 protesters demonstrated in front of Fuji TV's headquarters in Odaiba, Tokyo against the broadcasting of Korean dramas.[14] Earlier, in July 2011, well-known actor Sousuke Takaoka was fired from his agency, Stardust Promotion, for tweeting criticisms against the influx of Korean dramas.[15] The general perception of Koreans on 2channel is negative, with users depicting them as a violent, unethical, and irrational people who are a 'threat' to Japan.[16] Users often reference stereotypes of Koreans, such as the use of dogs in Korean cuisine.[17] On April 2014, several anti-Korean stickers were found posted at 13 locations along the Shikoku Pilgrimage route; the stickers were denounced by a spokesman from the Shikoku 88 Temple Pilgrimage Association.[18] According to a 2014 BBC World Service Poll, Japanese people alike hold the largest anti–North Korean sentiment in the world, with 91% negative views of North Korea's influence, and with only 1% positive view making Japan the third country with the most negative feelings of North Korea in the world, after South Korea and the United States. Since the end of World War II, the relationship between both North Korea and South Korea have been hostile. The two nations fought in the Korean War, which ended with an armistice agreement in 1953 without a peace treaty. Both nations claim the entire Korean Peninsula and have competed for sovereignty. Tensions after the war reached their highest point in 1968, starting from a North Korean assassination attempt on South Korean President Park Chung Hee, a failed counter-assassination attempt against Kim Il Sung, the Uljin–Samcheok Landings, and the alleged execution of a 9 year old South Korean boy by North Korean commandoes during the landings.[21] Although the relationship somewhat warmed during the Sunshine Policy of the late 1990s to early 2000s,[22] they have since cooled.[23][24] In South Korea, hostility toward North Korea is called ""anti-North [sentiment]"" (반북) and is commonly associated with right-leaning politics.[23][24] According to a 2014 BBC World Service poll, 3% of South Koreans viewed North Korea's influence positively, with 91% expressing a negative view, making South Korea, after Japan, the country with the most negative feelings of North Korea in the world.[25] Naval skirmishes frequently occur between the two states, with North Korea targeting South Korean naval bases. The Bombardment of Yeonpyeong was cited by former UN ambassador Bill Richardson to be ""the most serious crisis on the Korean peninsula since the 1953 armistice"".[26] As of 2023[update], there are around 33,000 North Korean defectors in South Korea. They have widely and consistently reported experiencing discrimination.[27][28] Areas of discrimination include but are not limited to employment discrimination, social isolation, and difficulty finding spouses.[27] Some South Koreans even admit to avoiding businesses owned by North Korean defectors.[27] According to a 2012 study, North Korean men have greater difficulty than North Korean women in finding a spouse.[29] A 2015 paper highlighted the tendency of a South Korean variety show, Now On My Way to Meet You, to disproportionately present North Korean women as attractive marriage partners.[29] The treatment of ethnic Koreans who were born abroad and returned to South Korea has changed over time. In the 1990s, many young people with pro-unification sentiment viewed ethnic Koreans positively, and saw them as ""representatives of the authentic Korean nation"".[29] However, sentiments subsequently cooled, and South Korean identity came to exclude both North Koreans and foreign-born ethnic Koreans.[29] Foreign-born Koreans who now live in South Korea have widely reported experiencing discrimination from South Koreans. They are reportedly seen as lazy, prone to commit crimes, and dirty.[29] A 2009 study found that while foreign-born ethnic Koreans were preferred over non-Korean workers by employers, ethnic Koreans were ""at least as likely to report discrimination"".[29] People with partial Korean heritage have also experienced discrimination in South Korea, although this trend may be diminishing since at latest the late 2000s.[29] In 2009, South Korean schools were prohibited from promoting ideas of ethnic purity and homogeneity, and in 2011 the Korean military amended their oath, replacing the term minjok, meaning ""nation"", with ""citizen"".[29] For much of the US's early relationship with Korea, the overall American public was overwhelmingly disinterested in or even unaware of Korea. The perception of Korea by politicians and the press, however, began much more negatively.[30][31][32] Korea's earliest interactions with the US caused it to gain notoriety with American politicians and press. The 1866 General Sherman incident, in which Koreans destroyed and killed the crew of a US ship that was illegally navigating its rivers, drew widespread condemnation in American newspapers. The New York Tribune wrote:[30] Of Corea [sic], a country in North-Eastern Asia, little is known. It is nominally tributary to China, and is inhabited by a semi-barbarous people, extremely jealous of foreigners, with whom they hold but a very limited intercourse. Koreans were widely portrayed as vicious, xenophobic, and savage ""orientals"" that rejected the ideals of the civilized West. The following 1871 United States expedition to Korea and its ensuing conflict also contributed to these negative perceptions. The American press mostly reacted positively to the Joseon–United States Treaty of 1882, which Joseon was compelled to sign.[30] For decades, most publications portrayed Korea as backwards, poor, and inferior to Japan. Most exoticised the country with nicknames such as ""The Hermit Kingdom"" and ""The Land of Hats"". W. C. Kitchin wrote in his 1884 book Christianity in Corea:[30] The story is told by those who have seen it that it takes three able-bodied Coreans to run a common spade. The people are extremely indolent and as a consequence miserably poor. Many publications commented negatively on the poor social status of Korean women. One newspaper headline read ""Corean [sic] Women: Noble Ladies and Degraded Slave Girls of the Hermit Kingdom: They are More Secluded than Turks and Have Few Rights Respected by Man"".[30] The minority of American journalists, politicians, and activists who visited Korea generally held more favorable opinions of it, and some expressed frustration at the negative opinions of their countrymen.[30] Initially, sentiments towards the Japanese colonization of Korea were positive. US President Theodore Roosevelt was an outspoken critic of Joseon and Korean people. He described Koreans as ""unenlightened and recalcitrant"" and proudly called himself ""pro-Japanese"".[31][32] These sentiments were mostly shared by other high-level US officials, who felt that colonization by the more-enlightened Japanese would be beneficial to Korea.[30] Negative impressions may have been somewhat influenced by ""Japanese information channels"", which had significantly higher funding and reach in the US than any Korean sources did.[31] In 1894, an article in the New York Herald declared: [Japan] has the right to occupy Corea in the interest of the commerce and civilization of the Western world. She will remain in Corea as warder of the little kingdom just emerging from the Chinese darkness, assisting her in moral, intellectual, and material development, leaving the country when her work is done, when the Hermit Kingdom has been place on the proper pathway of good government. Like Japan, and through Japan, Corea must be made the outpost of Western civilization and commerce against Mongolian decrepitude and exclusiveness. However, writings about Korea became more sympathetic in the late 1910s, after information about Japan's Twenty-One Demands to China became public knowledge, and after Japan's violent suppression of the Korean March 1st Movement in 1919.[30][31] Following North Korea's heavy re-militarization and a series of missile tests, Americans were conditioned to fear a possible attack by a ""rogue state"" such as North Korea. In United States President George W. Bush's State of the Union Address on January 29, 2002, he described North Korea as a part of the ""Axis of evil"". Following the development of the nuclear program of North Korea and the 2006 North Korean nuclear test, the United States imposed UN sanctions on North Korea. These economic sanctions are very unlikely to be lifted by the United States due to North Korea's noncompliance with the six-party talk agreements.[citation needed] From 1988 until 2008, and since November 2017, North Korea has been designated a state sponsor of terrorism for supporting Hamas and Hezbollah against Israel,[33] their role in the murder of Kim Jong-nam, supporting dictator Bashar al-Assad in the Syrian Civil War, close relationships with Iran, and the suspicious death of Otto Warmbier.[citation needed] The 1992 Los Angeles riots were partially motivated by Anti-Korean sentiment among African Americans,[34] and famously lead to the rise of the phrase ""roof Koreans"" or ""rooftop Koreans"".[35][36] A year before the riots, on March 16, 1991, Korean American store owner Soon Ja Du fatally shot 15-year-old African American Latasha Harlins.[34][37] This incident and other tensions became a significant part of the 1992 riots, which were sparked by alleged police brutality towards Rodney King.[35][34][37][36] The protests saw mass ransacking and destruction of Korean American and other Asian-owned stores in the Koreatown, Los Angeles area by groups of African-Americans, as well as armed Korean Americans defending stores from the rooftops of buildings.[38][36][37][35] Both Koreans and African Americans were killed in the riots.[39] Of the $1 billion in damages the city experienced, around half was suffered by Korean business owners.[39] In 1937, nearly 172,000 ethnic Koreans were forcefully transferred from the Russian Far East to Soviet Central Asia under the national delimitation policy.[40] The deportation was preceded by a typical Soviet scenario of political repression: falsified trials of local party leaders accused of insurrection, accusations of plans of the secession of the Far Eastern Krai, local party purges, and articles in Pravda about the Japanese espionage in the Far East.[41] The resettlement plans were revived with new vigor in August 1937, ostensibly with the purpose of suppressing ""the penetration of the Japanese espionage into the Far Eastern Krai"". This time, however, the direction of resettlement was westward, to Soviet Central Asia. From September to October 1937, more than 172,000 Soviet Koreans were deported from the border regions of the Russian Far East to Kazakh SSR and Uzbek SSR (the latter including Karakalpak ASSR).[42][43] Within Taiwan, some existing animosity towards Koreans amongst Taiwanese may be present as a result of the rivalry between the two states in relation to baseball.[44][45] Disputes between Taiwan and Korea in the international sport competition arose numerous times. In November 2010, Taiwanese citizens protested against the disqualification of Taekwondo athlete Yang Shu-chun at the 2010 Asian Games after a Korean-Filipino referee[46][47] disqualified a Taiwanese fighter.[48] Images and messages deriding South Korean products and culture were widely shared online. There were reports of restaurants displaying ‘No Koreans’ signs on their doors, and protesters burning the Korean flag or destroying South Korean products.[49] On 23 August 1992, South Korea's ""Nordpolitik"" (Northern diplomacy) have made it to establish a diplomatic ties with the People's Republic of China after Soviet Union. This resulted in the change in the diplomatic relationship of South Korea with the Republic of China, since it replaced anti-communist foreign policy with an effort to improve relations with other surrounding countries in the sense of geopolitics, including the People's Republic of China, in order to pressure and appease North Korea that eases the political anxiety and softens military tension in the Korean Peninsula and enables the possibility of a peaceful reunification of Korea. As normalization begun, President Roh transferred diplomatic recognition from the ROC to PRC, and confiscated the property of the ROC embassy, transferring it to the PRC.[50] According to an official from the Korean trade office in Taipei, sales of Korean products are not very successful in Taiwan because ""the Taiwanese felt very betrayed after Korea severed diplomatic ties with Taiwan and reestablished ties with China in 1992, because the people of Taiwan had seen Korea as an ally in the fight against Communism... Now because the two countries have similar export-oriented economies and focus on the same business sectors, the Taiwanese see Korea as a great rival, and think that losing to Korea would be the end of Taiwan.""[51] In June 2012, CEO of Foxconn Terry Gou stated that he had ""great esteem for Japanese (businessmen), especially those who are able to disagree with you in person and not stab you in the back, unlike the Gaoli bangzi (a racial slur for Koreans)"", sparking controversy.[52] It has been alleged that South Korean soldiers committed war crimes during the Vietnam War that killed somewhere between 40,000 to 50,000 civilians, which has led to lingering anti-Korean sentiment especially amongst older Vietnamese people.[53] The South Korean government has long denied these charges. However in 2020, alleged Vietnamese survivors of these war crimes, including citizen Nguyen Thi Thanh,[54] filed lawsuits against South Korea.[53] By contrast, Vietnam and North Korea enjoyed a more positive relationship in the Vietnam War.[55] Allegations of sex trafficking in South Korea of Vietnamese women has also sparked some negative sentiment amongst Vietnamese people.[56] While the Korean Wave has been mostly enthusiastically accepted among younger women in Vietnam, there has been some pushback from government and the public.[57] Criticisms focused on the perceived femininity of Korean male idols and perceived self-indulgence that went against the spirit of the collectivist Communist culture.[57] Historically, Korean soldiers were compelled to serve on the side of the Empire of Japan during the Japanese occupation of the Philippines in World War II. This has caused some Filipinos, especially older ones, to associate the Koreans with atrocities committed during the war.[58] Ethnographic fieldwork done in Sabang from 2003 to 2015 found that the influx of Koreans was viewed negatively by some locals and resident Westerners.[59] South Koreans were also identified in 2007 as the top violator of immigration laws according to the Philippine Bureau of Immigration.[60] Many Korean social media influencers have been accused of a marketing strategy dubbed pinoy baiting,[61] a practice that many other foreigners are also accused of.[62] The strategy refers to the insincere usage, appropriation, and acknowledgment of Filipino culture by foreigners to pander to a Filipino audience. Some Filipinos perceive Koreans to be rude and to refuse integration into Filipino culture.[63] Another area of concern was the prohibition of South Korean tourists from doing business with local tourist firms by South Korean tour operators. This would mean that Filipino firms would benefit significantly less from South Korean tourists.[citation needed] Some Korean media portrayals of Filipinos in movies such as Wandeuki (Punch) and negative treatment of Filipino-born or Filipino-raised celebrities living in South Korea such as politician Jasmine Lee and entertainer Sandara Park, have worsened Filipino views of Koreans.[64] In an interview, Sandara Park stated, ""[Filipinos] are really gentle. I feel upset because the Korean media only reports crime [when talking about the Philippines].""[65] Senator Jinggoy Estrada proposed banning all Korean dramas and movies in the Philippines, and said ""My observation is if we continue showing Korean telenovelas, our citizens praise the Koreans while Filipino artists continue losing jobs and money. So sometimes it comes to my mind that we should ban the telenovelas of the foreigners, and the Filipino artists who have great talent in acting are what we should be showing in our own country.""[66][67] Estrada clarified that he was only frustrated ""that while we are only too eager and willing to celebrate South Korea's entertainment industry, we have sadly allowed our own to deteriorate because of the lack of support from the movie going public.""[68][69] In September 2020, Filipino-American TikTok star Bella Poarch posted a video of herself dancing, in which Japan's rising sun flag could be seen tattooed on her arm. Koreans swarmed the comments section saying the tattoo was offensive and that she should apologize and get it removed.[70] Shortly after backlash and criticism from her video, Bella posted a comment of apology on TikTok : ""I’m very sorry if my tattoo offends you,"" she wrote. ""I love Korea, please forgive me."" Additionally, her caption read, ""I would never do anything to hurt anyone."" Bella also explained that she got the tattoo back in March 2020 but had it scheduled for removal. She also promised to learn more about the symbol's history and help educate people further on the symbol, but has been unable to remove the tattoo as a result of the COVID-19 pandemic. Despite the apology, many Korean users continued with hostile comments, attacking Filipinos referring to them as poor, slaves, ugly, and uneducated, as well as making racist remarks. The issue soon spilled over Twitter, sparking an argument on Korea's racist attitudes and the long history between South Korea and the Philippines. Along with #CancelKorea, the hashtags #ApologizeToFilipinos including #CancelRacism and #한국취소 (meaning Cancel Korea, or in Hanja: #韓國取消) also trended with Twitter, with Filipino users airing out their anger at the mockery and insults.[71] However, the anger was relieved when other Korean netizens apologized on behalf of the racist remarks, spreading the hashtag '미안해요 필리핀 (#SorryToFillipinos)'. From these apologies, some Filipinos suggested to change the hashtag #CancelKorea to #CancelRacism.[72] Some Filipino netizens went out to apologize for any offensive remarks made against the Koreans during the spat, using the hashtag #SorryToKoreans and accepting the apology.[73][74] In Indonesia, Anti-Korean sentiment emerged in the 2000s. The emergence of anti-Korean sentiment is caused by several factors, such as plastic surgery and atheism in South Korea. Some Indonesians call Koreans ""plastic"".[75] This stereotype arises because of the popularity of plastic surgery in South Korea.[76] This stereotype has strengthened since the suicide of the former member of Shinee, Jonghyun.[77] In addition, there are assumptions that Korean drama lovers are excessive and people of Korea are always committing adultery.[78][79] It was reported in 2013 that some Bali businesses had put up signs prohibiting Korean customers, due to reports that a number of them flouted regulations during their stay.[80][81] In 2021, a South Korean man allegedly launched racist attack against Indonesian woman on social media, this sparked anger among Indonesian public and triggered further anti-Korean sentiment in the country.[82] also in that year, A Korean internet personality living in the country named SunnyDahye also under fire by Indonesian people due to her past comments calling Indonesians are ""stupid"" and she also allegedly pretended to fast during the month of Ramadhan, the live coverage of the 2020 Olympics in garnered ire to some Indonesians after MBC mistakenly setting a picture of the map of Malaysia when the Indonesian contingent arrives at the opening ceremony.[citation needed] In 2008, it was reported that some South Korean men took sex tourism trips to Mongolia, often as clients of South Korean-run businesses in the country. This was said to spark anti-Korean sentiment and an increased number of assaults on South Korean nationals in the country.[83] The popularity of the Korean wave in Thailand has led some Thai authorities to cast it as a threat to local culture.[84] Some locals in 2017 reportedly began to perceive Hallyu negatively or as a form of cultural imperialism.[85] In early 2020, a leading Italian music school banned all East Asian students from attending classes due to coronavirus fear, with South Koreans the largest nationality being affected.[86][87] South Korean students also describe being barred from the building and being mocked by other students because of their origin. In addition, some South Korean residents have reported fear of leaving their homes amid rising incidents of discrimination and mockery, and others considered leaving Italy because they could not ""stay in a place that hates us"".[88] Because of the COVID-19 pandemic, South Korean tourists were instructed to avoid public places and remain in isolation in their hotels.[89] The Israeli military announced its intention to quarantine South Korean nationals to a military base.[90] Many of the remaining South Koreans were rejected by hotels and were forced to spend nights at Ben Gurion Airport.[91] An Israeli newspaper subsequently published a Korean complaint that ""Israel is Treating [Korean and other Asian] Tourists Like Coronavirus"".[92] South Korean Foreign Minister Kang Kyung-wha has described Israel's response as ""excessive"".[93] Many Koreans residents in Germany have reported an increase in anti-Korean incidents following the outbreak of COVID-19, and the South Korean embassy has warned its citizens of the increasing hateful climate facing them.[94] As suspicion toward Koreans is growing, locals are also opting to avoid Korean restaurants, some of which have reported a sales decline of 80%.[95] KLM, the country's flag carrier airline, prohibited only Korean passengers from using their toilets on one of their flights.[96] In general, there has recently been a spate of anti-Korean incidents in the Netherlands, which have targeted both Korean nationals and Dutch people of Korean descent. These incidents range from vandalism of homes to violent assault to harassment. More than 150 Korean expat respondents in an online survey indicated they had experienced an xenophobic incident.[97][98] Despite the popularity of South Korean culture in Brazil among young people,[99] some anti-Korean incidents have occurred in Brazil.[100] In 2017, the Brazilian television host Raul Gil was accused of racism and xenophobia while making derogatory jokes to Asians and a ""slit eye"" gesture during a live interview with the K-Pop group K.A.R.D. This drew backlash from the Brazilian and foreign press.[101][102][103] In 2019, a Brazilian couple published several videos on social media making fun of Korean food and language during a trip to South Korea. The case generated harsh criticism on social media.[104] The following is a list of derogatory terms referring to either Korea or Korean people."
Anti-literacy laws in the United States,/wiki/Anti-literacy_laws_in_the_United_States,"Anti-literacy laws in many slave states before and during the American Civil War affected slaves, freedmen, and in some cases all people of color.[1][2] Some laws arose from concerns that literate slaves could forge the documents required to escape to a free state. According to William M. Banks, ""Many slaves who learned to write did indeed achieve freedom by this method. The wanted posters for runaways often mentioned whether the escapee could write.""[3] Anti-literacy laws also arose from fears of slave insurrection, particularly around the time of abolitionist David Walker's 1829 publication of Appeal to the Colored Citizens of the World, which openly advocated rebellion,[4] and Nat Turner's slave rebellion of 1831. The United States is the only country known to have had anti-literacy laws.[5] Between 1740 and 1834 Alabama, Georgia, Louisiana, Mississippi, North and South Carolina, and Virginia all passed anti-literacy laws.[6] South Carolina passed the first law which prohibited teaching slaves to read and write, punishable by a fine of 100 pounds and six months in prison, via an amendment to its 1739 Negro Act.[7][8] Some slaveowners blamed abolitionists for the supposed need for anti-literacy laws. For example, South Carolina's James H. Hammond, an ardent pro-slavery ideologue, wrote in a letter written in 1845 to the British abolitionist Thomas Clarkson: ""I can tell you. It was the abolition agitation. If the slave is not allowed to read his bible, the sin rests upon the abolitionists; for they stand prepared to furnish him with a key to it, which would make it, not a book of hope, and love, and peace, but of despair, hatred and blood; which would convert the reader, not into a Christian, but a demon. [...] Allow our slaves to read your writings, stimulating them to cut our throats! Can you believe us to be such unspeakable fools?""[9] Significant anti-black laws include: Mississippi state law required a white person to serve up to a year in prison as ""penalty for teaching a slave to read.""[12] A 19th-century Virginia law specified: ""[E]very assemblage of negroes for the purpose of instruction in reading or writing, or in the night time for any purpose, shall be an unlawful assembly. Any justice may issue his warrant to any office or other person, requiring him to enter any place where such assemblage may be, and seize any negro therein; and he, or any other justice, may order such negro to be punished with stripes.""[13] In North Carolina, black people who disobeyed the law were sentenced to whipping while whites received a fine, jail time, or both.[14] AME Bishop William Henry Heard remembered from his enslaved childhood in Georgia that any slave caught writing ""suffered the penalty of having his forefinger cut from his right hand."" Other formerly enslaved people had similar memories of disfigurement and severe punishments for reading and writing.[8] Arkansas, Kentucky, and Tennessee were the only three slave states that did not enact a legal prohibition on educating slaves.[15] It is estimated that only 5% to 10% of enslaved African Americans became literate, to some degree, before the American Civil War.[15] Restrictions on the education of black students were not limited to the South.[15] While teaching blacks in the North was not illegal, many Northern states, counties, and cities barred black students from public schools.[16] Until 1869, only whites could attend public schools in Indiana and Illinois.[16] Ohio excluded black children from public schools until 1849, when it allowed separate schools for black students.[16] Public schools were also almost entirely segregated in Michigan, Minnesota, New Jersey, Pennsylvania, and New York.[16] Only Massachusetts had de-segregated public schools before the Civil War (it barred segregation in public schools in 1855).[15][16] An attempt in 1831 to open a college for black students in New Haven, Connecticut was met with such overwhelming local resistance that the project was almost immediately abandoned (see Simeon Jocelyn).[17] Private schools that attempted to educate black and white students together, often opened by abolitionists, were destroyed by mobs, as in the case of Noyes Academy in Canaan, New Hampshire[18] and the Quaker Prudence Crandall's Female Boarding School in Canterbury, Connecticut.[17] After the Civil War, most Northern states legally prohibited segregation in public schools, although it often continued in practice, pre-Brown v. Board of Education, including through racially gerrymandered boundaries of school districts.[16] Once, finding us all three busily writing, Violet stood for some moments silently watching the mysterious motion of our pens, and then, in a tone of deepest sadness, said: ""O! dat be great comfort, Missis. You can write to your friends all 'bout ebery ting, and so hab dem write to you. Our people can't do so. Wheder dey be 'live or dead, we can't neber know—only sometimes we hears dey be dead.""[19] Educators and slaves in the South found ways to both circumvent and challenge the law. John Berry Meachum, for example, moved his school out of St. Louis, Missouri when that state passed an anti-literacy law in 1847, and re-established it as the Floating Freedom School on a steamship on the Mississippi River, which was beyond the reach of Missouri state law.[20] After she was arrested, tried, and served a month in prison for educating free black children in Norfolk, Virginia, Margaret Crittendon Douglas wrote a book on her experiences, which helped draw national attention to the anti-literacy laws.[21]Frederick Douglass taught himself to read while he was enslaved.[22] A runaway slave ad published in Tuscaloosa, Alabama in 1845 complained, ""[Fanny] can read and write, and so forge passes for herself.""[23] In Tennessee, ""Any slave who forged a pass or certificate was to be whipped with not exceeding thirty-nine lashes; any person giving, or causing to be given...any other instrument of writing intended to aid the slave in escape from his master was to suffer imprisonment for not less than three nor more than ten years.""[24] Despite the risks, literacy was seen by the enslaved as a means of advancement and liberation, and they secretly learned from and taught one another. One historian noted that 20% of the runaway slaves in antebellum Kentucky were able to read, and 10% were able to write. Enterprising child slaves would trade items like marbles and oranges to white children in exchange for reading lessons, and adults sometimes learned from other adults, black and white. One enslaved man, Lucius Holsey, acquired a library of five books by selling rags: two spelling books, a dictionary, John Milton's Paradise Lost, and the Bible. With these five books, he painstakingly taught himself to read by memorizing single words.[8] John Hope Franklin says that despite the laws, schools for enslaved Black students existed throughout the South, including in Georgia, the Carolinas, Kentucky, Louisiana, Florida, Louisiana, Tennessee, and Virginia. In 1838, Virginia's free black population petitioned the state, as a group, to send their children to school outside of Virginia to bypass its anti-literacy law. They were refused.[8] In some cases, slaveholders ignored the laws. They looked the other way when their children played school and taught their slave playmates how to read and write. Some slaveholders saw the economic benefit in having literate slaves who could undertake business transactions and keep accounts. Others believed that slaves should be sufficiently literate to read the Bible.[3] In Norfolk, Virginia, the anti-literacy law was not abolished until after the Civil War, in 1867, as a result of black residents petitioning the federal government to end it.[25]"
Discrimination against autistic people,/wiki/Discrimination_against_autistic_people," Discrimination against autistic people is the discrimination, persecution, and oppression that autistic people have been subjected to. Discrimination against autistic people is a form of ableism.[1] Research published in 2019 used data from more than 8,000 children in the University of London's Millennium Cohort Study, which tracks the lives of about 19,000 people born in the United Kingdom starting in 2000. Out of the children selected, 231 were autistic. The study from the Journal of Autism and Developmental Disorders found that these autistic children were more likely to engage in ""two-way sibling bullying"", meaning being both a victim and perpetrator of bullying.[2] Further research published in 2017, a meta-analysis of three studies, demonstrated that ""first impressions of individuals with ASD made from thin slices of real-world social behavior by typically-developing observers are not only far less favorable across a range of trait judgments compared to controls, but also are associated with reduced intentions to pursue social interaction.""[3] The meta-analysis continues, ""These patterns are remarkably robust, occur within seconds, do not change with increased exposure, and persist across both child and adult age groups.""[3] This may be why autistic people have ""smaller social networks and fewer friendships, difficulty securing and retaining employment, high rates of loneliness, and an overall reduced quality of life.""[3] Smaller social networks, fewer friendships, and loneliness correlate with severe health outcomes. According to a paper published in the Journal of Health and Social Behavior, ""Health risks associated with social isolation have been compared in magnitude to the well-known dangers of smoking cigarettes and obesity.""[4] Furthermore, according to the UK Office for National Statistics, the unemployment rate of autistic people may reach 85%, the highest rate among all disabled groups studied.[5] autistic adults are also more likely to face healthcare disparities, such as being unvaccinated against common diseases like tetanus and being more likely to use emergency services.[6] In the United States, people with disabilities are victims of violent crime three times as often as people without disabilities. The Bureau of Justice Statistics does not report separately on autistic victims, but it does note that the victimization rate is especially high among those whose disabilities are cognitive. A small-sample study of Americans and Canadians found that autistic adults face a greater risk of sexual victimization than their peers. Autistic respondents were more than twice as likely to say they had been the victim of rape and over three times as likely to report unwanted sexual contact.[7] In 2018, a large scale study found that autistic girls were almost three times more likely to be a victim of sexual abuse.[8] Representation of autistic people in media has perpetuated myths about autism, including characterizing autism as shameful and burdensome for family members, advertising fake cures for autism, and publicizing the long-disproven arguments surrounding vaccines and autism. These myths are perpetuated in mass media as well as news media and social media.[9] Stigmatization of autism can also be perpetuated by advertising from autism conversion organizations, such as Autism Speaks' advertising wherein a mother describes having considered murder-suicide in front of her autistic daughter or the NYU Child Study Center's advertisements where autism is personified as a kidnapper holding children for ransom.[10] The advertising from Autism Speaks also brings up another form of discrimination autistic children in particular face, which is subpar education. In the US, only one third of autistic children in public schools receive special education services.[11] Moreover, the autistic behavior known as stimming is frequently referred to as ""distracting"" and the way autistic people naturally talk is often described as rude.[12] In the United States, recently published research[13] shows that the National Institutes of Health (NIH) has not funded grants focused on the treatment of physical health disparity conditions, such as insomnia, cardiovascular disease, and other conditions, in autistic adults across four decades of research funding. McDonald and Scudder (2023) explicitly distinguish health disparity conditions that affect the lives of autistic people from research grants focused on the cause, cure, and prevention of autism. Although their research focused on autistic adults without an intellectual disability, the research did not exclude this population and found a lack of funded grants focused on the treatment of health disparity conditions for autistic adults with intellectual disabilities. The research describes several systemic discriminatory ""nodes"" of practices and policies that may contribute to the apparent discrimination in research grant funding. These include the exclusion of disability populations from groups designated for physical health disparity research grants, the designation of autism as a ""primary disease;"" a designation used as a rationale for some National Institutes of Health (e.g., the National Heart Lung and Blood Institute) to exclude research focused on autistic populations from grant funding review, and the pressure on early career researchers by academic institutions (that receive NIH funding) to either change their research topic or their population for NIH grant applications and to avoid challenging NIH policies until they are at a more advanced stage in their career. In the United States, the Trump administration supported restrictive immigration policies that discriminated against autistic people. Under these policies, autistic immigrants faced deportation.[14][15] In Canada, autistic immigrants have been denied citizenship or they have faced deportation due to the perception that they are a ""burden"" upon the Canadian medical system. In 2018, reforms in Canadian immigration law were announced, the reforms are supposed to make it easier for autistic and disabled immigrants to migrate to Canada.[16] New Zealand effectively prohibits the immigration of autistic people.[17] Australia's health criteria have prompted criticism about its immigration policies. Australia forbids the immigration of people who would be exceptionally costly for the nation's health care or social services. People with autism may be subject to this policy.[18][neutrality is disputed] Singapore, which is renowned for having rigorous rules and regulations, also has severe immigration laws. According to these laws, prospective immigrants with long-term medical issues, such as autism, could be turned away by the country's immigration officials.[19]"
Baker Roll,/wiki/Baker_Roll,"The Baker Roll of the Eastern Band of Cherokee Indians was created by the Eastern Cherokee Enrolling Commission after it was commissioned by the United States Congress on June 4, 1924. The purpose of the Baker Roll was to collect and compile data from older Eastern Cherokee censuses and determine tribal affiliation. The roll is named after Special Agent Fred A. Baker.[1] In order for a person to be or become a citizen of the Eastern Band of Cherokee Indians, they must: Blood quantum is traced from the ancestor listed on the 1924 Baker Roll. A person with a blood quantum of less than 1/16th is an Eastern Band Cherokee descendant, but not a tribal citizen. The Eastern Band Cherokee nation does not allow DNA testing to be used to determine tribal citizenship, unless the test is to determine parentage.[2][3] Individuals who are not citizens of the Eastern Band but who believe they have Eastern Band heritage can fill out a Baker Roll search form with the Eastern Band's tribal enrollment office. The form must list the applicant's ancestors as well as their dates of birth and death. Applicants must also submit a DNA test to prove a biological connection to a recent ancestor, such as a parent or a grandparent, who was an enrolled citizen.[4]"
Bamboo ceiling,/wiki/Bamboo_ceiling,"The term ""bamboo ceiling"" was coined and popularized in 2005 by Jane Hyun in Breaking the Bamboo Ceiling: Career Strategies for Asians, where she addresses the barriers faced by many Asian Americans in the professional arena, such as stereotypes and racism, while also providing solutions to overcome these barriers.[1][2] The bamboo ceiling, as defined by Jane Hyun, is a combination of individual, cultural, and organizational factors that impede Asian Americans' career progress inside organizations. Since the publication of Hyun's book, a variety of sectors (including nonprofits, universities, and the government) have discussed the impact of the ceiling as it relates to people of Asian descent and the challenges they face. As described by a senior writer at Fortune magazine, ""bamboo ceiling"" refers to the processes and barriers that serve to exclude Asians and Asian-Americans from executive positions on the basis of subjective factors such as ""lack of leadership potential"" and ""lack of communication skills"" that cannot actually be explained by job performance or qualifications.[3] Articles regarding the subject have been written in Crains, Fortune, The Atlantic and Forbes (2016).[4][5][6] The term is a derivative of the glass ceiling, which refers to the more general metaphor used to describe invisible barriers through which people of marginalized genders, and/or Black, Indigenous, and racialized peoples can see managerial positions, but cannot reach them. Similar metaphor includes canvas ceiling[7] posed on refugees and their workforce integration efforts. Based on publicly available government statistics, Asian Americans have the lowest chance of rising to management when compared with African Americans, Hispanics, and women in spite of having the highest educational attainment.[8][9][10][11][12][13] The Civil Rights Act of 1964 prohibited discrimination on the basis of race. However, covert forms of racism persist in the workforce. The Census Bureau reports that Asian Americans have the highest education levels of any racial category in the United States. Of Asian Americans, 52.4% are college graduates, while the national average is 29.9%.[14] The bamboo ceiling in the United States is a subtle and complex form of discrimination, and the umbrella term ""Asian American"" extends to include a number of diverse groups, including South Asians, East Asians, and Southeast Asians. These groups are often subject to ""model minority"" stereotypes, and viewed as quiet, hardworking, family-oriented, high achieving in math and science, passive, non-confrontational, submissive, and antisocial.[28] In the workforce, some of these perceptions may seem positive in the short-term, but in the long-term they impede progression up the corporate and academic ladders. While Asian Americans are often viewed as a ""model minority"" race, many feel that they are an invisible or ""forgotten minority"", despite being one of the fastest growing groups in the country.[29] Because they are generally considered ineligible for many of the minority rights of underrepresented races, and Asian Americans have been shown to be less likely to report incidents of racial discrimination in the workplace, there are far fewer institutional avenues and programs for them to combat these labels and perceptions.[30] Some analysts attribute the racial disparity in administrative capacities to negative extensions of the aforementioned stereotypes of Asian Americans, such as common assumptions that they are ""lacking in leadership skills"" or that they have ""poor communication abilities"".[31] Asian Americans are also sometimes expected to have higher qualifications than their white counterparts, such as graduating from more prestigious universities, to achieve the same positions in American companies.[15][31] Many of these stereotypes and expectations have a basis in cultural misunderstandings. Some Asian Americans claim that they are raised with culture-specific values that affect perceptions of their workplace behaviour. For example, some report being taught from an early age to be self-effacing, reticent, respectful, and deferential towards authority.[3][31] These values do not translate well into the American workplace, where Asian Americans' respectfulness can be misinterpreted as aloofness, arrogance, and inattentiveness.[3] As a result, Asian Americans are less likely to be seen as having qualities that appeal to American employers, such as leadership, charisma and risk-taking, and are often passed over for promotions in spite of satisfactory or high job performance. Asian Americans are also less likely to aggressively network, self-promote, and speak up at work meetings with concerns and ideas when compared to their coworkers.[3] Whilst Asian Americans who do are received negatively. [32][33] Another factor may be an existing lack of connections and Asian American role models in upper management and in politics. Until relatively recently with the Civil Rights Movement, a large number of individuals of Asian descent had few political and social rights, or were denied rights of citizenship by naturalisation. While many Asian Americans are active in political life and government positions today, their representation is still disproportionately small, and there remain unofficial barriers to political access.[34] A survey that was taken revealed that while 83% of Asian Americans felt loyal to their jobs, only 49% felt as though they belonged in the American workforce.[35] According to researchers that study diversity and talent management, 25% of Asians surveyed said ""they had felt workplace discrimination because of their ethnicity.""[36] Asian American men, more than any other demographic, said they felt stalled in their careers and were more likely to quit their current jobs to search for advancement elsewhere.[36] In another survey, 66% of Asian American men and 44–50% of Asian American women said they felt their careers had stalled, showing that not only do Asian Americans face large amounts of workplace discrimination, but also that Asian American men are discriminated against more than Asian American women by a wide margin, revealing a significant gender disparity.[37] AAPI of marginalized genders face additional barriers as a result of being both Asian American and not a cis man.[38] Articles estimate that, on average, Asian American women earned 27–40% less than Asian American men, the highest of any racial group.[12][13] Another commonly cited barrier, complementary to the bamboo ceiling, is the ""sticky floor"". When applied to the Asian American experience, the sticky floor refers to the phenomenon by which young professionals of Asian descent are often trapped in low-level, low-mobility jobs.[30] Asian Americans graduate from universities in high numbers, and firms tend to hire them in high numbers as well. However, within a few years, many claim to find themselves pigeonholed into dead-end careers with no path for advancement to upper-level corporate careers.[31] This process is visible across a number of fields, including business,[3] academia,[28] and law.[31] Even in areas where Asian Americans are believed to excel, such as software engineering, there is an overall tendency to see them assigned to low-ranking positions with fewer opportunities for advancement compared to other racial groups.[original research?] The bamboo ceiling is an issue that many Asian Americans face within the workplace. It is a mix of individual, cultural, and organizational factors that hinder the growth and success of these people in office settings, mainly managerial roles. Stereotypes about Asian Americans' personality characteristics and cultural values may place them at a disadvantage in the workplace.[original research?] A psychological experiment was done by two researchers on the bamboo ceiling, and their findings revealed that East Asians who do not conform to racial stereotypes of Asians and possess qualities such as assertiveness, dominance, and leadership skills are less likely to be popular in the workplace, with one researcher stating that ""[i]n general, people do not want dominant co-workers, but they really do not want to work with a dominant East-Asian co-worker.""[39][40] Wesley Yang tried to define what the force is that has held Asian Americans back, and does so by communicating that Asian Americans have a hard time with the networking and highlighting of their own accomplishments, as well as with challenging authority. He adds that Asian Americans tend to be culturally trained to be less flamboyant in the aforementioned skills, which slightly limits their ability to rise above the field in certain professions. To be successful within a managerial role or in a corporate position, it is important for an individual to know how to promote themselves in order to get ahead, but, as Yang and Hyun explain, there are cultural nuances that impede upward mobility for Asian Americans.[41] Contrary to popular beliefs, Asian Americans do openly ask for the professional rewards they feel that they deserve, but despite their overwhelming desire to climb higher on the corporate ladder as well as the American workforce in general, Asian Americans hit barriers that prevent them from doing so.[36] The bamboo ceiling is a socially recognized phenomenon, and a number of different methods have been proposed to help address the issue. People also advise that finding a mentor can be beneficial. Mentors are a resource commonly used by other minority groups, such as Hispanics and African Americans, to give advice and be a person to talk to about the issues being faced by their community. Groups such as Asian Pacific Americans in Higher Education and Leadership Education for Asian Pacifics, Inc. provide resources for mentorship.[42][43] For example, Linda Akutagawa, founder of Leadership Education for Asian Pacifics, Inc., explains how her company provides leadership training for Asian employees, in addition to their mentorship service.[44] However, some people argue that it should not be the responsibility of Asian Americans to bridge the gap between the differences in their culture and the environment of the standard workplace. So there is an ongoing debate between those who believe that personal adaption is the best solution and those that there are things that the business can do to fix this issue.[45] Some companies have leadership programs designed for Asian Americans to cultivate skills and to help translate values across cultures.[25] Among these inclusive companies is Cisco, which recently created an Advanced Leadership Program for Asian-American Executives at the Stanford Graduate School of Business. The program charges $11,000 for a five-day session.[46] Instead of training Asian Americans to ""be more white,"" some argue that instead, Asian Americans can learn to leverage their cultures and values rather than hiding them.[47] In 2020, a series of studies conducted by researchers at MIT Sloan School of Management (Jackson G. Lu), University of Michigan (Richard E. Nisbett), and Columbia Business School (Michael W. Morris), re-investigated the Bamboo-ceiling issue in the United States. Published in PNAS, the research highlighted the importance of differentiating Americans of East Asian and South Asian descent.[48] The research found that South Asian Americans' leadership attainment in US is significantly higher than that of East Asians. In addition, South Asians are more likely than Whites to attain leadership positions, as shown by data on the ethnicities of S&P 500 companies' CEOs and senior leadership positions. All findings controlled for demographics factors including English fluency, age, gender, education, tenure at company, personality, home country, and GDP per capita of cultural origin.[citation needed] Moreover, according to the research, cultural differences in assertiveness is the main reason why East Asian Americans hit the bamboo ceiling, while South Asian Americans are able to transcend it. Compared to South Asians, East Asians are less likely to speak up, engage in constructive debates, and stand their own grounds in conflicts. The difference in culture mediated the leadership attainment gap between East Asians and South Asians in the United States. Thus, the researchers suggest that Bamboo Ceiling is not an ""Asian issue"", but an issue of cultural fit with the US prototype of leadership.[citation needed] In 2014, Asians made up 5.6% of the U.S. population and accounted for 5.3% of people and characters shown on film.[49] In 2015, ABC Sitcom Fresh off the Boat aired on television. This was the first time in 20 years that a show featuring predominantly Asian-Americans had been on national TV, the last one being All-American Girl in 1994, which was cancelled after one season.[50] A major issue in the media industry has been the concept of ""white-washing,"" where white actors and actresses are cast in roles portraying people of color. In 2015, Emma Stone was cast as Allison Ng in the romantic-comedy Aloha. Allison was supposed to be a quarter Hawaiian and a quarter Chinese, but her appearance in the film did not reflect her heritage. This casting choice led to an outcry in the community; the movie was met with negative reviews and a disappointing box office performance, which may have been due in part to the response to the casting choice.[51] Matt Damon was cast as the lead role in the action movie The Great Wall. Actress Constance Wu from Fresh off the Boat chimed in, stating that Hollywood will find a way to cast white actors no matter what – even in a film set in China 1,000 years ago.[52] In the United States, Asians are stereotyped as being physically and athletically inferior to other races.[53] This has led to much discrimination in the recruitment process of professional American sports where Asian American athletes are now highly underrepresented.[54][55][56][57] In 2015, despite making up 5.6% of the nation's population, Asian American athletes only represented 1.1% of the NFL and 1.2% of the MLB, and as of 2014 0.2% of the NBA.[58][59][60] Basketball is a sport noted for its low number of Asian athletes, despite the fact that the sport's color barrier was broken by an Asian American named Wataru Misaka athlete in 1947. Misaka was the first person of color to play in the NBA.[61] The Utah native played for the New York Knicks. In American sports, there are and has been a higher representation of Asian American athletes who are of mixed racial heritage in comparison to those of full racial heritage. For instance, former football player Roman Gabriel was the first Asian-American to start as an NFL quarterback and was only of half Southeast Asian descent (Filipino). In 2016, 2.6% (14/535) of the members of Congress were Asian Americans, compared to 5.6% of the total population in the United States. Proportional representation would necessitate 30 members out of the 535 in order to mirror the US population.[62] As of 2015, less than 2% of Fortune 500 CEOs are Asian, although they comprise 5.6% of the total United States population, and about 30% of the graduates in the top 20 MBA programs in the last 20 years.[63] Aside from corporate underrepresentation, some believe there is also a bias against students applying to colleges.[64] This existing bias is explained by professor of Public Policy at University of California Riverside, Karthick Ramakrishan, as having a predetermined and singular idea of what an Asian student should be like. This idea of expecting a specific kind of Asian is referred to as being the ""model minority"". This concept can be explained as a driving factor in admissions process that hurts Asian Americans by having a certain expectation for these students because they are believed to be the ""model minority"". According to model minority, if an Asian student does not fit into the stereotypical mold, they are seen as useless to the college.[65] Asian-American organizations asked the Department of Education in 2016 to investigate Brown University, Dartmouth College and Yale University, alleging they discriminate against Asian-American students during the admissions process.[66] In other parts of the educational field, Asian Americans are underrepresented in the areas of college admissions and university leadership positions. In addition to this existing bias, there is also the marginalization of Asian American academics in positions of power with only 1% of them reaching the status of university presidents and chancellors despite 7% of the faculty being made up of people who are of Asian descent. In a study conducted by Bryan S. K. Kim, Donald R. Atkinson, and Peggy H. Yang, these disproportionate numbers were attributed to values commonly seen in Asian households that were passed down from parents to children even when living in the United States. These values included ""deference to authority figures, respect for elders, self-effacement, [and,] restraint"". (Khator, Renu) These practices commonly associated with people of Asian heritage allow for a strong bias to grow against Asian Americans in higher up university positions as administrators begin to believe that they ""either have no interest in leadership opportunities or simply won't be good at it"" because the traits associated with Asian Americans do not adhere to typical western ideals of leadership (Oguntoyinbo 10). Many Asian Americans such as Frank Wu, chancellor and dean of the University of California Hastings College of the Law, are frustrated with stereotypes like these due to several contradicting facts such as ""Asian Americans [being] overrepresented in the academy as students, graduate students and faculty members"" and still being barely represented (or sometimes even considered) for upper level academic positions.[42] In an article by Peggy Li, this topic is addressed as she presents an argument stating that neither of the movements to break down the barriers of the glass or bamboo ceiling works in favor of Asian American women because of these intersecting identities. This unspoken patriarchal hierarchy leads to the exclusion of AAPI im/migrant sex workers within Asian communities. The classism and misogyny in Asian communities are being pushed to the side in order to focus on issues that impact cis men in leadership roles and cis men-dominated fields (business or politics). Li continues by attributing this lack of intersectionality to the leadership of the movements. According to Li, in the fight to break the bamboo ceiling, men are making a majority, if not all, the decisions.[38] Asians are disproportionally represented in highly paid jobs such as education, medical and tech fields.[67] While Asians only account for 5.6% of the US population, they hold a little less than 19% of management positions in the Silicon Valley tech industry. However, Asians make up more than 50.1% percent of the Silicon Valley tech industry as a whole.[68] This means that while they may be overrepresented in management positions compared to their prevalence in the US population, they are still less likely to advance from a lower position."
Patricia Banks Edmiston,/wiki/Patricia_Banks_Edmiston," Patricia Noisette Banks Edmiston (born April 27, 1937)[1] is an American who was one of the first Black flight attendants.[2] She combated discriminatory practices in the United States by initiating a legal action against Capital Airlines (merged into United Airlines in 1961) via the New York State Commission Against Discrimination. She won the case which lead to the start of more airlines employing Black women. Following her employment as a flight attendant, Banks Edmiston went on to work various roles in the substance abuse prevention industry. She continued her education and received a bachelor's degree from Empire State College. She served on the board of the Black Flight Attendants of America, and in 2010 was accepted into the Black Aviation Hall of Fame. Patricia Noisette Banks was born in New York City on April 27, 1937 to parents Sadie and Joseph Banks.[3] She graduated from Aquinas High School in 1955.[3] Banks Edmiston attended Queens College for a year to study psychology.[1] While attending Queens College, she read an article about Grace Downs Air Career School.[1] Banks Emiston knew ""African-American people didn't have the opportunity to travel that much,"" and thought it would be a promising opportunity.[1] She applied and was accepted into Grace Downs Air Career School in 1956.[1][3] After completing her training to be a stewardess, Banks Edmiston sought employment in various airline companies. However, she encountered consistent rejections,[1][4] which was not uncommon for Black women in similar positions at the time.[5] She was interviewed by Mohawk Airlines, Trans World Airlines, and Capital Airlines, but was not selected to move forward in the hiring process.[6] During the screening process for Capital Airlines, Banks Edmiston initially received a rating of ""B+"" from the chief hostess, which the company considered to be ""accepted for future employment"", but her application was later nullified at the request of the director of passenger service.[4] Banks Edmiston was told by a chief hostess at Capital Airlines that ""the company didn't hire black people in flight capacities"".[1] At the time, pilots, engineers, and stewardesses were rarely Black.[7] In 1957, after seeking advice from Adam Clayton Powell Jr., the only Black Congressman from New York at the time,[8] Banks Edmiston decided to take legal action against Capital Airlines. She did so by filing a comprehensive 72-page complaint with the New York State Commission Against Discrimination.[9][10][11] The airline, in its defense, contended that the complaint was null and void due to its age, surpassing the 90-day statute of limitations.[12] Furthermore, they claimed that they did not conduct hiring operations in New York.[12] They insisted that they do not discriminate and that the reason they refused to hire Banks Edmiston was not because of her race, but rather because of her lack of experience and ""because she had a bad tooth"".[13][12] In response to the airline's arguments, Banks Edmiston brought to the committee's attention that Captial Airline's discrimination was ongoing, negating the expiration of the statute of limitations.[12] She also pointed out that the airline regularly recruits and hires from the Grace Downs School in New York,[14] her alma mater, thereby establishing her qualifications.[15] Lastly, Banks Edmiston argued that Capital Airlines indeed engaged in discrimination, noting ""they have 570 employed hostesses and not one is a Negro"", a pattern that extended to all 1,350 flight-related employees.[15][10] ""The historical weight is beautiful, but I think the most important part is that the barrier was broken. I always said it didn't have to be me, but that it was going to be a Black woman."" –Patricia Banks Edmiston[1] After the three-year legal battle, the Commission ruled in 1960 that the airline had illegally discriminated against Banks Edmiston because of her race and required them to offer her employment.[4] They also ordered Capital Airlines to ""cease and desist from maintaining a policy of barring negroes from employment because of their color, in all flight capacities, including that of flight hostess.""[7] The case has been called a ""hallmark legal proceeding that revolutionized the industry"",[5] and is widely credited with leading to the start of other airlines hiring Black women.[16] Her winning of the case resulted in harassment and violent threats at the time, to the point that Edmiston had to seek law enforcement assistance to safeguard her well-being.[5] In May 1960, four years after first applying to Capital Airlines, Banks Edmiston started her position as a stewardess.[4][17] She was the first African-American stewardess at the airline.[7] The stress of experiencing racial discrimination while flying in the Southern United States took a toll on Edmiston.[5] This, along with the pressure she felt to maintain a flawless record in order to preserve future prospects for other Black flight attendants, caused Edmiston to resign from Capital Airlines in 1961.[5][3] From 1970 to 1972, Banks Edmiston served as a counselor at New York City's Addicts Rehabilitation Center. Following that, she was a program manager at the New York City Manpower Planning Council. Banks Edmiston assumed a role as a program manager within the New York State Office of Alcohol and Substance Abuse Services in 1974. In 1975 she achieved a Bachelor of Arts degree in psychology through Empire State College.[3] In 1999, she returned to the Addicts Rehabilitation Center where she acted as a consultant until 2015. Emidston also dedicated her expertise as a member of the board of directors for the Black Flight Attendants of America. Between 1999 and 2001, Banks Edmiston worked for the American Red Cross as captain of the disaster team. Her professional endeavors also extended to her involvement with American Airlines Medical Wings International from 2000 to 2002. In her personal life, Banks Edmiston has practiced Shotokan, a style of karate in which she holds a black belt.[3] On August 5, 2010, Banks Edmiston was inducted into the Black Aviation Hall of Fame at the National Civil Rights Museum for her world class contributions to aviation.[18] She has additionally been honored by the Smithsonian.[1]"
Black Codes (United States),/wiki/Black_Codes_(United_States),"The Black Codes, sometimes called the Black Laws, were laws which governed the conduct of African Americans (both free and freedmen). In 1832, James Kent wrote that ""in most of the United States, there is a distinction in respect to political privileges, between free white persons and free colored persons of African blood; and in no part of the country do the latter, in point of fact, participate equally with the whites, in the exercise of civil and political rights.""[1] Although Black Codes existed before the Civil War and although many Northern states had them, the Southern U.S. states codified such laws in everyday practice. The best known of these laws were passed by Southern states in 1865 and 1866, after the Civil War, in order to restrict African Americans' freedom, and in order to compel them to work for either low or no wages. Since the colonial period, colonies and states had passed laws that discriminated against free Blacks. In the South, these were generally included in ""slave codes""; the goal was to suppress the influence of free blacks (particularly after slave rebellions) because of their potential influence on slaves. Restrictions included prohibiting them from voting (North Carolina had allowed this before 1831),[citation needed] bearing arms, gathering in groups for worship, and learning to read and write. The purpose of these laws was to preserve slavery in slave societies. Before the war, Northern states that had prohibited slavery also enacted laws similar to the slave codes and the later Black Codes: Connecticut, Ohio, Illinois, Indiana, Michigan,[2] and New York enacted laws to discourage free blacks from residing in those states. They were denied equal political rights, including the right to vote, the right to attend public schools, and the right to equal treatment under the law. Some of the Northern states, those which had them, repealed such laws around the same time that the Civil War ended and slavery was abolished by constitutional amendment. In the first two years after the Civil War, white legislatures passed Black Codes modeled after the earlier slave codes. (The name ""Black Codes"" was given by ""negro leaders and the Republican organs"", according to historian John S. Reynolds.[3][4][5]) Black Codes were part of a larger pattern of Democrats trying to maintain political dominance and suppress the freedmen, newly emancipated African-Americans. They were particularly concerned with controlling movement and labor of freedmen, as slavery had been replaced by a free labor system. Although freedmen had been emancipated, their lives were greatly restricted by the Black Codes. The defining feature of the Black Codes was broad vagrancy law, which allowed local authorities to arrest freedpeople for minor infractions and commit them to involuntary labor. This period was the start of the convict lease system, also described as ""slavery by another name"" by Douglas Blackmon in his 2008 book of this title.[6] Vagrancy laws date to the end of feudalism in Europe. Introduced by aristocratic and landowning classes, they had the dual purpose of restricting access of ""undesirable"" classes to public spaces and of ensuring a labor pool. Serfs were not emancipated from their land.[7] ""Black Codes"" in the antebellum South strongly regulated the activities and behavior of blacks, especially free Blacks, who were not considered citizens. Chattel slaves basically lived under the complete control of their owners, so there was little need for extensive legislation. ""All Southern states imposed at least minimal limits on slave punishment, for example, by making murder or life-threatening injury of slaves a crime, and a few states allowed slaves a limited right of self-defense.""[8] As slaves could not use the courts or sheriff, or give testimony against a white man, in practice these meant little. North Carolina restricted slaves from leaving their plantation; if a male slave wished to court a female slave on another property, he needed a pass in order to pursue this relationship. Without one he risked severe punishment at the hands of the patrollers.[9] Free blacks presented a challenge to the boundaries of white-dominated society.[10] In many Southern states, particularly after Nat Turner's insurrection of 1831, they were denied the rights of citizens to assemble in groups, bear arms, learn to read and write, exercise free speech, or testify against white people in Court.[11][12][13][8] After 1810, states made manumissions of slaves more difficult to obtain, in some states requiring an act of the legislature for each case of manumission. This sharply reduced the incidence of planters freeing slaves.[8] All the slave states passed anti-miscegenation laws, banning the marriage of white and black people. Between 1687 and 1865, Virginia enacted more than 130 slave statutes, among which were seven major slave codes, with some containing more than fifty provisions.[14] Slavery wus a bad thing en' freedom, of de kin' we got wid nothin' to live on wus bad. Two snakes full of pisen. One lying wid his head pintin' north, de other wid his head pintin' south. Dere names wus slavery an' freedom. De snake called slavery lay wid his head pinted south and de snake called freedom lay wid his head pinted north. Both bit de nigger, an' dey wus both bad. Maryland passed vagrancy and apprentice laws, and required Blacks to obtain licenses from Whites before doing business.[17] It prohibited immigration of free Blacks until 1865.[18] Most of the Maryland Black Code was repealed in the Constitution of 1867. Black women were not allowed to testify against white men with whom they had children, giving them a status similar to wives.[18] As the abolitionist movement gained force and the Underground Railroad helped fugitive slaves escape to the North, concern about Black people heightened among Northern white people. Territories and states near the slave states did not welcome freed Black people. But north of the Mason–Dixon line, anti-Black laws were generally less severe. Some public spaces were segregated, and Black people generally did not have the right to vote.[11] In Oregon, Black people were forbidden to settle, marry, or sign contracts.[19] In Ohio, Black people required a certificate they were free and a good behavior bond.[20] All the slave states passed anti-miscegenation laws, banning the marriage of white and Black people, as did several new free states of the former Northwest Territory, including Indiana, Illinois, and Michigan.[21] Ohio, Indiana, and Illinois shared borders with the slave states across the Ohio and Mississippi rivers (Kentucky, Missouri, and Virginia respectively). The population of the southern parts of these states had generally migrated from the Upper South; their culture and values were more akin to those of the South across the river than those of the northern settlers, who had migrated from New England and New York. In some states these codes included vagrancy laws that targeted unemployed Black people, apprentice laws that made Black orphans and dependents available for hire to white people, and commercial laws that excluded Black people from certain trades and businesses and restricted their ownership of property.[22] The Indiana Legislature decreed in 1843 that only white students could attend the public schools.[23] Article 13 of Indiana's 1851 Constitution banned Black people from settling in the state. Anyone who helped Black people settle in the state or employed Black settlers could be fined. Article 13 had the most popular vote among Hoosiers compared to all other articles voted upon. The Supreme Court declared Article 13 invalid in 1866.[24] The 1848 Constitution of Illinois contributed to the state legislature passing one of the harshest Black Code systems in the nation until the Civil War. The Illinois Black Code of 1853 prohibited any Black persons from outside of the state from staying in the state for more than ten days, subjecting Black people who violated that rule to arrest, detention, a $50 fine, or deportation.[25] However, while slavery was illegal in Illinois, landowners in the southern parts of the state would legally bring in slaves from adjacent Kentucky, and force them to do agricultural work for no wages. They had to be removed from the state for one day each year, thus preventing them from being citizens of Illinois and receiving the protection of its laws. A campaign to repeal these laws was led by John Jones, Chicago's most prominent black citizen. In December 1850, Jones circulated a petition—signed by black residents of the state—for Illinois legislators to repeal the Black Laws.[26] In 1864, the Chicago Tribune published Jones’ pamphlet, “The Black Laws of Illinois and a Few Reasons Why They Should Be Repealed.” It was not until 1865 that Illinois repealed the state's provision of its Black Laws.[27] In some states, Black Code legislation used text directly from the slave codes, simply substituting Negro or other words in place of slave.[28][29] The Union Army relied on the labor of newly freed people, and did not always treat them fairly. Thomas W. Knox wrote: ""The difference between working for nothing as a slave, and working for the same wages under the Yankees, was not always perceptible.""[30] At the same time, military officials resisted local attempts to apply pre-war laws to the freed people.[31] After the Emancipation Proclamation, the Army conscripted Black ""vagrants"" and sometimes others.[32] The Union Army applied the northern wage system of free labor to freedmen after the Emancipation Proclamation; they effectively upgraded free Blacks from ""contraband"" status. General Nathaniel P. Banks in Louisiana initiated a system of wage labor in February 1863 in Louisiana; General Lorenzo Thomas implemented a similar system in Mississippi.[33][34] The Banks-Thomas system offered Blacks $10 a month, with the Army's commitment to provide rations, clothing, and medicine. The worker would have to agree to an unbreakable one-year contract.[34] In 1864, Thomas expanded the system to Tennessee, and allowed white landowners near the Nashville contraband camp to rent the labor of refugees.[35] Against opposition from elements of the Republican Party, Abraham Lincoln accepted this system as a step on the path to gradual emancipation.[33] Abolitionists continued to criticize the labor system. Wendell Phillips said that Lincoln's proclamation had ""free[d] the slave, but ignore[d] the Negro"", calling the Banks-Thomas year-long contracts tantamount to serfdom. The Worcester Spy described the government's answer to slavery as ""something worse than failure.""[33][36] As the war ended, the U.S. Army implemented Black Codes to regulate the behavior of black people in general society. Although the Freedmen's Bureau had a mandate to protect blacks from a hostile Southern environment, it also sought to keep blacks in their place as laborers in order to allow production on the plantations to resume so that the South could revive its economy.[37] The Freedmen's Bureau cooperated with Southern authorities in rounding up black ""vagrants"" and placing them in contract work.[38][39][40] In some places, it supported owners to maintain control of young slaves as apprentices.[41] New restrictions were placed on intermarriage, concubinage, and miscegenation with Black people in Arizona in 1864, California in 1880, Colorado in 1864, Florida[when?], Indiana in 1905, Kentucky in 1866, Montana in 1909, Nebraska in 1865, Nevada in 1912, North Dakota in 1943, Ohio 1877, Oregon in 1867, Rhode Island in 1872, South Dakota in 1913, Tennessee in 1870, Texas in 1858, Utah in 1888, Virginia[when?], Washington in 1866 but promptly repealed it in 1867, West Virginia in 1863 but overturned by Loving v Virginia in 1967, and Wyoming in 1908. In all, twenty-one states put in place Jim Crow laws against miscegenation. Free whites could no longer marry a slave and thereby emancipate her and her children,[further explanation needed] and no freedman could receive a donation or inheritance from a white person.[14] Soon after the end of slavery, white planters encountered a labor shortage and sought a way to manage it. Although blacks did not all abruptly stop working, they did try to work less. In particular, many sought to reduce their Saturday work hours, and women wanted to spend more time on child care.[42] In the view of one contemporary economist, freed people exhibited this ""noncapitalist behavior"" because the condition of being owned had ""shielded the slaves from the market economy"" and they were therefore unable to perform ""careful calculation of economic opportunities"".[43] An alternative explanation treats the labor slowdown as a form of gaining leverage through collective action.[44] Another possibility is that freed blacks assigned value to leisure and family time in excess of the monetary value of additional paid labor. Indeed, freedpeople certainly did not want to work the long hours that had been forced upon them for their whole lives.[45] Whatever its causes, the sudden reduction of available labor posed a challenge to the Southern economy, which had relied upon intense physical labor to profitably harvest cash crops, particularly cotton.[46] Southern Whites also perceived Black vagrancy as a sudden and dangerous social problem.[47][48] Preexisting White American belief of Black inferiority informed post-war attitudes and White racial dominance continued to be culturally embedded; Whites believed both that Black people were destined for servitude and that they would not work unless physically compelled.[48] The enslaved strove to create a semi-autonomous social world, removed from the plantation and the gaze of the slave owner.[49] The racial divisions that slavery had created immediately became more obvious.[50] Blacks also bore the brunt of Southern anger over defeat in the War.[50] Legislation on the status of freedpeople was often mandated by constitutional conventions held in 1865. Mississippi, South Carolina, and Georgia all included language in their new state constitutions which instructed the legislature to ""guard them and the State against any evils that may arise from their sudden emancipation"".[51] The Florida convention of October 1865 included a vagrancy ordinance that was in effect until process Black Codes could be passed through the regular legislative process.[52] Black Codes restricted black people's right to own property, conduct business, buy and lease land, and move freely through public spaces.[53] A central element of the Black Codes were vagrancy laws. States criminalized men who were out of work, or who were not working at a job whites recognized.[48][examples needed] Failure to pay a certain tax, or to comply with other laws,[examples needed] could also be construed as vagrancy.[54] Nine Southern states updated their vagrancy laws in 1865–1866. Of these, eight allowed convict leasing (a system in which state prison hired out convicts for labor) and five allowed prisoner labor for public works projects.[55] This created a system that established incentives to arrest black men, as convicts were supplied to local governments and planters as free workers. The planters or other supervisors were responsible for their board and food, and black convicts were kept in miserable conditions. As Douglas Blackmon wrote, it was ""slavery by another name"".[6] Because of their reliance on convict leasing, Southern states did not build any prisons until the late 19th century. Another important part of the Codes were the annual labor contracts, which Black people had to keep and present to authorities to avoid vagrancy charges.[39][53] Strict punishments against theft also served to ensnare many people in the legal system. Previously, Blacks had been part of the domestic economy on a plantation, and were more or less able to use supplies that were available. After emancipation, the same act performed by someone working the same land might be labeled as theft, leading to arrest and involuntary labor.[56] Some states explicitly curtailed Black people's right to bear arms, justifying these laws with claims of imminent insurrection.[57][58] In Mississippi and Alabama, these laws were enforced through the creation of special militias.[59] Historian Samuel McCall, who published a biography of abolitionist Thaddeus Stevens, commented in 1899 that the Black Codes had ""established a condition but little better than that of slavery, and in one important respect far worse"": by severing the property relationship, they had diminished the incentive for property owners to ensure the relative health and survival of their workers.[60] Regarding the question of whether Southern legislatures deliberately tried to maintain white supremacy, Beverly Forehand writes: ""This decision was not a conscious one on the part of white legislators. It was simply an accepted conclusion.""[61] During Reconstruction, state legislatures passed some laws that established some positive rights for freedmen. States legalized Black marriages and in some cases increased the rights of freedmen to own property and conduct commerce.[58] The Black Codes outraged public opinion in the North because it seemed the South was creating a form of quasi-slavery to negate the results of the war.[62] When the Radical 39th Congress reconvened in December 1865, it was generally furious about the developments that had transpired during Johnson's Presidential Reconstruction. The Black Codes, along with the appointment of prominent Confederates to Congress, signified that the South had been emboldened by Johnson and intended to maintain its old political order.[63] Railing against the Black Codes as returns to slavery in violation of the Thirteenth Amendment, Congress passed the Civil Rights Act of 1866, the Fourteenth Amendment, and the Second Freedmen's Bureau Bill.[64] The Memphis Riots in May 1866 and the New Orleans Riot in July brought additional attention and urgency to the racial tension state-sanctioned racism permeating the South.[64] After winning large majorities in the 1866 elections, the Republican Congress passed the Reconstruction Acts, placing the South under military rule. This arrangement lasted until the military withdrawal arranged by the Compromise of 1877.[54] In some historical periodizations, 1877 marks the beginning of the Jim Crow era.[a] The 1865–1866 Black Codes were an overt manifestation of the system of white supremacy that continued to dominate the American South.[68] Historians have described this system as the emergent result of a wide variety of laws and practices, conducted on all levels of jurisdiction.[69] Because legal enforcement depended on so many different local codes, which underwent less scrutiny than statewide legislation, historians still lack a complete understanding of their full scope.[70] It is clear, however, that even under military rule, local jurisdictions were able to continue a racist pattern of law enforcement, as long as it took place under a legal regime that was superficially race-neutral.[71] In 1893–1909, every Southern state except Tennessee passed new vagrancy laws.[54] These laws were more severe than those passed in 1865, and used vague terms that granted wide powers to police officers enforcing the law.[72] An example were the so-called ""Pig Laws"", with harsh penalties for crimes such as stealing a farm animal.[73][74] Pig Laws were solely applied to African Americans related to agricultural crimes.[75] In wartime, Blacks might be disproportionately subjected to ""work or fight"" laws, which increased vagrancy penalties for those not in the military.[76] The Supreme Court upheld racially discriminatory state laws and invalidated federal efforts to counteract them; in Plessy v. Ferguson (1896), it upheld the constitutionality of racial segregation and introduced the ""separate but equal"" doctrine.[77] A general system of legitimized anti-Black violence, as exemplified by the Ku Klux Klan, played a major part in enforcing the practical law of white supremacy. The constant threat of violence against Black people (and White people who sympathized with them) maintained a system of extralegal terror. Although this system is now well known for prohibiting Black suffrage after the Fifteenth Amendment, it also served to enforce coercive labor relations.[78] Fear of random violence provided new support for a paternalistic relationship between plantation owners and their Black workers.[50] Mississippi was the first state to pass Black Codes. Its laws served as a model for those passed by other states, beginning with South Carolina, Alabama, and Louisiana in 1865, and continuing with Florida, Virginia, Georgia, North Carolina, Texas, Tennessee, and Arkansas at the beginning of 1866.[79] Intense Northern reaction against the Mississippi and South Carolina laws led some of the states that subsequently passed laws to excise overt racial discrimination; but, their laws on vagrancy, apprenticeship, and other topics were crafted to effect a similarly racist regime.[80] Even states that carefully eliminated most of the overt discrimination in their Black Codes retained laws authorizing harsher sentences for Black people.[80] Mississippi was the first state to legislate a new Black Code after the war, beginning with ""An Act to confer Civil Rights on Freedmen"". This law allowed Blacks to rent land only within cities—effectively preventing them from earning money through independent farming. It required Blacks to present, each January, written proof of employment. The law defined violation of this requirement as vagrancy, punishable by arrest—for which the arresting officer would be paid $5, to be taken from the arrestee's wages. Provisions akin to fugitive slave laws mandated the return of runaway workers, who would lose their wages for the year.[81][82][83] An amended version of the vagrancy law included punishments for sympathetic whites:[54][82] That all freedmen, free negroes and mulattoes in this State, over the age of eighteen years, found on the second Monday in January, 1866, or thereafter, without lawful employment or business, or found unlawfully assembling themselves together, either in the day or night time, and all white persons so assembling themselves with freedmen, free negroes or mulattoes, or usually associating with freedmen, free negroes or mulattoes, on terms of equality, or living in adultery or fornication with a freed woman, free negro or mulatto, shall be deemed vagrants, and on conviction thereof shall be fined in a sum not exceeding, in the case of a freedman, free negro, or mulatto, fifty dollars, and a white man two hundred dollars, and imprisoned, at the discretion of the court, the free negro not exceeding ten days, and the white man not exceeding six months. Whites could avoid the code's penalty by swearing a pauper's oath. In the case of blacks, however: ""the duty of the sheriff of the proper county to hire out said freedman, free negro or mulatto, to any person who will, for the shortest period of service, pay said fine or forfeiture and all costs.""[82] The laws also levied a special tax on blacks (between ages 18 and 60); those who did not pay could be arrested for vagrancy.[82] Another law allowed the state to take custody of children whose parents could or would not support them; these children would then be ""apprenticed"" to their former owners.[81][84] Masters could discipline these apprentices with corporal punishment.[81] They could re-capture apprentices who escaped and threaten them with prison if they resisted.[83] Other laws prevented blacks from buying liquor and carrying weapons; punishment often involved ""hiring out"" the culprit's labor for no pay.[81] Mississippi rejected the Thirteenth Amendment on December 5, 1865. General Oliver O. Howard, national head of the Freedmen's Bureau, declared in November 1865 that most of the Mississippi Black Code was invalid.[85] The next state to pass Black Codes was South Carolina, which had on November 13 ratified the Thirteenth Amendment—with a qualification that Congress did not have the authority to regulate the legal status of freedmen. Newly elected governor James Lawrence Orr said that blacks must be ""restrained from theft, idleness, vagrancy and crime, and taught the absolute necessity of strictly complying with their contracts for labor"".[86] South Carolina's new law on ""Domestic Relations of Persons of Color"" established wide-ranging rules on vagrancy resembling Mississippi's. Conviction for vagrancy allowed the state to ""hire out"" blacks for no pay. The law also called for a special tax on blacks (all males and unmarried females), with non-paying blacks again guilty of vagrancy. The law enabled forcible apprenticeship of children of impoverished parents, or of parents who did not convey ""habits of industry and honesty"".[86] The law did not include the same punishments for Whites in dealing with fugitives.[87] The South Carolina law created separate courts for Black people, and authorized capital punishment for crimes including theft of cotton.[88] It created a system of licensing and written authorizations that made it difficult for Blacks to engage in normal commerce.[89] The South Carolina Code clearly borrowed terms and concepts from the old slave codes, re-instituting a rating system of ""full"" or ""fractional"" farmhands and often referring to bosses as ""masters"".[90] A ""Colored People's Convention"" assembled at Zion Church in Charleston, South Carolina, to condemn the Codes. In a memorial (petition) to Congress, the Convention expressed gratitude for emancipation and establishment of the Freedmen's Bureau, but requested (in addition to suffrage) ""that the strong arm of law and order be placed alike over the entire people of this State; that life and property be secured, and the laborer as free to sell his labor as the merchant his goods.""[91][92] Some people, meanwhile, thought the new laws did not go far enough. One planter suggested that the new laws would require paramilitary enforcement: ""As for making the negroes work under the present state of affairs it seems to me a waste of time and energy ... We must have mounted Infantry that the freedmen know distinctly that they succeed the Yankees to enforce whatever regulations we can make.""[93] Edmund Rhett (son of Robert Rhett) wrote that although South Carolina might be unable to undo abolition, it should to the utmost extent practicable be limited, controlled, and surrounded with such safe guards, as will make the change as slight as possible both to the white man and to the negro, the planter and the workman, the capitalist and the laborer.[94] General Daniel Sickles, head of the Freedmen's Bureau in South Carolina, followed Howard's lead and declared the laws invalid in December 1865.[85] Even as the legislators passed these laws, they despaired of the forthcoming response from Washington. James Hemphill said: ""It will be hard to persuade the freedom shriekers that the American citizens of African descent are obtaining their rights.""[95] Orr moved to block further laws containing explicit racial discrimination.[96] In 1866, the South Carolina code came under increasing scrutiny in the Northern press and was compared unfavorably to freedmen's laws passed in neighboring Georgia, North Carolina, and Virginia.[97] In a special session held in September 1866, the legislature passed some new laws in concession to the rights of free Blacks. Shortly after, it rejected the Fourteenth Amendment.[98] The Louisiana legislature, seeking to ensure that freedmen were ""available to the agricultural interests of the state"", passed similar yearly contract laws and expanded its vagrancy laws. Its vagrancy laws did not specify Black culprits, though they did provide a ""good behavior"" loophole subject to plausibly racist interpretation. Louisiana passed harsher fugitive worker laws and required blacks to present dismissal paperwork to new employers.[99] State legislation was amplified by local authorities, who ran less risk of backlash from the federal government. Opelousas, Louisiana, passed a notorious code which required freedpeople to have written authorization to enter the town. The code prevented freedpeople from living in the town or walking at night except under supervision of a White resident.[100] Thomas W. Conway, the Freedmen's Bureau commissioner for Louisiana, testified in 1866:[29] Some of the leading officers of the state down there—men who do much to form and control the opinions of the masses—instead of doing as they promised, and quietly submitting to the authority of the government, engaged in issuing slave codes and in promulgating them to their subordinates, ordering them to carry them into execution, and this to the knowledge of state officials of a higher character, the governor and others. ... These codes were simply the old black code of the state, with the word 'slave' expunged, and 'Negro' substituted. The most odious features of slavery were preserved in them. Conway describes surveying the Louisiana jails and finding large numbers of Black men who had been secretly incarcerated. These included members of the Seventy-Fourth Colored Infantry who had been arrested the day after they were discharged.[29] The state passed a harsher version of its code in 1866, criminalizing ""impudence"", ""swearing"", and other signs of ""disobedience"" as determined by whites.[85] Of the Black Codes passed in 1866 (after the Northern reaction had become apparent), only Florida's rivaled those of Mississippi and South Carolina in severity.[101] Florida's slaveowners seemed to hold out hope that the institution of slavery would simply be restored.[102] Advised by the Florida governor and attorney general as well as by the Freedmen's Bureau that it could not constitutionally revoke Black people's right to bear arms, the Florida legislature refused to repeal this part of the codes.[57] The Florida vagrancy law allowed for punishments of up to one year of labor.[103] Children whose parents were convicted of vagrancy could be hired out as apprentices.[104] These laws applied to any ""person of color"", which was defined as someone with at least one Negro great-grandparent, or one-eighth black ancestry.[103] White women could not live with men of color.[103] Colored workers could be punished for disrespecting White employers.[85] The explicit racism in the law was supplemented by racist enforcement discretion (and other inequalities) in the practice of law enforcement and legal systems.[105] In Maryland, a fierce battle began immediately after emancipation (by the Maryland Constitution of 1864) over requiring apprenticeship of young black people. By 1860, 45.6% of the black population in the state was already free. Former slave owners rushed to place the children of freedpeople in multi-year apprenticeships; the Freedmen's Bureau and some others tried to stop them. The legislature stripped Baltimore Judge Hugh Lennox Bond of his position because he cooperated with the Bureau in this matter. Salmon Chase, as Chief Justice of the United States Supreme Court, eventually overruled the Maryland apprentice laws on the grounds of their violation of the Civil Rights Act of 1866.[106] North Carolina's Black Code specified racial differences in punishment, establishing harsher sentences for Black people convicted of rape.[100] The Texas Constitutional Convention met in February 1866, declined to ratify the (already effective) Thirteenth Amendment, provided that Blacks would be ""protected in their rights of person and property by appropriate legislation"" and guaranteed some degree of rights to testify in court.[107] Texas modeled its laws on South Carolina's.[90] The legislature defined ""Negroes"" as people with at least one African great-grandparent.[108] ""Negroes"" could choose their employer, before a deadline. After they had made a contract, they were bound to it. If they quit ""without cause of permission"" they would lose all of their wages.[109] Workers could be fined $1 for acts of disobedience or negligence, and 25 cents per hour for missed work.[109] The legislature also created a system of apprenticeship (with corporal punishment) and vagrancy laws.[110] Convict labor could be hired out or used in public works.[111] ""Negroes"" were not allowed to vote, hold office, sit on juries, serve in local militia, carry guns on plantations, homestead, or attend public schools. Interracial marriage was banned.[108][110] Rape sentencing laws stipulated either capital punishment, or life in prison, or a minimum sentence of five years. Even to commentators who favored the codes, this ""wide latitude in punishment"" seemed to imply a clear ""anti-Negro bias"".[111] Tennessee had been occupied by the Union for a long period during the war. As military governor of Tennessee, Andrew Johnson declared a suspension of the slave code in September 1864. However, these laws were still enforced in lower courts.[112] In 1865, Tennessee freedpeople had no legal status whatsoever, and local jurisdictions often filled the void with extremely harsh Black Codes.[113] During that year, Blacks went from one-fiftieth to one-third of the State's prison population.[113] Tennessee had a particularly urgent desire to re-enter the Union's good graces and end the occupation.[114] When the Tennessee Legislature began to debate a Black Code, it received such negative attention in the Northern press that no comprehensive Code was ever established.[115] Instead, the State legalized Black suffrage and passed a civil rights law guaranteeing Blacks equal rights in commerce and access to the Courts.[116] However, Tennessee society, including its judicial system, retained the same racist attitudes as did other states. Although its legal code did not discriminate against Blacks so explicitly, its law enforcement and criminal justice systems relied more heavily on racist enforcement discretion to create a de facto Black Code.[117] The state already had vagrancy and apprenticeship laws which could easily be enforced in the same way as Black Codes in other states.[118] Vagrancy laws came into much more frequent use after the war.[119] And just as in Mississippi, Black children were often bound in apprenticeship to their former owners.[120] The legislature passed two laws on May 17, 1865; one to ""Punish all Armed Prowlers, Guerilla, Brigands, and Highway Robbers""; the other to authorize capital punishment for thefts, burglary, and arson. These laws were targeted at Blacks and enforced disproportionately against Blacks, but did not discuss race explicitly.[121] Tennessee law permitted Blacks to testify against Whites in 1865, but this change did not immediately take practical effect in the lower courts.[122] Blacks could not sit on juries.[123] Still on the books were laws specifying capital punishment for a Black man who raped a White woman.[124] Tennessee enacted new vagrancy and enticement laws in 1875.[125][71] Kentucky had established a system of leasing prison labor in 1825.[126] This system drew a steady supply of laborers from the decisions of ""negro courts"", informal tribunals which included slaveowners.[127] Free Blacks were frequently arrested and forced into labor.[128] Kentucky did not secede from the Union and therefore gained wide leeway from the federal government during Reconstruction.[129] With Delaware, Kentucky did not ratify the Thirteenth Amendment and maintained legal slavery until it was nationally prohibited when the Amendment went into effect in December 1865.[130] After the Thirteenth Amendment took effect, the state was obligated to rewrite its laws.[131] The result was a set of Black Codes passed in early 1866. These granted a set of rights: to own property, make contracts, and some other innovations.[131] They also included new vagrancy and apprentice laws, which did not mention Blacks explicitly but were clearly directed toward them.[132] The vagrancy law covered loitering, ""rambling without a job"" and ""keeping a disorderly house"".[132] City jails filled up; wages dropped below pre-war rates.[133] The Freedmen's Bureau in Kentucky was especially weak and could not mount a significant response.[134] The Bureau attempted to cancel a racially discriminatory apprenticeship law (which stipulated that only White children learn to read) but found itself thwarted by local authorities.[135] Some legislation also created informal, de facto discrimination against Blacks. A new law against hunting on Sundays, for example, prevented Black workers from hunting on their only day off.[136] Kentucky law prevented Blacks from testifying against Whites, a restriction which the federal government sought to remedy by providing access to federal courts through the Civil Rights Act of 1866. Kentucky challenged the constitutionality of these courts and prevailed in Blyew v. United States (1872).[137] All contracts required the presence of a White witness.[138] Passage of the Fourteenth Amendment did not have a great effect on Kentucky's Black Codes.[139] This regime of white-dominated labor was not identified by the North as involuntary servitude until after 1900.[140] In 1907, Attorney General Charles Joseph Bonaparte issued a report, Peonage Matters, which found that, beyond debt peonage, there was a widespread system of laws ""considered to have been passed to force negro laborers to work"".[68] After creating the Civil Rights Section in 1939, the Federal Department of Justice launched a wave of successful Thirteenth Amendment prosecutions against involuntary servitude in the South. Many of the Southern vagrancy laws remained in force until the Supreme Court's Papachristou v. Jacksonville decision in 1972.[72] Although the laws were defended as preventing crime, the Court held that Jacksonville's vagrancy law ""furnishes a convenient tool for 'harsh and discriminatory enforcement by local prosecuting officials, against particular groups deemed to merit their displeasure.'""[141] Even after Papachristou, police activity in many parts of the United States discriminates against racial minority groups. Gary Stewart has identified contemporary gang injunctions—which target young Black or Latino men who gather in public—as a conspicuous legacy of Southern Black Codes.[142] Stewart argues that these laws maintain a system of white supremacy and reflect a system of racist prejudice, even though racism is rarely acknowledged explicitly in their creation and enforcement.[143] Contemporary Black commentators have argued that the current disproportionate incarceration of African Americans, with a concomitant rise in prison labor, is comparable (perhaps unfavorably) with the historical Black Codes.[144][145] The desire to recuperate the labor of officially emancipated people is common among societies (most notably in Latin America) that were built on slave labor. Vagrancy laws and peonage systems are widespread features of post-slavery societies.[146] One theory suggests that particularly restrictive laws emerge in larger countries (compare Jamaica with the United States) where the ruling group does not occupy land at a high enough density to prevent the freed people from gaining their own.[147] It seems that the United States was uniquely successful in maintaining involuntary servitude after legal emancipation.[148] Historians have also compared the end of the slavery in the United States to the formal decolonization of Asian and African nations. Like emancipation, decolonization was a landmark political change—but its significance, according to some historians, was tempered by the continuity of economic exploitation.[149] The end of legal slavery in the United States did not seem to have major effects on the global economy or international relations.[150] Given the pattern of economic continuity, writes economist Pieter Emmer, ""the words emancipation and abolition must be regarded with the utmost suspicion.""[151]"
Roy Early Blick,/wiki/Roy_Early_Blick,"Roy Early Blick (1899 – 1972) was the director of the Morals Division (the vice squad) of the Metropolitan Police Department of the District of Columbia (MPD) in the United States during the mid-twentieth-century.[2] He oversaw investigation of and apprehension for offenses related to burlesque, pornography, child pornography, and other obscenity and indecency, prostitution, crimes of ""sex perversion"" including homosexuality, and gambling. Even before becoming director of the Morals Division, during his preceding career with the MPD, he was consulted by US federal lawmakers,[3] testified before Congress on several occasions,[1][4][5][6] and worked with the FBI on related law enforcement matters. Freedom of Information Act lawsuits in the twenty-first century revealed previously-classified documents indicating frequent meetings and correspondence between the Central Intelligence Agency and Blick during his service as a police official. Blick oversaw operations similar to the later 1989 DC prostitute expulsion: en masse coercion of sex workers to leave the city. In 1954 a newspaper report stated that US senators Styles Bridges and Herman Welker threatened to compel Blick's resignation if he did not take steps to ensure the prosecution of the son of fellow senator Lester C. Hunt—Lester Hunt, Jr.—who had been arrested for soliciting an undercover policeman.[7][8] Blick married Lee Anna Embrey, an author and charter staff member at the founding of the National Science Foundation[9] who later joined the National Academy of Sciences. His career in the MPD began in 1931[6] and he rose to the rank of Deputy Chief of Police. [Pornography] is more dangerous than narcotics, because you inject narcotics to an individual and it is over with. These pamphlets, these booklets, can be passed from one to another. It is the same as a prostitute that can infect an army of men if she is permitted to hang around the camp. It is the same as this pornography that is being passed around. It can be passed from one hand to another, and it is causing a lot of headaches in the country. It is causing kids who are just at the age that they should know right from wrong to become perverts and homosexuals. 
 
 
 This biography article about law enforcement in the United States is a stub. You can help Wikipedia by expanding it."
Caste discrimination in the United States,/wiki/Caste_discrimination_in_the_United_States,"Caste discrimination in the United States is a form of discrimination based on the social hierarchy which is determined by a person's birth.[1] Though the use of the term caste is more prevalent in South Asia and Bali, in the United States, Indian Americans also use the term caste.[2][3] Caste is not officially recognized by law in the United States, except in Seattle, Washington; on February 20, 2023, Seattle became the first U.S. jurisdiction to add caste to its list of categories protected against discrimination.[4] The existence of caste discrimination in the US tech sector was also acknowledged by a group of Dalit female engineers from Microsoft, Google, Apple and other tech companies. In 2021, the student body of California State University system passed a resolution against caste discrimination. Caste is a birth-based social stratification system. Historically, the dominant caste Hindus and Sikhs have been the ones to acquire citizenship in America. In 1923, A.K. Mozumdar and Bhagat Singh Thind argued that they passed the whiteness test because they were caste Hindus and had pure ""Aryan"" blood.[page needed] In 1910, the Asiatic Exclusion League argued that Asian Indians should be denied citizenship through naturalization. The league described Hindu ancestry as ""enslaved, effeminate, caste ridden, and degraded"" and Hindus as the ""slaves of Creation"".[page needed] Such racist rhetoric formed the idea of the ""Hindoo invasion"", an iteration of the ""Yellow Peril.""[page needed] In 1953, W. Norman Brown, founder of the Department of South Asian Studies at the University of Pennsylvania, wrote that ""a large number of Americans...have a picture of India ... where everyone is a beggar and caste is more important than life"". In recent times, caste discrimination has followed the immigrants to the US from India, Nepal and other south Asian countries. There are more than five million South Asians living in the US.[5] Despite being one of the fastest growing immigrant groups, it has been mostly underreported despite its influence on job opportunities and marriage prospects among south Asian immigrants. Indian migrants account for a large number of high-skilled workers in STEM fields, leading to an issue of caste discrimination in the workplace in areas such as Silicon Valley. Thenmozhi Soundararajan, executive director of Equality Labs, says, “Caste has been here (in the US) for a long time. Wherever South Asians go, they bring caste.""[6] Several observers see parallels between the issues of race in the United States and the issues of caste. When Martin Luther King, Jr. visited India in 1959, he was introduced by the principal of a school with Dalit students (then called ""untouchables"") as a ""fellow untouchable from the United States of America"". Though taken aback with this description, King agreed with it after reflection, thinking, ""Yes, I am an untouchable, and every negro in the United States of America is an untouchable.""[7] More recently, Isabel Wilkerson wrote Caste: The Origins of Our Discontents, which argues that racial stratification in the United States is best understood as a caste system, akin to those in India and in Nazi Germany.[8]The New York Times calling it ""an instant American classic"",[8] and Publishers Weekly calling it a “powerful and extraordinarily timely social history.”[9] The book reached the number one spot in The New York Times nonfiction best-seller list.[10] Lower caste activists in India have found common ground with the struggles of African Americans in the US.[11] The lower caste activist body Dalit Panthers was formed taking inspiration from Black Panther Party.[3][12] Caste is not officially recognized by law as a category of discrimination in the United States.[13][14] The reason is that caste discrimination was not a known phenomenon when the laws were written.[2] It has come to light only in recent times due to recent reports of discrimination.[15][16] The California law bars discrimination on the basis on ancestry. Dalit lawyers believe that caste discrimination is covered under it.[17] Legal scholars have also argued that caste discrimination is cognizable as race discrimination, religious discrimination and national origin discrimination.[18] In August 2002, the UN Committee for the Elimination of Racial Discrimination approved a resolution condemning caste or descent-based discrimination.[19] In February 2023, Seattle became the first city in the United States to explicitly ban discrimination based on caste.[20] In March 2023, the state of California began to consider a bill in the senate for a similar ban.[21] The bill was vetoed by Governor Gavin Newsom in October 2023.[22] In September 2023, the California state senate passed a bill banning discrimination based on caste. SB403, authored by Democratic Senator Aisha Wahab, is a legislative initiative in California that aims to make the state the first in the nation to include caste discrimination in the list of protected rights.[23] The bill defines caste as “an individual’s perceived position in a system of social stratification on the basis of inherited status”, which can be determined by several factors including the “inability or restricted ability to alter inherited status; socially enforced restrictions on marriage, private and public segregation, and discrimination; and social exclusion on the basis of perceived status.”[24] SB403 proposes amendments to California's housing, labor, and education codes to explicitly prohibit discrimination based on one's ancestry, which notably includes caste. Nirmal Singh, a physician from Bakersfield, California, touches on the impact of SB403 for South Asians: ""“It has become psychological trauma that carries over, one generation to the other generation. “This was a very important bill for us.”[25] The oppressed castes of South Asia, known as Dalits, form 1.5% of all Indian immigrants to the United States, according to a University of Pennsylvania study carried out in 2003.[26] The 'high' or 'dominant' castes make up more than 90% of Indian migrants as per a study in 2016.[27][a] A study done under Carnegie Endowment for International Peace reported that 47% of Hindu respondents identified with a caste; the remaining 53% do not identify with any caste group. In the words of the researchers, the majority do not identify with caste, and this is much more so for American-born Hindu Americans. Of those that identified with a caste, roughly 1 percent each identified with scheduled caste (dalit) and scheduled tribe (adivasi) categories.[29] A survey on caste discrimination conducted by Equality Labs[b] found 67% of Indian Dalits living in the US reporting that they faced caste-based harassment at the workplace, and 27% reporting verbal or physical assault based on their caste.[32] The survey also documents personal anecdotes about discrimination and isolation at schools, workplaces, temples and within communities.[26] The Carnegie Endowment researchers pointed out that the study used a non-representative snowball sampling method to identify participants, which might have skewed the results in favour of those with strong views about caste.[29][30] The Carnegie Endowment study, using a sample from YouGov, found 5% of the Indian Americans reporting they faced caste discrimination.[c] A third of them said that they faced discrimination from other Indian Americans, another third said they faced it from non-Indian Americans, and a final third said that they faced it from both Indian and non-Indian Americans. The researchers found this response perplexing as non-Indians would not have had caste as a salient category.[29] Homophily based on caste, i.e., tendency to associate with the people of the same caste, was reported by 21% of the respondents; 24% said that they did not know the caste of the people they associated with. The remainder said that they associate with some or most people of their caste (23% and 31% respectively).[29] The Ambedkar King Study Circle collected testimonies of how caste consciousness and discrimination are practiced by the Indian Diaspora. The testimonies record various types of discriminatory practices in schools, workplaces, social gatherings and neighborhoods. Usually this discrimination borders on the sense of notional and real 'untouchability'.[33][34] Caste-based discrimination imposes psychological distress on its victims, especially from lower castes. Dalits suffer at the hand of discrimination, being referred to as ""untouchable"" and ""dirty"".[35] Dalits are often bullied by upper-caste classmates and treated differently by teachers, which is trauma that translates from one generation to the next.[25] South Asians report experiencing diminished self-esteem, feelings of isolation, and enduring anxiety and fear. The first extensive study of caste and its effects in the US found that those from lower castes ""fear retaliation and worry about being ""outed"" and hence ""hide their caste.""[36] This fear of being outed can manifest in several ways; for example, some families opt to change their surnames to one considered more ""caste neutral"" (i.e. ""Kumar"", ""Singh"", ""Khan"") in order to avoid ridicule and isolation.[37] The long-term impact on mental health can be severe. However, psychological intervention from the lens of casteism has lagged behind. Research of the Dalit community in Nepal demonstrates that social inequality by way of economic, social, cultural, and psychological stress all contribute to their experience of diabetes; Dalits used diabetes to relay ""psychosocial stress through somatic symptoms."" Diabetes and other illnesses disproportionately affect those of lower castes, and the causes linked to intergenerational poverty, lack of access to healthcare and education, and other stressors which prolong diabetes impact the mental health of these individuals.[38] The existence of caste discrimination in the US tech sector was acknowledged by group of Dalit female engineers from Microsoft, Google, Apple and other tech companies.[39][40] Ambedkar King Study Circle (AKSC), a US based activists group, along with 15 other organizations sent an appeal to top American companies including Google, Apple, Microsoft demanding that the CEOs intervene immediately to address the issue of caste discrimination. The AKSC wanted the companies to bring in caste sensitivity training similar to the gender, race, sexuality training practices. AKSC emphasized the fair and equal opportunity recruitment, retention and appraisal policies.[41][42][43] In May 2021, the Federal Bureau of Investigation raided Akshardham in Robbinsville Township, New Jersey to investigate forced labor of lower caste Indian workers.[44][45][16] The workers were brought on religious visa and the FBI removed about 90 workers from the site.[46] In April 2022, Google cancelled a planned talk by Thenmozhi Soundararajan as part of its Diversity Equity and Inclusivity programme. It was allegedly done under pressure from employees who accused her of being ""Hindu-phobic"" and ""anti-Hindu"". Some felt their lives would be endangered if the talk went ahead. Rather than bringing their community together, it caused ""division and rancor"", according to the Google spokesperson. The senior Google manager who invited Soundararajan resigned over the incident.[47][48][49] In 2020, caste-based discrimination issues in Silicon Valley came to the surface with a lawsuit by the State of California against Cisco Systems filed by the California Department of Fair Employment and Housing (DFEH, later named Civil Rights Department).[50] The Department sued Cisco and two of its senior engineers for discrimination against a Dalit engineer (identified as ""John Doe""), who had alleged that he received lower wages and fewer opportunities because of his caste.[51][52] After an initial filing in a United States District Court, the department refiled it in Santa Clara County Superior Court in October 2021[53] Cisco filed a demurrer asking for dismissal on the grounds that caste and ethnicity were not protected categories under the Fair Employment and Housing Act of California. The Ambedkar International Center and other Dalit organizations filed an amicus curiae brief, arguing that the California law does in fact prohibit caste discrimination.[50] The Hindu advocacy group Hindu American Foundation (HAF) filed a claim in a United States District Court stating that the California department infringed on the civil rights of Hindus by asserting that Hinduism mandates caste discrimination.[54] In April 2023, the California Civil Rights Department dismissed (withdrew) its case against the two engineers accused of discrimination, followng an order from the Santa Clara Superior County Court, though it continued with the case against the Cisco corporation.[55] According to court filings, the accused CEO of the division had actively recruited ""John Doe"", offering him a generous starting package and stock grants, knowing all along his caste background. He had also recruited other Dalits, including the one that was eventually chosen for the leadership role that John Doe was denied.[54][56] HAF reviewed the case files and alleged that the California department's narrative in the case was ""full of lies"".[57] In 2015, California State Board of Education initiated a regular ten-year public review of the school curriculum framework.[58] The Hindu American Foundation (HAF) and a coalition of other Hindu activists sought to literally erase the word ""dalit"" from the syllabus,[59][60] which was contested by a coalition of interfaith, multi-racial, inter-caste groups called ""South Asian Histories for All"".[58][61][relevant?] In 2021, the student body of California State University system, representing half a million students, passed a resolution seeking a ban on caste-based discrimination.[62] The campaign was spearheaded by Prem Pariyar, a Nepali origin Dalit student, who came to the US in 2015 escaping persecution in his home country, and claimed that he faced discrimination in the US as well.[62] For the affected students, casteism is manifested through slurs, microaggressions and social exclusion.[63] The resolution cited the survey by Equality Labs where 25 percent of Dalits reported having faced verbal or physical assaults.[62]Al Jazeera noted that the resolution was authored by a higher caste student and backed by other students from other racial and religious groups.[62] In January 2022, the Board of Trustees of the California State University responded, announcing that they added ""caste"" as a protected category in the university's anti-discrimination policy.[64] The change was subtle, according to CNN. The word ""caste"" was added in parentheses after the term ""race and ethnicity"".[63] A group of faculty in the university had written to the Board of Trustees citing lack of ""due diligence"" in instituting the measure. They said that the existing policy of the university, which covers national origin, ethnicity and ancestry, already provided adequate protection, and claimed that the new measure would result in singling out and targeting the Hindu faculty.[65][66] But for the advocates and student leaders who campaigned for it for over two years, it was a civil rights victory.[63] In December 2022, Brown University became the first Ivy League institution to add caste to its nondiscrimination policy. Brown's vice president for Institutional Equity and Diversity noted that caste was covered under existing nondiscrimination policies, ""but we felt it was important to lift this up and explicitly express a position on caste equity.”[67]"
Cat's paw theory,/wiki/Cat%27s_paw_theory,"The Cat's Paw theory is a legal doctrine in employment discrimination cases that derives its name from the fable ""The Monkey and the Cat,"" attributed to Jean de La Fontaine. In the fable, a cunning monkey persuades a naive cat to retrieve chestnuts from a fire, with the cat ultimately burning its paws while the monkey enjoys the chestnuts.[1] In the context of employment law, the theory addresses situations where a biased employee or supervisor manipulates a neutral decision-maker into taking an adverse employment action against another employee, based on discriminatory motives. This legal doctrine has been applied in United States employment discrimination cases since the early 1990s.[2] The United States Supreme Court formally recognized and clarified the application of the Cat's Paw theory in the 2011 case Staub v. Proctor Hospital.[3] In Staub, the Court held that an employer may be held liable for employment discrimination under the Uniformed Services Employment and Reemployment Rights Act (USERRA) if a biased supervisor's actions are a proximate cause of an adverse employment action, even if the ultimate decision-maker was not personally biased.[4] The Cat's Paw theory has since been applied in cases involving other anti-discrimination statutes, such as Title VII of the Civil Rights Act of 1964, the Age Discrimination in Employment Act (ADEA),[5] and the Americans with Disabilities Act (ADA). The doctrine provides a basis for holding employers accountable when they inadvertently rely on the discriminatory animus of an employee in making employment decisions, even when the decision-maker is otherwise unbiased. 
"
A Class Divided,/wiki/A_Class_Divided,"""A Class Divided"" is a 1985 episode of the PBS series Frontline. Directed by William Peters, the episode profiles the Iowa schoolteacher Jane Elliott and her class of third graders, who took part in a class exercise about discrimination and prejudice in 1970 and reunited in the present day to recall the experience. ""I watched what had been marvelous, cooperative, wonderful, thoughtful children turn into nasty, vicious, discriminating, little third-graders in a space of fifteen minutes."" —Jane Elliott in ""A Class Divided"" recalling her 1970 experiment[1] In 1968, following the assassination of Martin Luther King Jr., Jane Elliott tried discussing issues of discrimination, racism, and prejudice with her third grade class in Riceville, Iowa. Not feeling that the discussion was getting through to her class, who did not normally interact with minorities in their rural town, Mrs. Elliott began a two-day ""Blue Eyes/Brown Eyes"" exercise to reinforce the unfairness of discrimination and racism: Students with blue eyes were given preferential treatment, given positive reinforcement, and made to feel superior over those with brown eyes for one day; the procedure was reversed the next day, with Mrs. Elliott giving favorable preference to brown-eyed students. As a result, whichever group was favored by Elliott performed enthusiastically in class, answered questions quickly and accurately, and performed better in tests; those who were discriminated against felt more downcast, were hesitant and uncertain in their answers, and performed poorly in tests. William Peters became interested in Mrs. Elliott after reading an article about her work in The New York Times and arranged soon afterward to film the class. The resulting footage would become The Eye of the Storm, which originally aired on ABC in 1970. Peters was surprised by the change he observed in the children and remarked at how disinterested they were with the cameras, because they were so involved in the exercise that they had no idea they were being filmed.[2] ""A Class Divided"" picks up the story in August 1984, with Peters following up on Mrs. Elliott and eleven of the now-grown children, who reunite during their high-school reunion. At their request, the former students and Mrs. Elliott together rewatch The Eye of the Storm. Scenes from that original film are interspersed with the participants' present-day reactions and anecdotes. As Charlie Cobb notes in his narration, the get-together is Mrs. Elliott's first chance to find out how much of the lesson her students retained. The students recall in interviews their memories of their feelings at the time of the film, including that of shame and anger when wearing the brown identifying collars (Mrs. Elliott employed them to easily identify the group being discriminated against) as well as that of elation and superiority when they shed the collars. The now-adults agree, as they had learned after the 1970 experiment, that racism and prejudice are wrong, and that the life-affecting lesson should be experienced by other children, teachers, and adults in the present day as a form of understanding. ""A Class Divided"" confirms that Mrs. Elliott has continued her ""Blue Eyes/Brown Eyes"" experiment in the present day, though there has been little outward reaction from parents or school authorities in Riceville. ""A Class Divided"" also demonstrates that The Eye of the Storm and the lessons it demonstrates have been widely used in other schools, government, the business world, and correctional systems across the country. The latter is evidenced by scenes in New York's Green Haven Correctional Facility, where Eye is shown to a class taught by a sociology professor, and in Iowa, where Mrs. Elliott is shown presenting her ""Blue Eyes/Brown Eyes"" lesson to employees of the state's corrections department. Frontline's presentation of ""A Class Divided"" earned PBS an Emmy Award in 1986 for Outstanding Informational, Cultural or Historical Programming"" (program length).[3] William Peters would follow up ""A Class Divided"" with the book A Class Divided: Then and Now (Yale University, 1987), which expanded on the topics and thoughts displayed in both ""A Class Divided"" and The Eye of the Storm.[4][5]"
Dawes Rolls,/wiki/Dawes_Rolls,"The Dawes Rolls (or Final Rolls of Citizens and Freedmen of the Five Civilized Tribes, or Dawes Commission of Final Rolls) were created by the United States Dawes Commission. The commission was authorized by United States Congress in 1893 to execute the General Allotment Act of 1887.[1] Traditionally, the land in these tribal communities had been held communally.[2] With the establishment of the Dawes Commission, the ruling was made by the colonial agents to divide the land into parcels and institute a system of individual ownership in accordance with US laws, overriding the treaty and tribal laws of the region.[2] To allot the communal lands, citizens of the Five Civilized Tribes (Cherokee, Chickasaw, Choctaw, Creek, and Seminole) were to be enumerated and registered by the US government. These counts also included the freedmen – formerly-enslaved African-Americans who had been emancipated after the American Civil War, and their descendants. The rolls were used to assign allotments to heads of household and to provide an equitable division of all monies obtained from sales of surplus lands. These rolls became known as the Dawes Rolls. When word got out that people could get land, many non-Natives appeared at the offices and falsely claimed to be Native. Most of these false claimants claimed to be Cherokee. Family myths still persist of ""hiding in the hills"",[3] or of being ""rejected from the rolls"", or ""refusing to enroll"" when the reason for having not been enrolled is that the applicants were simply not Native American.[2][4] The Dawes Commission went to the individual tribes to obtain the membership lists, but it took a series of attempts to gain anything approaching an accurate count. In 1898, Congress passed the Curtis Act, which provided that a new roll would be taken and supersede all previous rolls. Difficulties in enumerating the population included the forced migrations of the period as well as the American Civil War.[5][6] Additionally, non-Native census takers introduced the idea of Blood Quantum, a concept previously foreign to the tribal communities.[7] Those recording this percentage of ancestry wrote down an estimation, based on physical appearance and personal opinion if the individual was present.[5][6] Tribal citizens were listed under several categories: More than 250,000 people applied for membership, and the Dawes Commission enrolled just over 100,000. Most were rejected because they were non-Natives who showed up demanding land, but could not prove any connection to an existing Native community, such as naming living relatives or speaking the Native language. Overrun with prospective claimants, the commission was overwhelmed and instituted guidelines: It rejected the unconscionable claim that a white person once admitted into the tribe by marriage to an Indian could confer citizenship upon any white person whom he might afterwards marry and upon his white descendants. It also uncovered a great mass of nauseous evidence, and rejected a large number of claims upon the ground they had been advanced through perjury and forgery.[2] An act of Congress on April 26, 1906 closed the rolls on March 5, 1907. An additional 312 persons were enrolled under an act approved August 1, 1914. While some initially refused to be enumerated, almost all were later arrested and enrolled against their will; enrollment was not a matter of ""choice.""[4][8] Refusing to be enumerated, and even fleeing, would mean warrants being issued for the person's arrest; they could then be treated brutally and imprisoned in the process of being enrolled by force.[4][8] Still, due to understandable distrust of the government, there were those who tried to avoid ennumeration. Notable among those who resisted were Muscogee Chitto Harjo (Crazy Snake), and Cherokee Redbird Smith. But both Harjo and Smith were eventually coerced into enrolling. According to Cherokee professor Steve Russell, some Natives hiding in the Cookson Hills never enrolled,[9] but some of them were later arrested and forcibly enrolled, while others were enrolled on their behalf by people in their communities. Additionally all individuals on the Census Roll of 1896 were enrolled without notification to the parties involved.[10] The only real choice to avoid enumeration entirely meant completely leaving one's community and assimilating.[4][8] Since that period, the tribes have relied on the Dawes Rolls as part of the membership qualification process, using them as records of citizens at a particular time, and requiring new members to document direct descent from a person or persons on these rolls.[11][5] Courts have upheld this rule even when it has been proven that a brother or sister of an ancestor was listed on the rolls but not the direct ancestor himself/herself. Another issue on the Dawes Rolls are people termed ""Five-Dollar Indians."" Some white people bribed government officials to obtain land allotments, but this was not as widespread as some would believe.[12][2] Gregory Smithers, associate professor of history at Virginia Commonwealth University stated, ""These were opportunistic white men who wanted access to land or food rations. ... These were people who were more than happy to exploit the Dawes Commission – and government agents, for $5, were willing to turn a blind eye to the graft and corruption.""[12] For the small minority that managed this, this fraudulent enrollment may have earned white people potential benefits for themselves and their descendants, but also could have subjected them to further removal, relocation or incarceration. There were also land runs during this era, and other methods for white people to obtain land.[4] Most of the white people on the Dawes roll are noted as included due to marrying a member of the tribe and having Indian children.[2] The Dawes Rolls, though recognized as flawed, are still essential to the citizenship process of the Nations that include them in their laws.[5][6] The federal government uses them in determining blood-quantum status of individuals for Certificate of Degree of Indian Blood.[5]"
DePauw University Delta Zeta discrimination controversy,/wiki/DePauw_University_Delta_Zeta_discrimination_controversy,"The DePauw University Delta Zeta discrimination controversy occurred at the end of 2006 when the Delta Zeta (ΔΖ) national leadership was criticized after The New York Times published an article accusing the sorority of deactivating certain members of the Delta Chapter at DePauw University based on their perceived attractiveness. The controversy made national headlines and resulted in the chapter's closing and various legal actions. Founded in 1902, the Delta chapter was the sorority's second oldest active chapter and its fourth oldest chapter overall. Despite its long history at DePauw, however, the Delta chapter in recent years had struggled with declining membership and acquired a negative reputation on campus.[1] In August 2006, national representatives, concerned about the Delta chapter's inability to recruit new members, announced that the chapter would be closed at the end of the 2006–7 school year if they failed to increase their numbers substantially or elected not to participate in active recruitment or informal rush.[2] At a school where an estimated 70% of the student body belongs to a fraternity or sorority, the ΔΖ house was two-thirds empty. According to some former members, the chapter was known on campus as ""The Dog House,""[3] and a DePauw psychology professor's survey of students found that ΔΖ was considered ""socially awkward."" The sorority attracted ""brainy women"", many with science and math majors, along with the gifted disabled.[1] Many current members felt that it might be in their best interest to let the chapter close without recruiting new members.[2] In anticipation of Delta's 2009 centennial, ΔΖ Executive Director Cynthia Winslow Menges had originally planned to close the chapter temporarily.[4] In September 2006, however, the university informed ΔΖ headquarters that if the chapter were closed, it would not be allowed to reopen on the sorority's chosen timetable, out of fairness to other National Panhellenic Conference (NPC) sororities with dormant chapters at DePauw. Instead, ΔΖ would have to wait for the campus to open for NPC extension and then compete for consideration with whichever NPC groups were not represented at DePauw.[2] In an emergency attempt to salvage the chapter, a team of national representatives came to the university in November 2006 to conduct a membership review, interviewing women individually about their dedication to the sorority. (Delta Zeta representatives have repeatedly asserted that they undertook the review on the advice of the university, whereas DePauw officials ""vehemently"" deny this.[5]) A few days after the interview process, the national team with women from ΔΖ's Epsilon Chapter at nearby Indiana University held a recruiting event in the house, where some allege 25 of the chapter members were asked not to participate and to remain out of sight. A February 25, 2007 article in The New York Times quoted one former Delta Chapter member as saying, ""They had these unassuming freshman girls downstairs with these plastic women from Indiana University, and 25 of my sisters hiding upstairs. It was so fake, so completely dehumanized.""[1] Subsequently, 23 out of 35 active members, including Delta's president, were assigned early alumna status and asked to vacate the house.[1] (Four members had resigned in September after the review was announced,[2] and three others who were living off-campus at the time have claimed they were never contacted about their membership status.) Former members later told university officials that when they first learned of the review, they were led to believe that they would be allowed to decide for themselves whether to continue their involvement in the sorority,[6] but in early December 2006—shortly before finals week at DePauw—national headquarters sent out letters informing members that they were either still active or had been recommended for alumnae status and were to move out of the house by the end of January 2007. Each new alumna received $300 to cover the difference between sorority housing and campus housing; nonetheless, many of the women were unsure that they would find another place to live. The university eventually found housing for the evicted women.[1] In the months following the review, reports began appearing on the Internet alleging that the evicted women had been threatened with expulsion if they refused to take alumnae status, and accusing the national team of choosing which women got to stay active according to their perceived attractiveness. In response to growing criticism from DePauw students and administrators, Delta Zeta representatives stated that the women who had been asked to leave lacked commitment to the chapter's future, but according to The New York Times report, the 23 evicted members included all of the overweight women in the chapter, as well as 3 of the 4 minorities; conversely, the 12 women invited to stay ""were slender and popular with fraternity men,"" but eventually half of that group also resigned as a show of solidarity.[1] In the days following the NYT article, other national media outlets picked up the story, including CNN, CBS News, Good Morning America, MSNBC, Newsweek, and People. On February 19, 2007, DePauw President Robert Bottoms formally reprimanded Delta Zeta's national headquarters for its actions and instituted a new rule requiring that all housed fraternities and sororities at the university provide housing for their members throughout the school year, except when risk-management violations or behavioral problems make eviction necessary.[6] During a February 26 interview with Paula Zahn of CNN, Cynthia Menges, then executive director of Delta Zeta, denied the chapter's allegations of discrimination based on religion, race, or ethnicity and maintained that the 23 affected women left voluntarily. She also justified the reorganization on the grounds that the chapter had voted to close anyway and that DePauw officials would not guarantee ΔΖ an opportunity to return in the near future. Asked whether appearance had played a part in the evictions, Menges did not directly respond.[7] On March 1, 2007, Delta Zeta headquarters announced that it would no longer respond to media inquiries about the DePauw chapter.[8] As of March 6, the sorority's national Web site features an apology to the evicted students but still includes a letter calling into question those women's loyalty to the chapter and blaming them in part for its recent struggles.[9] On March 12, 2007, Bottoms withdrew Delta Zeta's status as a recognized campus organization, stating that the national organization's values, as demonstrated by the evictions, are not compatible with the university's. The sorority was required to leave DePauw following the conclusion of the 2006–2007 academic year.[10][11] Delta Zeta responded with a statement on the front page of its Web site expressing disappointment with Bottoms's decision and reiterating its position that the review was necessary, but conceding that the 23 alumnae should probably have been notified of their change in status in person and at a different point in the school year.[12] Bottoms characterized The New York Times story as inaccurate and a misrepresentation of the school.[13] Meanwhile, 10 Delta Chapter alumnae have formed a college-sanctioned student organization, Psi Lambda Xi (ΨΛΞ), with the goal of becoming a new sorority. A founder said, ""The founding purpose of the sorority is to promote a positive self-image in each other, the University and the community.""[14] On March 29, 2007, it was reported in The New York Times that Delta Zeta's national office, based in Oxford, Ohio, had filed suit in U.S. Federal District Court in Indianapolis against DePauw University for expelling the sorority from campus.[15] The university said the lawsuit completely lacks merit. ""From the beginning, DePauw University has acted to protect its students.""[16] In November, 2007, Delta Zeta withdrew the lawsuit and DePauw agreed that the sorority will have the opportunity to compete with other sororities attempting to colonize a chapter on the campus beginning in academic year 2010/11[17][18] however Delta Zeta elected not to compete for the 2010/2011 academic year and as of May 2010 has no future plans to do so.[19] Meanwhile, eight former members hired an attorney seeking an apology and other corrective actions from Delta Zeta's national office.[20] In 1999, eight former members of the sorority's Alpha Theta Chapter at the University of Kentucky sued the national organization, claiming they had been forced to take early alumnae status based on their appearance during a similar reorganization. ΔΖ settled the lawsuit in 2001. The attorney who represented the women from Kentucky called the sorority's actions toward the DePauw chapter ""egregious"".[21] As of December 2021, ΔΖ has not returned to the DePauw campus. The local sorority, ΨΛΞ, formed by eight previous members of ΔΖ, remains active on campus, though not affiliated with the local Panhellenic or any other local governing council. In response to the controversy, Delta's six remaining active members issued a statement alleging that their chapter had been misrepresented in the original article in The New York Times.[1] They claimed that one of their current members had offered an opposing viewpoint to Sam Dillon, the reporter who wrote the story, but that her comments were not used. They went on to deny the allegations that ""race, weight, and academic majors were used as criteria in the membership review process"".[22] Concern was expressed by other students that at a meeting held February 2, 2007, an educational leadership consultant from Delta Zeta national stated, ""Image, I'm not going to lie to you, is a huge part of it.""[23] Since the reorganization and before the national publicity, 8 of 11 freshman women who received invitations to join Delta Zeta during formal recruitment chose not to become members. One said, ""We all got together and talked about it and tried to have an open mind. But all of us were really against what they did, and we didn't want to be associated with it.""[4] Texas Christian University selected Gamma Phi Beta International Sorority over Delta Zeta as a new member of its Panhellenic community on February 28, 2007.[24] ""It will be a concern because if you bring on a sorority with nationwide conflict,"" said the Panhellenic director of recruitment, ""that's taking a big liability."" The Panhellenic president stated that the recent events at DePauw were a factor in the decision.[25]"
Discrimination against people with red hair,/wiki/Discrimination_against_people_with_red_hair," Discrimination against people with red hair is the prejudice, stereotyping and dehumanization of people with naturally red hair, which is the result of a genetic mutation. In contemporary form, it often involves a cultural discrimination against people with red hair. A number of stereotypes exist about people with red hair, many of which engender harmful or discriminatory treatment towards them. While discrimination against people with red hair has occurred for thousands of years and in many countries, in modern times it has been described as particularly acute in the United Kingdom, where there have been calls to designate red hair a protected characteristic covered by hate crime legislation.[1] Naturally occurring red hair appears in a small minority of humans and is the rarest natural hair color, occurring in 0.6 percent of people.[2] A smaller percentage of humans, approximately 0.17 percent or 13 million, have a combination of red hair and blue eyes.[3] Red hair is one potential manifestation of a gene mutation in the melanocortin 1 receptor (MC1R).[4] While red hair most frequently occurs among European peoples, it is also present among persons of Asian descent or Africans with European admixture (though extremely rare).[5] A higher prevalence of the MC1R mutation in Europe may be due to it promoting adaptability to low light environments as it facilitates efficient biosynthesis of Vitamin D among other survival traits such as higher resilience to certain types of pain, and increased levels of adrenaline that accelerates the fight-or-flight response.[6][7][8] The history of prejudice and discrimination against people with red hair dates back thousands of years.[9] According to The Week, persons with red hair have to deal with ""insults ... taunts, and ... hate crimes"".[10] Discrimination against people with red hair may be a factor of its relative rareness, as well as cultural attitudes and collective mythology.[11]Judas Iscariot may have had red hair, and some Indo European folklore presents that people with red hair are vampires or transform into vampires after death.[11] The assignment of prejudicial characteristics towards people with red hair, such as a propensity towards violence, may also be a long-lasting association Eurasian peoples had to the hair color resulting from their contact with aggressive and violent Thracian tribes which had a high prevalence of red hair.[12] In Ancient Egypt, men with red hair may have been used as human sacrifices to the god Osiris due to the belief that his archenemy, Set, had red hair and that those with red hair were, therefore, devotees of Set.[13][14] Though, rulers of the Nineteenth Dynasty of Egypt may have been red haired followers of Set; Ramesses II had red hair and his father's name, Seti I, means ""follower of Set"".[15] In 2008, the Adelaide Zoo faced criticism after it launched a promotional campaign for its orangutan exhibit in which people with red hair were offered free admission.[16] In promotional communication, the zoo compared people with red hair to the ape species, claimed people with red hair were destined for extinction, and used the pejorative term ""ranga"" to refer to them.[16] In response to what the zoo characterized as a ""negative reaction"" to the campaign, it dropped one element that involved photographing people with red hair next to the orangutan exhibit for use in advertising materials.[16] In 2010, several ads in Australia centered around ridiculing people with red hair.[17] One, a government road safety campaign, suggested that using a mobile phone while driving might cause unwanted side effects such as sexual intercourse between two people with red hair, an occurrence which might result in the offspring also having red hair.[17] Another, for ANZ Bank, featured a character of a bank clerk who was comically rude towards customers.[17] In 2009, students with red hair from at least three Canadian schools, were reportedly assaulted by their classmates, with one incident being confirmed by a court verdict. The students were influenced by a Facebook group that promoted so-called ""Kick a Ginger Day"", and possibly by a 2005 South Park episode.[18] Until recently, it was not uncommon for people with red hair in France to be called Poil de Judas (""hair of Judas""), a reference to the idea that Judas Iscariot had red hair.[19] In the past, red hair has been wrongly believed to be a characteristic associated exclusively or significantly with Jews, due to the belief that Judas Iscariot had red hair.[20] In medieval Germany, some believed a tribe of Rote Juden, or ""Red Jews"", inhabited the Caucasus Mountains.[20] According to myth, this was a reclusive tribe of Jews with red hair conspiring with the Antichrist to destroy Christianity.[20] It has been hypothesized that this belief may have originated in the fragment of a social memory of the Khazars who, according to some sources, had a high prevalence of red hair and blue eyes.[21] In some instances, women with red hair were often presumed to be witches and subject to punitive violence.[19] According to some observers, red haired people in the United Kingdom face particularly ""aggressive"" discrimination due to systemic ""prejudice ... related to centuries-old matters of imperialism, religious bigotry and war"".[22] According to TRT World, the UK is ""arguably the nation most hostile to this hair colour"" despite red hair having the highest incidence in that country.[23] The UK's Anti-Bullying Alliance has called for red hair to be listed as a protected characteristic, which would result in the targeting of people with red hair for criminal acts classified as a hate crime.[23] Some have said that people with red hair are abused by those who would prefer to abuse racial minorities but feel restrained by hate crimes legislation and, therefore, target classes of people not protected by law.[24] In a 2013 article in New Statesman, columnist Nelson Jones chronicled several anecdotes of people with red hair who had been physically assaulted that year in the United Kingdom due to their hair color, including at least one stabbing.[1] A 2014 study found that more than 90 percent of men in the UK with red hair had been the target of bullying due exclusively to their hair color.[25] In addition, the study found, approximately 61 percent of males and 47 percent of females with red hair reported encountering ""some kind of discrimination in the past"" as a result of their hair color.[25] According to Lily Cole, who has red hair, being bullied as a child for red hair in the UK was ""not dissimilar"" to experiencing racial abuse.[26]Prince Harry and David Kitson have reported being abused as a result of their hair color.[27] The head of one children's charity reported that levels of abuse in the UK were significant and said there was ""nothing like this in the U.S.""[27] In 2015, a person with red hair was convicted of terrorist offenses over a plot to assassinate Prince Charles and Prince William in order to ensure Prince Harry, who has red hair, would become King of the United Kingdom.[28] The man attributed his genetic supremacist views towards childhood bullying to which he'd been subjected over his hair color.[28] In 2018, a television advertisement for Carlton & United Breweries Yak Ales was criticized after the Advertising Standards Authority found that it vilified people with red hair by suggesting society should work towards their eradication.[29] Carlton & United ultimately pulled the ad, but declined to apologize for it.[29] In 2022, Sheffield-based human rights advocate Chrissy Meleady called for more protection for red-haired children, noting some bullying incidents, including a teaching assistant being fired for making fun of a red-haired student.[30] The television program South Park has dealt with the topic of discrimination against people with red hair, most notably in the 2005 episode ""Ginger Kids"". According to anecdotal reports, children with red hair are regularly assaulted on the so-called ""Kick a Ginger Day"" supposedly inspired by the episode.[citation needed] In 2015, police in Massachusetts investigated a conspiracy among students who attacked other students with red hair on the date.[31] Cryos International, one of the world's largest sperm banks, said in 2011 that they had too many sperm doses from red-haired individuals, but agency director Ole Schou said that they ""have nothing against red-haired donors"".[32] In some cases, discrimination can occur in the form of preferencing people with red hair over those without red hair. A 2014 study found that 30 percent of television commercials during primetime viewing hours in the United States prominently featured someone with red hair with, at one point, CBS showing a person with red hair once every 106 seconds, numbers not accurately reflective of the actual population of persons with the hair color.[7] Andrew Rohm, professor of marketing at Loyola Marymount University, attributed the prevalence of red hair in television advertising as an attempt by companies to capture viewer attention by showing people with what they perceived to be unusual or exotic physical characteristics.[33] The casting of Halle Bailey, who does not have natural red hair, to perform the role of Ariel in The Little Mermaid, a remake of the 1989 film where the protagonist does, was criticized by some people as the character was ""a wonderful role model for young ginger girls, and this casting is a loss for them"", however, due to the concurrent backlash regarding Bailey's race (Bailey is not white like Ariel in the 1989 film), such criticisms have faced accusations of racism.[34] Stereotypes can contribute to hostility towards a group, engender toxic prejudices, and are often used to justify discrimination and oppression. The propagation of stereotypes results, according to linguist Karen Stollznow, in those with red hair frequently having ""low self-esteem ... [experiencing] insecurity, and ... [feeling] a profound sense of being not only different from other people but also inferior"".[19] Stereotypes about people with red hair include the ideas that they are in league with Satanic forces, or of Irish ancestry, both of which are not supported by evidence.[19] In some areas, people with red hair may be stereotyped as more ""competent"" than persons with other hair colors, which may manifest in the form of a reverse discriminatory selection bias in which persons with red hair are placed into leadership positions over other humans at atypically high rates.[35] A study in the UK found that the number of CEOs of top companies with red hair was four times higher than the percentage of persons with red hair in the general population.[35] Other stereotypes include that red-haired persons have a propensity to violence or are short-tempered, which are not directly supported by scientific evidence, though some research suggests they produce higher levels of adrenaline which accelerates the fight-or-flight response.[19][36] A 1901 eugenicist study in the United States concluded that ""red hair is infrequent among born criminals but abundant among the insane and sexual offenders"".[37] One contemporary study has shown that persons with red hair are significantly less likely to experience mental illness.[38] A 1946 study by Hans von Hentig published in the Journal of Criminal Law and Criminology observed that there was a high prevalence of red hair among high-profile criminals in the Old West and that ""the number of redheaded men among the noted outlaws surpassed their rate in the normal population"". Von Hentig, however, attributed this not to a numerically higher incidence of crime among red haired persons but because ""of their striking appearance, they might have been remembered rather than ordinary men who killed and were killed.""[39] It is also reported that many red-haired persons are perceived as ""soulless"" and had the ability to steal your soul if you looked in their eyes long enough.[40] Men have been stereotyped as being well endowed, and both men and women with red hair have been stereotyped as sexually promiscuous or having unusually active libidos.[35] Recent research has suggested that women with red hair are more sexually active, exhibit lower sexual dominance (i.e. more sexually permissive), have more sex partners throughout life, and initiate sex earlier in life on average than other women.[41] Researchers have suggested men and women with red hair engage in sexual intercourse with greater frequency than others due to a combination of the psychology of color that preferences red, and their ability to attract attention through hair color uniqueness, though this isn't as certain in redheaded men.[41][42][43] Studies have shown that people with red hair experience pain differently than others. Recent research suggests that redheads have a generally higher pain tolerance and respond more effectively (i.e. lower doses) to opiate medication.[44] Persons with red hair may also experience changes in temperature faster and with greater intensity than others and biosynthesize Vitamin D more efficiently than those without red hair.[43] For unclear reasons related to the MC1R mutation, men with red hair are significantly less likely to develop prostate cancer than others.[45] Because the MC1R mutation does not bind to the PTEN gene, persons with red hair are more at risk of melanoma and benefit from limited sun exposure.[43] For unknown reasons, they are also significantly more likely to develop Parkinson's disease than persons without red hair.[43] Women with red hair have a higher prevalence of endometriosis.[46] It has been hypothesized that these problems are due to the fact that genetic factors causing red hair emerged at a later date than those for other hair colors and have not yet had an opportunity to benefit from corrective evolution.[38] The term ""ginger"" is considered by some to be pejorative or offensive, with some considering it only acceptable when used by a person with red hair to refer to themselves or others with red hair.[19][22][47][48] The use of the term to refer to persons with red hair may be a reference to the spicy ginger root, an amplification of the stereotype that persons with red hair have abrupt tempers or are prone to violence.[49][50] The phrase ""redheaded stepchild"" is a term used, mainly in the United States, to describe a ""person or thing that is neglected, unwanted, or mistreated"".[51] Using ""Red"" as a nickname to refer to a person with red hair has been described as overly familiar and potentially offensive.[19] The ""white-skinned other"" is considered a prejudicial term to refer to Caucasians with red hair.[19] ""Ranga"" is a slang term used in New Zealand and Australians to refer to a person with red hair and is an abbreviation of ""orangutan"", a subhuman primate.[52] It is considered an insult.[53]Andrew Rochford has called for Australians and New Zealanders to stop using it.[54] According to the Associated Press Stylebook, ""red-haired, redhead and redheaded are all acceptable for a person with red hair.""[55] Some people with red hair prefer the term auburn to describe their hair color.[19] American author Mark Twain, who had red hair, said that auburn is typically a color descriptor used for persons with red hair of higher social class.[12]"
Discrimination based on hair texture in the United States,/wiki/Discrimination_based_on_hair_texture_in_the_United_States,"In the United States, discrimination based on hair texture is a form of social injustice that has been predominantly experienced by African Americans and predates the founding of the country.[citation needed] In the 21st century, multiple states and local governments have passed laws that prohibit such discrimination. California was the first state to do so in 2019 with the CROWN (Create a Respectful and Open Workplace for Natural Hair) Act (SB 188). As of June 2023, twenty-three states have passed similar legislation, but there is no equivalent law at the federal level. A federal CROWN act was proposed in 2020, and was passed by the House of Representatives but not the Senate. Another bill was introduced in the House of Representatives in 2021; it was approved by the House in 2022, and awaits consideration in the Senate. In the late 1700s, free Africans in New Orleans were able to buy their freedom from slavery, resulting in an increase of interracial marriage in Louisiana.[1] In response, Charles III of Spain demanded Louisiana colonial governor Esteban Rodríguez Miró to ""'establish public order and proper standards of morality,' with specific reference to a ""large class"" of ""mulattos"" and particularly ""mulatto"" women.'""[2] Louisiana women of African descent wore hairstyles that incorporated feathers and jewels, which caught the attention of white men. To comply with Charles III's demand, Miró issued an edict that required Creole women to wear a tignon to conceal their hair.[1] By the late 1800s, African American women were straightening their hair to meet a Eurocentric vision of society with the use of hot combs and other products improved by Madam C. J. Walker. However, the black pride movement of the 1960s and 1970s made the afro a popular hairstyle among African Americans and considered a symbol of resistance.[1] In 1964, the U.S. federal government passed the Civil Rights Act, which prohibited employment discrimination based on race, but it was left to interpretation by the courts as to what this constituted.[3] In 1970, Beverly Jenkins was denied a promotion in the Blue Cross by her white supervisor due to her afro.[4] In 1976, the federal court case Jenkins v. Blue Cross Mutual Hospital Insurance determined that afros were protected by Title VII of the Civil Rights Act of 1964. However, the case did not extend protections against hair discrimination.[3] In the 2010s, natural hairstyles saw an increase in popularity in response to celebrities such as Viola Davis, Lupita Nyong'o, Ava DuVernay, and Stacey Abrams wearing natural hair. However, the popularity also resulted in increased attention to dress codes and hair regulations as African American workers and students across the U.S. were subjected to punishment due to their hair.[5] Because of awareness to the issue, California passed the Crown Act in July 2019, becoming the first U.S. state to prohibit discrimination against workers and students based on their natural hair.[6] California's passage of the bill has led many other states to consider similar bills banning hair discrimination and a bill proposed at the federal level by U.S. representative Cedric Richmond and U.S. senator Cory Booker.[7] In September 2020, U.S. representative Ilhan Omar announced the passage of the Crown Act in the House of Representatives, which would prohibit racialized hair discrimination nationally if enacted.[8] As of June 15, 2023, 23 U.S. states have prohibited discrimination based on hair texture. In addition, similar legislation has been introduced in Florida, Georgia, Kansas, Kentucky, Mississippi, Missouri, Nebraska, New Hampshire, North Carolina, Ohio, Pennsylvania, Rhode Island, South Carolina, Utah, and Wisconsin. An executive order prohibiting discrimination by Arizona state agencies and contractors was signed by Governor Katie Hobbs on March 17, 2023.[32]"
Executive Order 11478,/wiki/Executive_Order_11478,"Executive Order 11478, signed by U.S. President Richard M. Nixon on August 8, 1969, prohibited discrimination in the competitive service of the federal civilian workforce on certain grounds. The order was later amended to cover additional protected classes. Executive Order 11478 covered the federal civilian workforce, including the United States Postal Service and civilian employees of the United States Armed Forces. It prohibited discrimination in employment on the basis of race, color, religion, sex, national origin, handicap, and age.[1] It required all departments and agencies to take affirmative steps to promote employment opportunities for those classes it covered.[1] Executive Order 11478 assigned the Equal Employment Opportunity Commission responsibility for directing the implementation of the Order's policies and to issue rules to promote its principles. Agencies were in turn required to abide by those rules and produce any reports the EEOC required.[1] Following the release of Executive Order 11478, the Federal Women's Program (FWP) was integrated into the EEOC program and placed under the direction of EEOC for each agency.[2] The Federal Personnel Manual 713 was issued to carry out Executive Order 11478.[3] On April 23, 1971, Executive Order 11590 amended Executive Order 11478 to apply to the recently reorganized United States Postal Service and to the newly established Postal Rate Commission.[4] On December 28, 1978, Executive Order 12106 amended Executive order 11478 to transfer certain functions of the Civil Service Commission to the EEOC.[5] On May 28, 1998, President Bill Clinton signed Executive Order 13087, which amended Executive Order 11478 to add sexual orientation to the list of classes covered.[6] On May 2, 2000, Executive Order 13152 amended Executive Order 11478 to add ""status as a parent"" to the list of categories and defined that term in detail.[7] On July 21, 2014, Executive Order 13672 amended Executive Order 11478 and Executive Order 11246 to add sexual orientation and gender identity.[8]"
Executive Order 12968,/wiki/Executive_Order_12968,"Executive Order 12968 was signed by U.S. President Bill Clinton on August 2, 1995. It established uniform policies for allowing employees of the federal government access to classified information. It detailed standards for disclosure, eligibility requirements and levels of access, and administrative procedures for granting or denying access and for appealing such determinations.[1] It expanded on the President Dwight D. Eisenhower's Executive Order 10450 of 1953.[2] Executive Order 12968 required as an initial condition of access to classified information the filing of financial disclosure statements ""including information with respect to the spouse and dependent children of the employee"" with possible annual updates, as well as the reporting of all foreign travel. These requirements constituted a response to the recent Aldrich Ames spy case. In another innovation, those receiving security clearances would now have to provide information that the government previously had to acquire through its own investigations.[1] As a counterbalance to the new burdens placed on employees, Executive Order 12968 detailed that an applicant for a security clearance had a right to a hearing and to a written explanation and documentation if denied.[1] Civil liberties groups expressed concerns about the intrusiveness of the disclosure requirements.[1] The usefulness of the financial information remained a subject of debate.[3] Executive Order 12968's anti-discrimination statement, ""The United States Government does not discriminate on the basis of race, color, religion, sex, national origin, disability, or sexual orientation in granting access to classified information."" responded to longstanding complaints by advocates for gay and lesbian rights by including ""sexual orientation"" for the first time in an Executive Order. It also said that ""no inference"" about suitability for access to classified information ""may be raised solely on the basis of the sexual orientation of the employee.""[1] The federal government had for decades assumed that homosexuality constituted a disqualification for holding a security clearance, despite the opposite findings of the U.S. Navy's Crittenden Report in 1957.[4] A 1990 U.S. Appeals Court decision, High Tech Gays v. Defense Industrial Security Clearance Office, upheld the denial of security clearances to homosexual employees of government contractors. In 1992, U.S. Army Col. Margarethe Cammermeyer had revealed she was a lesbian during a review of her top secret security clearance and received an honorable discharge, and her subsequent lawsuit helped keep the issue in the news.[5] Elizabeth Birch, executive director of the Human Rights Campaign Fund, called the Executive Order ""an important step toward ending governmentally sanctioned job-discrimination against gay and lesbian people.""[1] An analyst for the Family Research Council, a conservative group, issued a statement saying homosexuality ""is a behavior that is associated with a lot of anti-security markers such as drug and alcohol abuse, promiscuity and violence"" and ""in all healthy societies, homosexuality is recognized as a pathology with very serious implications for a person's behavior.""[1] It raised the issue of blackmail as well: ""If someone is an avowed homosexual and that is well known, the vulnerability to blackmail is not nearly as pertinent. Fortunately or unfortunately, the vast majority of homosexuals in this country are not wearing that on their lapel pin.""[6]Franklin Kameny, whose homosexuality prompted his firing from government service in 1957 and who had participated in the campaign to end the ban on homosexuals in the Federal Civil Service that proved successful in 1975, said: ""There has been a gradual falloff in enforcement over the years. What this represents is the next step. The Government has gone beyond simply ceasing to be a hostile and vicious adversary and has now become an ally.""[1] Representative Barney Frank, whom presidential advisor George Stephanopoulos called a ""dogged advocate"" for the new policy, said: ""It relieves an enormous strain in the lives of many decent people. It's one more denunciation of the myth that gay or lesbian people are less than full, good citizens.""[6] In 1996 the Servicemembers Legal Defense Network, an organization that advocates on behalf of gays and lesbians in the U.S. military, reported that it discovered ""fewer cases involving security violations"", that is, inappropriate questioning about sexual orientation, following the issuance of this Executive Order.[7] Executive Order 12968 also addressed evaluating the mental health of an employee seeking a security clearance. It included a proviso that ""No negative inference"" about eligibility ""may be raised solely on the basis of mental health counseling.""[1] In 1997, Daniel Patrick Moynihan noted in that the nondiscrimination promises of this Executive Order and its guarantees of transparency with respect to reasons for denying a security clearance had yet to be fully implemented.[8] Executive Order 12968 was amended by Executive Order 13467 on June 30, 2008.[9]"
Executive Order 13087,/wiki/Executive_Order_13087,"Executive Order 13087 was signed by U.S. President Bill Clinton on May 28, 1998, amending Executive Order 11478 to prohibit discrimination based on sexual orientation in the competitive service of the federal civilian workforce. The order also applies to employees of the government of the District of Columbia, and the United States Postal Service. However, it does not apply to positions and agencies in the excepted service, such as the Central Intelligence Agency, National Security Agency, and the Federal Bureau of Investigation. In a statement issued the same day that he signed the order, President Clinton said:[1] The Executive Order states Administration policy but does not and cannot create any new enforcement rights (such as the ability to proceed before the Equal Employment Opportunity Commission). Those rights can be granted only by legislation passed by the Congress, such as the Employment Non-Discrimination Act. Federal employees cannot appeal claims of discrimination under Executive Order 13087 to the EEOC, but they can file complaints under the grievance procedure of the agency where they work and, under certain conditions, may appeal their claims to the Merit Systems Protection Board or the Office of Special Counsel.[2] Clinton had previously included ""sexual orientation"" in Executive Order 12968 (1995) when listing the characteristics forbidden as the basis for discrimination when granting federal employees access to classified information. The order applied to civilian employees of the American military,[3] but not to uniformed members of the armed forces, who, at the time, were covered by the Don't ask, don't tell directive issued by Clinton in 1993.[4] Opponents in Congress objected to the Order and said that it provided special privileges and ""special breaks for special interests,""[5] Donald Devine, who headed the Office of Personnel Management from 1981 to 1995, criticized Clinton's decision to implement the non-discrimination policy by issuing an Executive Order, ""issued out of the glare of public attention,"" and called on Congress to act to undo the President's action ""before it can do much damage both to the orderly management of the government and to its equal employment policies generally.""[6] On June 11, 1998, the conservative Southern Baptist Convention passed a resolution asking the President to rescind the order and demanding that Congress nullify it if he did not do so.[5] Later in 1998, several congressmen, including Republicans Bob Barr of Georgia and Joel Hefley of Colorado, introduced bills designed to overturn 13087 or to prohibit government agencies from spending any funds to enforce it. In August 1998, an amendment to the Commerce, Justice, State, Judiciary, and Related Agencies appropriations bill that sought to prohibit spending on behalf of 13087 failed in the House of Representatives on a vote of 176 to 252, the only recorded vote on the issue.[6] According to the Equal Employment Opportunity Commission:[7] Executive Order 13087 did not create any new rights; however, it did set the stage for positive and constructive action by all units of the federal government to make certain that the workplace is one free from harassment and discrimination. In 2005, the Human Rights Campaign and others claimed that ""the head of the Office of Special Counsel, Scott Bloch, refuses to enforce these longstanding non-discrimination protections.""[8]"
Gender-based price discrimination in the United States,/wiki/Gender-based_price_discrimination_in_the_United_States,"Gender-based price discrimination is a form of economic discrimination that involves price disparities for identical goods or services based on an individual's gender, and may reinforce negative stereotypes about both women and men in matching markets. Race and class-based price discrimination also exists.[1] Acts of discrimination often have legal ramifications, but whether gendered price disparities prove an intent to discriminate or constitute illegal discrimination can become a legal inquiry. Policies against gender-based price discrimination is not universally approved and enforced in the United States.[2] Gender-based price discrimination is also described as pink tax. Gender-based price discrimination exists in many industries including insurance, dry cleaning, hairdressing, nightclubs, clothing, personal care products, discount prices and consumption taxes. A study by the New York City Department of Consumer and Worker Protection found that, on average, women's products cost seven percent more than similar products for men.[3] The utilization of big data in business also apply to personalized price discrimination which involves the factor of a consumer's gender.[4] Whether gender-based pricing is a form of discrimination, and whether it is illegal has been of a debate in the United States and European Union since the 1990s.[5] Opponents of the enforcement of laws against gender-based pricing make two arguments. They suggest that courts should dismiss cases involving gender-based pricing because the injury to the plaintiff is so inconsequential that they should not be entitled to relief. They also point to economic efficiency as a justification.[6] In response to the economic efficiency argument, scholars suggest that gender-based pricing should be prohibited on moral grounds, stating that gender should not be used as a proxy for other characteristics, especially when based on stereotypes.[6][7] Gender research has heavily focused on the interaction between gender and the economy. Typically, research in this area involves the issue of the gender pay gap. Another aspect of gender research in economics is the less studied issue of gender-based price disparities in the cost of goods and services across different industries. Scholars suggest that the existence of gendered price disparities contribute to gender inequality by creating an economic burden that does not exist for the other gender and by reinforcing gender roles in the marketplace.[1] There are a number of different studies on the price disparities between personal care products and services that are marketed towards females and males. For example, the New York City Department of Consumer and Worker Protection (DCA) conducted a study of prices of goods in New York City across five industries, including personal care products.[3] Other universities and academics have also studied the prices in personal care products and services.[1] However, it is also argued that if there is no barrier or prohibition to the consumer buying the cheaper product, the consumer must find added value in the more expensive product and thus there is no real discrimination. While studies have shown significant price disparities in personal care products between men and women, gendered price disparities across personal care services has been inconsistent.[1] A recent study titled Cost of Doing Femininity examined two areas of personal care services that had directly comparable prices between men and women: hair salons and dry cleaners. The study found that only 15 out of 100 randomly selected hair salons had the same prices for both men and women, and none of the salons charged women less than men. Dry cleaning prices depended on the type and amount of fabric, with more embellishments corresponding with higher prices. This price factor, however, tended to negatively impact women more often than men because women's garments are more likely to be embroidered or be made of delicate fabric. Gendered price disparities in personal care products are more apparent than in personal care services and across other industries. By and large, the price disparities in personal care products are notably higher than in other industries and cost women around 13 percent more than men. This disparity is especially significant considering women purchase these products more regularly than men.[3][8][9] Prices for hair products, followed by razors, cost the most for women - typically costing women almost 50 percent more than men.[3] Price disparities in hair salon services between men and women are thought to be justifiable because women's hair is often longer and more complicated to maintain and cut. In recent years, however, this stereotype has changed. Men are often experimenting with their hair, including hair loss treatment and hair color. While women may still spend a considerable amount for hair color and other treatments, many women prefer basic haircuts. Additionally, salon-quality hair styling tools are readily available and easy to use at home.[10] Gendered price disparities for hair salon services have also been documented in different locations across the United States and Europe. The California Assembly Office of Research conducted a survey of five large California cities and found that forty percent of the hair salons charged women, on average, five dollars more than men for a standard haircut.[11] Gendered price disparities in haircut prices also has been found in New York City: Of 199 hair salons examined, nearly 48 percent of hair-cutters charged women more than men for a simple haircut.[1] The price disparities between men and women in the vehicle insurance market is one of the few instances where men typically pay more than women for identical products and services, however, men start paying less than women as age increases.[12] Unlike in the markets for retail or personal care products, however, these gender-based price differences can be rationally explained. Because men are believed in general to behave in more risky behaviors than women (driving at excessive speeds, driving recklessly, driving under the influence of drugs and/or alcohol, etc.), the cost of insuring men is greater than the cost of insuring women, and this cost difference is reflected in insurance rates calculated in part on the gender of the insured.[13] Despite vehicle insurance typically costing men more, there is some research to suggest that women actually pay more under the fixed annual pricing system because men drive more miles and are involved in twice as many accidents.[14] For most Americans, new car purchases are their largest consumer investment after buying a home. In 1991, Harvard Law Professor Ian Ayres examined whether the process of negotiating for a new car disadvantaged women and minorities. The study was conducted in the Chicago area and involved 180 price negotiations at 90 dealerships. The study's testers included individuals of different races and genders and each was instructed to use the same negotiation strategy. Each tester entered a car dealership and bargained to purchase a new car. The results of the study concluded that white males received significantly better prices than non-whites and women.[15][16][17] It is argued women tend to pay more than men for health insurance. One explanation for this is that women make more use of the health care services made available to them by their insurance.[18] A detailed survey of medical costs of the period 1999 to 2016 by the U.S. Department of Health and Human Services shows spending on healthcare for women is higher than for men. According to their study, during the ages of 18 to 44, health spending for females is 84% higher than men. Even when removing the cost of childbirth, spending for women ages 44 to 64 was still 24% higher than for men. Spending for women ages 65+ is 8% higher.[19] Adjusting for the cost of treatment due to injuries sustained while in the military makes the disparity greater still. Nevertheless, the Affordable Care Act made gender-based differences in premium prices illegal. Discount prices is also a type of gender-based price discrimination that segmenting customers based on the factor of gender. A common gender-based price discount is a ""ladies' night"" promotion, in which female patrons pay less for alcoholic drinks or a lower cover charge than male patrons do.[6] Consumption taxes on certain products but not others have also been viewed a form of gender-based price disparity. For example, in the United States and Australia, feminine hygiene products such as menstrual pads and tampons are often subjected to a consumption tax, while related products such as condoms, lubricant, and several other medical items are exempt from the tax.[20] The Supreme Court of the United States has not enunciated a workable evidentiary standard to govern claims of subtle and unconscious forms of discrimination.[15] Further, there is no general federal law explicitly prohibiting gender-based price discrimination.[15][21] Because many pricing decisions are made by private businesses, the 14th Amendment generally does not apply, and sex was not included as a protected class under federal public accommodation law. As a result, these issues tend to be left to the states. In recent years, an increasing number of states and localities have enacted laws and policies prohibiting gender-based price discrimination in public accommodations, such as nightclubs and bars.[22][6] For example, courts in California, Florida, Pennsylvania, Iowa, and Maryland have consistently ruled against gender-based pricing. However, other states such as Illinois, Washington, and Michigan, have adopted a case-by-case approach on the issue. California passed the Unruh Civil Rights Act in 1959, making California the first state to enact legislation to protect against gender-based price discrimination.[23] California Assemblywoman Jackie Speier introduced the Gender Tax Repeal Act to ""try to address the persistent problem of gender-based discrimination in the sale of services related to haircuts, laundry, dry cleaning, and alterations.""[24] The law was enacted to prohibit businesses from charging different prices for services based on a customer's gender.[25][a] The Gender Tax Repeal Act provided for civil actions in which courts may award a minimum of $1,000 or up to three times the amount of actual damages in addition to attorney's fees.[26] The enactment of the Gender Tax Repeal Act aided in combating gender-based price discrimination in the pricing of services, but did not prohibit such price differentials with respect to products. On January 21, 2016, California State Senator Ben Hueso introduced Senate Bill 899[27] to extend prohibiting gender-based price discrimination from services to products.[20] Senate Bill 899 proposed to prohibit businesses from price discriminating with respect to goods of ""similar or like kind"" based on a customer's gender. On March 31, 2016, Senator Hueso introduced amendments clarifying that businesses may charge more for a product if there are legitimate differences in costs of labor, materials, or tariffs.[28] The amendments also clarified that ""substantially similar"" products included products of the same brand, share the same functional components, and share 90 percent of the same materials and ingredients.[28] Senate Bill 899, sponsored by Consumer Federation of California, received support from the American Civil Liberties Union of California, California Public Interest Research Group, Planned Parenthood Affiliates of California, and Women's Foundation of California.[28] On April 12, 2016, the Senate Judiciary Committee voted 5–1 in favor of Senate Bill 899 and will advance the bill with a Senate Hearing.[29] In 1997, Miami-Dade County in Florida passed an ordinance prohibiting businesses from charging different prices for products or services based solely on the customer's gender. However, businesses are permitted to charge a different price for products or services that involve more time, difficulty, or cost.[30][b] Miami-Dade County's ordinance also permits limited discount programs based on gender.[30] For instance, a business may advertise ""Ladies Free on Friday Night"", as long as men are not prevented from patronizing the business.[31]"
The Good Citizen,/wiki/The_Good_Citizen," The Good Citizen was a sixteen-page monthly political periodical edited by Bishop Alma White and illustrated by Reverend Branford Clarke.[1]The Good Citizen was published from 1913 until 1933 by the Pillar of Fire Church at their headquarters in Zarephath, New Jersey in the United States. White used the publication to expose ""political Romanism in its efforts to gain the ascendancy in the U.S.""[1][2] In 1915, the publication's anti-Catholic rhetoric aroused the local population in Plainfield, New Jersey and a mob formed to threaten the Pillar of Fire Church.[3] By 1921, the publication was a strong supporter of the Ku Klux Klan.[4][5] The Good Citizen espoused the political views of Alma White and consisted of essays, speeches and cartoons promoting women's equality, anti-Catholicism, antisemitism, nativism, white supremacy and the Ku Klux Klan.[6][7][8] The tract also contained numerous topically provocative illustrations by Reverend Branford Clarke. According to Wyn Craig Wade in his 1998 book The Fiery Cross: [Alma White] was also probably the most active and prolific fundamentalist minister in the 1920s. ... Her value to the Klan, however, came from her viciously anti-Catholic magazine, The Good Citizen, and her easily readable theological tracts that simultaneously found scriptural support for the Invisible Empire [the KKK] and excoriation for the Catholic Church. ... Her books abounded with conspiracy themes: ""We hail the K.K.K. in the great movement that is now on foot ... Were it not that the press is throttled by Rome and her Hebrew allies, astounding revelations would be made, showing the public the necessity for the rising of the Heroes of the Fiery Cross.""[5] The following is from the text of a speech given by Alma White on December 31, 1922 at the Pillar of Fire Church at 123 Sterling Place, Brooklyn, New York and published in the February, 1923 (Vol. 11 No. 2) edition of The Good Citizen. The speech is titled ""Ku Klux Klan and Woman's Causes"" and the section of the speech which is reprinted below is titled ""White Supremacy."" The Klansmen stand for the supremacy of the white race, which is perfectly legitimate and in accordance with the teachings of the Holy Writ, and anything that has been decreed by the Almighty should not work a hardship on the colored race...It is within the rights of civilization for the white race to hold the supremacy; and no injustice to the colored man to stay in the environment where he was placed by the Creator. ... When the black man was liberated it was time for women to be enfranchised, without which the colored man with his newly acquired rank became her political master. Such reflections make one feel that man's delinquency has been almost unpardonable. ...The white women bore the sting of humiliation for more than half a century in being placed in an inferior position to the black men in the use of the ballot and the rights of citizenship... To whom shall we look to champion the cause and to protect the rights of women? Is there not evidence that the Knights of the Klu [sic] Klux Klan are the prophets of a new and better age?[9] White published three books that were compendiums of the essays, speeches and cartoons from it entitled The Ku Klux Klan in Prophecy (1925), Klansmen: Guardians of Liberty (1926), and Heroes of the Fiery Cross (1928). In 1943 White reprinted her Klan books as a three volume set under the title Guardians of Liberty. In 1929, Ray Bridwell White, White's son and president of Zarephath Bible Institute published The Truth in Satire Concerning Infallible Popes which also was a compendium of his essays that had originally been published in The Good Citizen. In 1918, 500 members of the New York City Roman Catholic parish of Our Lady of Lourdes called on the United States Postmaster General to exclude The Good Citizen from the US mail.[3][10] Copies of The Good Citizen are available in six US libraries:"
Housing discrimination in the United States,/wiki/Housing_discrimination_in_the_United_States,"Housing discrimination in the United States refers to the historical and current barriers, policies, and biases that prevent equitable access to housing. Housing discrimination became more pronounced after the abolition of slavery in 1865, typically as part of Jim Crow laws that enforced racial segregation. The federal government didn't begin to take action against these laws until 1917, when the Supreme Court struck down ordinances prohibiting blacks from occupying or owning buildings in majority-white neighborhoods in Buchanan v. Warley. However, the federal government as well as local governments continued to be directly responsible for housing discrimination through redlining and race-restricted covenants until the Civil Rights Act of 1968. This Act included legislation known as the Fair Housing Act, which made it unlawful for a landlord to discriminate against or prefer a potential tenant based on their race, color, religion, gender, or national origin, when advertising or negotiating the sale or rental of housing. Such protections have also been extended to other ""protected classes"" including disabilities and familial status. Despite these efforts, studies have shown that housing discrimination still exists and that the resulting segregation has led to wealth, educational, and health disparities.[1] The prevalence of housing discrimination and redlining in the United States has led to wide-ranging impacts upon various aspects of the structure of society, such as housing inequality and educational inequality. These phenomena can be seen through the lens of critical race theory as examples of systemic racism. [2][3][4] After the end of the Civil War and the abolition of slavery, Jim Crow laws were introduced.[5] These laws led to the discrimination of racial and ethnic minorities, especially African Americans. While Jim Crow laws spread throughout the South, more subtle discriminatory practices were implemented in the North. Between 1900 and 1920, there was growth in the African American population, many of whom migrated to the North as part of the Great Migration. This led to a reaction by whites in Northern cities, fueling housing discrimination and residential segregation, which had previously not been as visible. Institutional tools such as zoning and racially restricted covenants were used as a way to stop the spread of African Americans into white neighborhoods.[6] In 1926, racially restrictive covenants were upheld by the Supreme Court case Corrigan v. Buckley. After this ruling, these covenants became popular across the country as a way to guarantee white, homogeneous neighborhoods.[7] In Village of Euclid v. Ambler Realty Co. in 1926, the Supreme Court also upheld exclusionary zoning. Fifteen state courts obeyed ordinances that enforced the denial of housing to African American and other minority groups in white-zoned areas. In the 1917 Supreme Court case Buchanan v. Warley, the court ruled that a Louisville, Kentucky ordinance prohibiting blacks from owning or occupying buildings in a majority-white neighborhood, and vice versa, was unconstitutional. Following this decision, however, nineteen states began legally supporting ""covenants,"" or agreements, between property owners to not rent or sell any homes to racial or ethnic minorities. Although the covenants were later made illegal in 1948, they were still allowed to be in private deeds.[8] The Federal Housing Administration (FHA) was responsible for much of the housing discrimination in the US due to explicit racially discriminatory policies. The FHA believed allowing Black Americans to live in white neighborhood would decrease the property value. This justification, however, was disproven as Black Americans were willing to pay higher prices to live in those neighborhoods.[7] The FHA was also responsible for denying mortgage insurance to Black neighborhoods, a practice known as redlining. This entailed categorizing neighborhoods according to risk level for lending. This risk level depended largely upon the racial composition of these neighborhoods.[9] The FHA's Underwriting Manual explicitly encouraged ""prohibition of the occupancy of properties except by the race for which they were intended.""[10] Encouraged by the FHA, developers established deed restrictions to maintain white neighborhoods. The GI bill allowed many veterans to become homeowners, leading to a housing boom. However, this bill did not support Black veterans in the same way because mortgages and loans were not provided by the United States Department of Veterans Affairs but by private mortgage lenders who often discriminated through redlining. Levittown was a neighborhood built to provide affordable housing for returning veterans from World War II, but the developer refused to allow people of color to live there. The FHA backed this decision by authorizing loans and providing racially-restricted deeds.[7] It was not until the Fair Housing Act, enacted as Title VIII of the Civil Rights Act of 1968, that the federal government made its first concrete steps to deem all types of housing discrimination unconstitutional.[11] The act explicitly prohibits housing discrimination practices common at the time, including filtering information about a home's availability, racial steering, blockbusting, and redlining.[12] The Fair Housing Act was passed at the urging of President Lyndon B. Johnson. Congress passed the federal Fair Housing Act (codified at 42 U.S.C. 3601-3619, penalties for violation at 42 U.S.C. 3631) Title VIII of the Civil Rights Act of 1968 only one week after the assassination of Martin Luther King Jr. The Fair Housing Act introduced meaningful federal enforcement mechanisms. It outlawed: When the Fair Housing Act was first enacted, it prohibited discrimination only on the basis of race, color, religion, sex, and national origin.[13] In 1988, disability and familial status (the presence or anticipated presence of children under 18 in a household) were added (further codified in the Americans with Disabilities Act of 1990).[13] In certain circumstances, the law allows limited exceptions for discrimination based on sex, religion, or familial status.[14] The United States Department of Housing and Urban Development is the federal executive department with the statutory authority to administer and enforce the Fair Housing Act. The Secretary of Housing and Urban Development has delegated fair housing enforcement and compliance activities to HUD's Office of Fair Housing and Equal Opportunity (FHEO) and HUD's Office of General Counsel. FHEO is one of the United States' largest federal civil rights agencies. It has a staff of more than 600 people located in 54 offices around the United States. As of June 2014, the head of FHEO is Assistant Secretary for Fair Housing and Equal Opportunity Gustavo Velasquez, whose appointment was confirmed on June 19, 2014.[15] Individuals who believe they have experienced housing discrimination can file a complaint with FHEO at no charge. FHEO funds and has working agreements with many state and local governmental agencies where ""substantially equivalent"" fair housing laws are in place. Under these agreements, FHEO refers complaints to the state or locality where the alleged incident occurred, and those agencies investigate and process the case instead of FHEO. This is known as FHEO's Fair Housing Assistance Program (or ""FHAP""). There is also a network of private, non-profit fair housing advocacy organizations throughout the country. Some are funded by FHEO's Fair Housing Initiatives Program (or ""FHIP""), and some operate with private donations or grants from other sources. However, victims of housing discrimination need not go through HUD or any other governmental agency to pursue their rights. The Fair Housing Act confers jurisdiction to hear cases on federal district courts. The United States Department of Justice also has jurisdiction to file cases on behalf of the United States where there is a pattern and practice of discrimination or where HUD has found discrimination in a case and either party elects to go to federal court instead of continuing in the HUD administrative process. The Fair Housing Act applies to landlords renting or leasing space in their primary residence only if the residence contains living quarters occupied or intended to be occupied by three or more other families living independently of each other, such as an owner-occupied rooming house. The Fair Housing Act gave the Department of Housing and Urban Development the power of enforcement, but the enforcement mechanisms were weak.[16] The Fair Housing Act has been strengthened since its adoption in 1968, but enforcement continues to be a concern among housing advocates. According to a 2010 evaluation of Analysis of Impediments (AI) reports done by the Government Accountability Office, enforcement is particularly inconsistent across local jurisdictions.[17] There have been calls for HUD to use disparate impact as a measure of housing discrimination. HUD's disparate impact rule was strengthened in 2013 and upheld in a court case in 2015. However, in 2020, HUD issued its final disparate impact rule, which shifted the burden of proof of discrimination to the victims of housing discrimination.[18] In 1968, the Kerner Commission report was released, which called for investment in housing to reduce residential segregation.[19] The federal government has passed other initiatives in addition to the Fair Housing Act of 1968. The Equal Credit Opportunity Act of 1974 and Community Reinvestment Act of 1977 helped with discrimination in mortgage lending and lenders' problems with credit needs.[20] The Fair Housing Amendments Act of 1988 was passed to give the federal government the power to enforce the original Fair Housing Act to correct past problems with enforcement.[21] The amendment established a system of administrative law judges to hear cases brought to them by the United States Department of Housing and Urban Development and to levy fines.[22] Because of the relationship between housing discrimination cases and private agencies, the federal government passed the two initiatives. The Fair Housing Assistance Program of 1984 was passed to assist public agencies with processing complaints, and the Fair Housing Initiatives program of 1986 supported private and public fair housing agencies in their activities, such as auditing.[21] Between 1990 and 2001, these two programs have resulted in over one thousand housing discrimination lawsuits and over $155 million in financial recovery.[21] However, the lawsuits and financial recoveries generated from fair housing discrimination cases only scratches the surface of all instances of discrimination. Silverman and Patterson concluded that the underfunding and poor implementation of federal, state and local policies designed to address housing discrimination resulted in less than 1% of all instances of discrimination being addressed.[23] Moreover, they found that local nonprofits and administrators responsible for enforcing fair housing laws had a tendency to downplay discrimination based on family status and race when designing implementation strategies.[24] Some states have passed laws on top of the Fair Housing Act that also outlaw housing discrimination based on the source of funding, particularly to combat landlords who openly refuse to serve tenants using Section 8 vouchers.[25] Housing vouchers have been promoted as a way to provide affordable housing for low-income households and promote housing choice. However, studies have found that using vouchers is a difficult and discouraging process as many landlords refuse vouchers.[26] The United States Census has shown that ethnic and racial minorities living in concentrated, high-poverty areas had actually increased following the passage of the Fair Housing Act from 1970 to 1990.[27] African-Americans residing in these areas rose from 16 percent to 24 percent, and Hispanics living in these areas have increased from 10 percent to 15 percent.[28] While this does not necessarily point to evidence of housing discrimination, it does mirror the phenomenon of white flight—the mass exodus during the 1970s and '80s of European-Americans from cities to the suburbs that left only one-fourth of the Anglo population still living in metropolitan areas. American sociologist Douglas Massey, in his essay, ""The New Geography of Inequality in Urban America"", argues that this new racial geography in the United States has laid the foundation for housing discrimination to occur in order to keep up the status quo.[28] After the passage of the Fair Housing Act and the end of redlining and more explicitly discriminatory practices, ""predatory inclusion"" began.[29] Housing and Urban Development and Federal Housing Authority officials encouraged the spread of homeownership among low-income African Americans. There was an emphasis on a ""public-private partnership,"" and the private sector was seen as the way to end the housing crisis.[29] However, the terms of mortgages and loans were at much worse rates than those for white households. Furthermore, the houses were often in poor conditions. Mortgage banks were unregulated and sought out individuals determined as high risk because their mortgages would be backed by the FHA. Thus, loan defaults and foreclosures created profit for mortgage banks at the expense of African American homeowners. There are two types of housing discrimination: exclusionary and non-exclusionary. Exclusionary refers to limiting one's access to housing while one is seeking to rent or buy housing, while non-exclusionary refers to discriminatory treatment within one's current housing.[30] Certain policies that do not explicitly discriminate have also been found to cause housing discrimination in the United States. Disparate impact is a facially neutral housing policy that negatively impacts minorities or other protected groups of people.[31] The Supreme Court upheld the decades long practice of holding housing providers liable for housing discrimination under a disparate impact theory in 2015.[32] Following the Supreme Court decision, HUD issued an opinion from their Office of General Counsel concluding that blanket prohibitions against tenants with criminal convictions would constitute disparate impact housing discrimination because some people are psychologically more likely to be criminals.[33] Disparate impact remains controversial among industry and business professionals because some feel that their freedom in implementing policies and rules is now limited .[34] John Yinger argued that discriminatory housing practices in the housing market have led to segregation, citing examples such as realtors opting to place public housing in crowded inner city minority neighborhoods instead of neighborhoods with an Anglo majority due to ""public and political pressure.""[35] Other housing phenomena that Yinger argues encourage segregation are those of sorting and bidding, in which bidders perceived to be higher-class win out on cheaper per-square-foot, larger homes farther away from inner cities.[21] Quasi-experimental audit studies, in which equally qualified individuals of different races both participate in housing searches, have found strong evidence of racial housing discrimination in the United States.[36] In a comprehensive study by the HUD in 2000, paired-tests (in which two applicants of different races but the same economic status and credit scores apply to rent or buy a house) were used to determine whether or not statistics about segregation truly pointed to housing discrimination. This study reported that although adverse treatment of racial and ethnic minorities has decreased over time, roughly 25 percent of white applicants were still favored above those who were African-American or Hispanic. About 17 percent of African American applicants and 20 percent of Hispanic applicants were subjected to adverse treatment, including receiving less information about a home or being shown fewer, lower-quality units.[37] A meta-analysis of housing discrimination by race/ethnicity published in 2020 found that discrimination is still prevalent but has declined in recent decades.[38] Sociologists have found that housing discrimination also extends to roommate selection.[39] Contrary to common misconception, the correlation between racial makeup and house price has risen over the past several decades.[40] Internet classified platforms have also faced scrutiny under the Fair Housing Act; in 2008, the Ninth Circuit Court of Appeals ruled in Fair Housing Council of San Fernando Valley v. Roommates.com, LLC, that Roommates.com had induced housing discrimination by allowing users to specify roommate preferences on their profiles in pre-determined categories relating to protected classes such as gender. This was ruled to not fall under the Section 230 safe harbor, which protects interactive computer services from liability for the actions of their users, because Roommates.com was specifically responsible for having provided specific means to engage in conduct illegal under the Fair Housing Act; however, the site was not deemed responsible for information provided in a field that allowed users to type in additional comments.[41] The Roommates.com decision was overturned in 2012, however, with the court ruling that due to the intimacy of this relationship, it would be a violation of their ""privacy, autonomy and security"" if tenants were unable to seek a roommate that was compatible with their own lifestyle.[42][43] Facebook has faced accusations that its targeted advertising platform facilitates housing discrimination, as it allowed advertisers to target or exclude specific audiences from campaigns, and that its advertising-delivery system is optimized to favor demographics that are the most likely to interact with an ad, even if they are not explicitly specified by the advertiser.[44] After an investigation by ProPublica, Facebook removed the ability for housing advertisers to target ads based on a user's ""affinity"" to a specific culture or ethnicity, a behavior that is calculated based on the user's interactions on the service. However, it was found that advertisers could still discriminate based on interests implicating protected classes (such as Spanish-language television networks), and redlining ZIP code ranges.[45][46] After signing a legally binding agreement with the State of Washington,[47] Facebook announced that it would remove at least 5,000 categories from its exclusion system to prevent ""misuse"" including those relating to races and religions.[48] On March 19, 2019, to settle a lawsuit with the National Fair Housing Alliance, Facebook agreed to create a separate portal for housing, employment, and credit (HEC) advertising with limited targeting options by September 2019, and to provide a public archive of all HEC advertising.[49][50] Less than two weeks later, HUD filed a lawsuit against Facebook, formally accusing the company of facilitating housing discrimination.[51][52] Housing discrimination focuses more on race, but recent studies have shown a growing trend toward discrimination in the housing market against those who identify themselves as gay, lesbian, or transgender. Since housing discrimination based on sexual orientation was not explicitly cited in the Fair Housing Act, as of 2007, it was banned in only 17 states. In all states, same-sex couples are frequently unable to apply to public housing as a family unit, thus decreasing their chances at being accepted into the program.[53] For instance, in a comprehensive study done by the Fair Housing Centers of Michigan in 2007, statistics showed that out of 120 paired-tests, almost 30 percent of same-sex couples were given higher rental rates and less encouragement to rent, both examples of non-exclusionary housing discrimination.[54] An HUD study released in 2011 surveyed of 6,450 transgender and gender non-conforming persons and found that ""19 percent reported having been refused a house or an apartment because of gender identity.""[55] On January 30, 2012, HUD Secretary Shaun Donovan announced new regulations that would require all housing providers that receive HUD funding to prevent housing discrimination based on sexual orientation or gender identity.[56] These regulations went into effect on March 5, 2012.[57] Ethnic and racial minorities are impacted the most by housing discrimination. Exclusionary discrimination against African Americans most often occurs in rental markets and sales markets. Families are vulnerable to exclusion, but African American women are especially overrepresented as victims, especially single African American mothers. This discriminatory exclusion is because of stereotypes concerning race and single women. The presence of children in a minority family at times is what warrants the discrimination.[58] African Americans are also the victims in most non-exclusionary cases, with African American women still overrepresented. Non-exclusionary forms of discrimination such as racial slurs and intimidation affect many minority victims. Some racial minorities suffer the purposeful neglect of service needs, such as a landlord fixing a white tenant's bathtub quickly but delaying to fix the bathtub of the minority tenant.[59] Data obtained by Ohio Civil Rights Commission studied housing discrimination cases between 1988 and 2003, and of the 2,176 cases filed, 1,741 were filed by African Americans.[59] A study by HUD released in 2005 found that more and more Hispanics are facing discrimination in their housing searches.[60] A 2011 article by HUD asserts that one out of five times, Asian Americans and Pacific Islanders receive less favorable treatment than others when they seek housing.[61] Some cases brought to the Department of Justice show that municipalities and other local government entities violated the Fair Housing Act of 1968 when they denied African Americans housing, permits, and zoning changes, or steered them toward neighborhoods with a predominantly minority population.[20] A study conducted by the U.S. Department of Housing and Urban Development (HUD) found that, ""The greatest share of discrimination for Hispanic and African American home seekers can still be attributed to being told units are unavailable when they are available to non-Hispanic whites and being shown and told about less units than a comparable non-minority.""[60] Consumer advocate groups conducted studies and found that many minority borrowers who were eligible for affordable, traditional loans were often steered toward incredibly high-priced subprime loans that they would never be able to repay.[62] The Fair Housing Act forbids discrimination based on disability status. That means a landlord cannot reject someone for housing for being disabled, and a resident with a disability is entitled to reasonable accommodations. It defines a person with a disability as ""Any person who has a physical or mental impairment that substantially limits one or more major life activities; has a record of such impairment; or is regarded as having such an impairment.""[63] The Americans with Disabilities Act of 1990 also forbids discrimination against people with disabilities by public entities or programs, such as public housing. Research has shown that there is discrimination against those who use wheelchairs and those who are deaf in the rental housing market.[64] This study found that many landlords and housing providers are not fully aware of their obligations to provide accessible housing or of the accessibility of their properties. A 2010 HUD study found evidence of housing discrimination against those with mental disabilities.[65] The researchers of these studies emphasized the need for increase paired testing for discrimination in the housing market. Research on discrimination in the rental housing market has uncovered gender biases. A meta-analysis of 25 separate correspondence studies done by the Journal of Housing Economics found that applicants with minority and male-sounding names are discriminated against in the rental housing market even under the same circumstances and with all else equal.[66] John Yinger, a sociologist who has studied housing discrimination, argues that it is something perhaps most concretely evidenced by its effects: concentrated poverty. People who suffer from housing discrimination often live in lower-quality housing. Housing inequalities often reflect the unequal distribution of income. Poor areas suffer from educational disparities, and a poor education translates into earnings disparities. Those who earn less can only afford lower-quality housing.[21] Segregation, health risks, and wealth disparities all relate to poverty.[1] According to a study in the Journal of Economics, homeownership rates for Asian American and Hispanic minorities are negatively impacted by an immigrant status. Thus, their homeownership rates are lower than other demographic groups even when factors like income are accounted for.[67] Perhaps the most unmistakable consequence of housing discrimination is residential segregation. Housing discrimination helps reinforce residential segregation through mortgage discrimination, redlining, and predatory lending practices. Racial avoidance and threats of violence also result in racial segregation.[1] Housing discrimination can also impact minority preferences over time, as individuals or families experiencing harassment and intimidation at their home on a daily basis may transition to more accepting neighborhoods.[59] After Brown v. Board of Education, many white families took advantage of the growing highway system and suburbanization to leave cities to avoid integrated schools, which became known as White flight.[68] White flight was facilitated by FHA policies such as race-restricted deeds and zoning, as well as by blockbusting. This led to what is known as urban decay, and the achievement gap between inner-city and suburban schools widened.[69] According to the U.S. Census of Population in 1990, 25.3 percent of all Anglo-Americans in the U.S. lived in central city areas. The percentage of African Americans living in inner cities was 56.9 percent, and the percentage of inner city Hispanics was 51.5 percent. Asian Americans living in central cities totaled 46.3 percent. According to a more recent U.S. Census Bureau study in 2002, the average white person living in a metropolitan area lives in a neighborhood that is 80 percent white and seven percent black, while the average African American lives in a neighborhood that is 33 percent white and more than 51 percent black. As of 2000, 75 percent of all African Americans lived in highly segregated communities, making them the most segregated group in the nation.[70] These statistics do not necessarily point to evidence of housing discrimination, but rather to segregation based on historical reasons which have made ethnic and racial minorities more economically deprived, and thus prone to living in more poverty-stricken inner city areas. Housing discrimination has contributed to environmental racism, which refers to how communities of color suffer from disproportionate exposure to toxins and other health risks. Those suffering housing discrimination and people living below the poverty threshold often rent small or low-quality housing. Lead paint left over from past years and animal pests, such as rats, can be found in older housing, resulting in serious health consequences. Lead can lead to lowered intelligence in children.[71] Asthma is also a problem that comes with lower-quality housing, since more air pollution, dust, mold, and mildew are more likely to occur.[21] Housing discrimination has led to climate differences between neighborhoods. A study found that formerly redlined neighborhoods are several degrees hotter than non-redlined neighborhoods.[72] This differences can be explained in differences of development and infrastructure. Poorer, non-white neighborhoods have fewer trees and often are closer to highways and factories. The larger amount of asphalt and cement contributes to the urban heat island effect. The differences in temperature contribute to health disparities and premature heat-related deaths.[73] Neighborhood effects are also seen due to housing discrimination and residential segregation. The housing inequality that comes with living in lower-quality housing means that neighborhood amenities are lacking.[74] Education policy is intrinsically connected to housing policy as integration of schools requires integration of neighborhoods.[69]Educational inequalities exist between wealthier and poorer areas. Poorer areas typically offer worse education, leading to educational and employment disadvantages and a higher school dropout rate. Schools are often segregated due to the effects of housing discrimination and residential segregation, in turn hindering students' educational performance. Schools with a high proportion of disadvantaged students, such as schools in segregated areas, have worse educational outcomes that are compounded by other disparities, such as differences in parental education, local crime, access to healthcare, and extracurricular opportunities.[75] These differences and their impacts are known as the achievement gap. A study conducted by the Century Foundation in Montgomery County, Md., showed that students from a low-income background enrolled in affluent schools did better than students in higher-poverty schools.[76] Criminal activity, including gang life and drug abuse, is also more prevalent in poorer areas. The rate of teenage pregnancy has been shown to increase in these areas as well.[77] Urban, low-income schools are often contributors to the school to prison pipeline. Students from low socioeconomic neighborhoods perform worse academically and on standardized testing, and high-stakes testing provides an incentive to push these students out to the juvenile justice system.[78] In the US, white households have a median wealth of $134,230, while Black households have a median wealth of $11,030, demonstrating significant wealth disparities.[79] Sociologists Thomas Shapiro and Jessica Kenty-Drane, as well as Richard Rothstein, state that wealth disparities are a result of housing discrimination, as housing discrimination acts as a barrier to homeownership. Other scholars have argued that African American homeowners and renters were exploited for profit as they often paid higher prices for their houses and apartments than those in surrounding white neighborhoods.[29] This ""race tax"" has contributed to wealth disparities as it hindered wealth accumulation. Homeowners may learn management and home repair skills, and the children of homeowners are less likely to drop out of high school or to have children as teenagers. Additionally, credit constraints limit homeownership for people with low income. Housing discrimination that keeps families from affordable loans and nicer areas with increasing property values keep victims from accumulating wealth.[21] Residential segregation also leads to generational wealth disparities. Children often inherit wealth from their parents, and if parents were forced into poor-quality housing because of housing discrimination, then there is less wealth to hand down.[1] Sociologist Douglas Massey argues that housing discrimination is a moving target.[28] As federal legislation concerning anti-housing discrimination policies become more effective, new forms of housing discrimination have emerged to perpetuate residential segregation, and in turn, poverty.[59] The Urban Institute and other policy experts have called for more paired testing research in order to expose current housing discrimination.[80] Paired testing research would involve sending two separate applicants who are similar except for race to a realtor or landlord, and their treatment by the landlord or agent is compared. Paired tests have found that realtors show white families more apartments and households than Black or Latino families. There have been a number of solutions proposed to finally end the threat of housing discrimination and eliminate any legal loopholes in which it may operate. So far fair housing enforcement of federal legislation concerning housing discrimination has faced challenges. The main burden of enforcement falls on federal financial regulatory institutions, like the Federal Reserve Board, and the HUD.[21] The enforcement provisions of the Fair Housing Act of 1968 were limited, and even though the act was amended in 1988, there are still problems with enforcement since housing discrimination often happens one-on-one and is not very visible, even in audits.[21] The Fair Housing Amendment Act of 1988 did make a system of administrative law judges to hear housing discrimination cases to help against the illegal actions. Other examples of federal legislation may include increased federal legislation enforcement, scattered-site housing,[21] or state and local enforcement on a more concentrated level.[81] Better methods of enforcement in addition to new policies are proposed to be a help. In 2010 the Justice Department under President Barack Obama made a new fair-lending unit.[62] Inclusionary remedies to truly enforce integration are also proposed. Inclusionary housing refers to making sure that areas are integrated, and inclusionary housing increases chances for racial minorities to gain and sustain employment.[77] Recently Montgomery County, Md., passed an ordinance to require new housing developments to consist of a percentage of moderately priced dwelling units, guaranteeing more affordable better housing for 10 years.[82] Other proposed solutions include subsidies, such as direct subsidies, project-based subsidies, household-based subsidies, and tax reductions.[21] As of 2001, only 15.7 percent of poor households received federal housing subsidies, meaning a majority of people in poor households did not receive that help. Household-based subsidies have been a significant source of new housing assistance as of late.[21] HUD has handed out housing certificates to allow participants of Section 8 to move into higher-quality housing units.[21] One experiment, known as Moving to Opportunity, gave vouchers to the treatment group that could only be used in low-poverty neighborhoods. Although the findings were mixed for education and income, improvements in physical wellbeing and mental health were statistically significant.[83] However, critics have argued that removing people from community networks can lead to isolation and social disintegration.[84] Angela Blackwell argues that it is important to prioritize policy and city planning. However, looking beyond urban regimes and accepting the nexus of these regimes is the first step for change that planners can take. This can be done through the notion of Equitable Development, an approach that aims to create communities of opportunity. Inequalities oppressing low-income communities composed of diverse ethnicities are not only unethical but prove to be economically and environmentally unsustainable. Partnership between government, private sectors, and community-based organizations to manipulate public policy for the promotion of social equity, as well as, economic growth and environmental sustainability are crucial for justice.[85]"
Institutionalized discrimination in the United States,/wiki/Institutionalized_discrimination_in_the_United_States,"Institutionalized discrimination refers to the unjust and discriminatory mistreatment of an individual or group of individuals by society and its institutions as a whole, through unequal selection or bias, intentional or unintentional; as opposed to individuals making a conscious choice to discriminate. It stems from systemic stereotypical beliefs (such as sexist or racist beliefs) that are held by the vast majority living in a society where stereotypes and discrimination are the norm (see institutionalized racism).[1] Such discrimination is typically codified into the operating procedures, policies, laws, or objectives of such institutions. Members of minority groups such as populations of African descent in the U.S. are at a much higher risk of encountering these types of sociostructural disadvantage. Among the severe and long-lasting detrimental effects of institutionalized discrimination on affected populations are increased suicide rates, suppressed attainment of wealth and decreased access to health care.[2][3] Examples of institutionalized discrimination include laws and decisions that reflect racism, such as the Plessy vs. Ferguson U.S. Supreme Court case. The verdict of this case ruled in favor of separate but equal public facilities between African Americans and non-African Americans. This ruling was struck down by the Brown vs. Board of Education Supreme Court decision. Institutionalized discrimination often exists within the government, though it can also occur in any other type of social institution including religion, education and marriage. Achievement gaps in education may represent an example of institutionalized discrimination. Two recent studies aimed to explain the complications of assessing educational progress within the United States. One study focused on high school graduation rates, whereas the other study compared dropout rates in suburban and urban schools. By taking a closer look at statistics of test scores and academic achievement, researchers noticed that wealthy whites do better than blacks, poor whites, and Latinos. According to Star Parker, reporter of the Durham Herald Sun, graduation rates among whites and Asians are about 25 percent higher than those of blacks, Hispanics, and American Indians. This signifies that academic achievement is linked to socioeconomic status.[4] Institutionalized discrimination also exists in institutions aside from the government such as religion, education, and marriage among many other. Routines that encourage the selection of one individual over another, for instance in an employment situation, is a form of institutionalized discrimination. The phenomenon occurs unintentionally at times. Thomas Shapiro’s The Hidden Cost of Being African American addresses many of the problems faced by African Americans in the United States and how their current social and economic situations compare to one another. These issues include the racial wealth gap between blacks and whites, assets, and education. Housing in the United States is valued differently based on the racial makeup of the neighborhood. There can be two identical houses in terms of amenities and size but the value of each house depends on the racial makeup of the people within the community. Tactics like blockbusting, a method where real estate agents survey white homeowners in an area can cause a shift in the composition of a neighborhood. Although the concept of blockbusting has been illegal since 1968 unintentional segregation continues to define neighborhoods today. The Cedar Grove Institute for Sustainable Communities has developed a plan to fight institutionalized discrimination in the Mebane, North Carolina area, and included minorities in local planning that have historically been excluded rendering them insufficient police and fire protection. Their land values are lower than others leading to zoning for schools and other related issues. As community boundaries are not visible, a mapping process from the Geographical Information System (GIS) divides it. It combines several types of information into a single picture. The base map is physical features (roads, city limits, county boundaries) onto which other variables (e.g. race, income, water service, etc.). If needed, the processing system can also show other types of economic variables to draw conclusions about the area. Once the individuals begin to understand this information and realize what is happening to them, they have the power to hold the government accountable and can fight back against the institutionalized discrimination.[5]"
Jazz club,/wiki/Jazz_club,"A jazz club is a venue where the primary entertainment is the performance of live jazz music, although some jazz clubs primarily focus on the study and/or promotion of jazz-music.[1] Jazz clubs are usually a type of nightclub or bar, which is licensed to sell alcoholic beverages. Jazz clubs were in large rooms in the eras of Orchestral jazz and big band jazz, when bands were large and often augmented by a string section. Large rooms were also more common in the Swing era, because at that time, jazz was popular as a dance music, so the dancers needed space to move. With the transition to 1940s-era styles like Bebop and later styles such as soul jazz, small combos of musicians such as quartets and trios were mostly used, and the music became more of a music to listen to, rather than a form of dance music. As a result, smaller clubs with small stages became practical. In the 2000s, jazz clubs may be found in the basements of larger residential buildings, in storefront locations or in the upper floors of retail businesses. They can be rather small compared to other music venues, such as rock music clubs, reflecting the intimate atmosphere of jazz shows and long-term decline in popular interest in jazz.[2] Despite being called ""clubs"", these venues are usually not exclusive. Some clubs, however, have a cover charge if a live band is playing. Some jazz clubs host ""jam sessions"" after hours or on early evenings of the week. At jam sessions, both professional musicians and amateurs will typically share the stage. In the 19th century, before the birth of jazz, popular forms of live music for most well-to-do white Americans included classical concert music, such as concerti and symphonies, music played at performances, such as the opera and the ballet, and ballroom music. For these people, going out was a formal occasion, and the music was treated as something to listen to (if at the symphony or the opera house), or dance reservedly to (if at a ball). During the same century, African-American communities were marginalized from an economic perspective. But despite this lack of material wealth, they had thriving community and a culture based around informal music performances, such as brass band performances at funerals, music sung in church and music played for families eating picnics in parks. African-American culture developed communal activities for informal sharing, such as Saturday night fish fries, Sunday camping along the shores of Lake Pontchartrain at Milneburg and Bucktown, making red beans and rice banquettes on Mondays, and holding nightly dances at neighborhood halls all over town.[3] This long and deep commitment to music and dance, along with the mixing of musical traditions like spiritual music from the church, the blues carried into town by rural guitar slingers, the minstrel shows inspired by plantation life, the beat and cadence of military marching bands and the syncopation of the ragtime piano, led to the creation of a new way to listen to live music. In the jazz history books, places such as New Orleans, Chicago, Harlem, Kansas City, U Street in Washington D.C., and the Central Avenue zone of Los Angeles are often cited as the key nurturing places of jazz.[4] The African musical traditions primarily made use of a single-line melody and call-and-response pattern, and the rhythms have a counter-metric structure and reflect African speech patterns. Lavish festivals featuring African-based dances to drums were organized on Sundays at Place Congo, or Congo Square, in New Orleans until 1843.[5] Another influence on black music came from the style of hymns of the church, which black slaves had learned and incorporated into their own music as spirituals.[6] During the early 19th century an increasing number of black musicians learned to play European instruments. The ""Black Codes"" outlawed drumming by slaves, which meant that African drumming traditions were not preserved in North America, unlike in Cuba, Haiti, and elsewhere in the Caribbean. African-based rhythmic patterns were retained in the United States in large part through ""body rhythms"" such as stomping, clapping, and patting juba.[7] In the post-Civil War period (after 1865), African Americans were able to obtain surplus military bass drums, snare drums and fifes, and an original African-American drum and fife music emerged, featuring tresillo and related syncopated rhythmic figures.[8] The abolition of slavery in 1865 led to new opportunities for the education of freed African Americans. Although strict segregation limited employment opportunities for most blacks, many were able to find work in entertainment. Black musicians were able to provide entertainment in dances, minstrel shows, and in vaudeville, during which time many marching bands were formed. Black pianists played in bars, clubs and brothels, as ragtime developed.[9][10]Blues is the name given to both a musical form and a music genre,[11] which originated in African-American communities of primarily the ""Deep South"" of the United States at the end of the 19th century from their spirituals, work songs, field hollers, shouts and chants and rhymed simple narrative ballads.[12] The music of New Orleans had a profound effect on the creation of early jazz. Many early jazz performers played in venues throughout the city, such as the brothels and bars of the red-light district around Basin Street, known as ""Storyville"".[13] In addition to dance bands, there were numerous marching bands who played at lavish funerals (later called jazz funerals), which were arranged by the African-American and European American communities. The instruments used in marching bands and dance bands became the basic instruments of jazz.[14] Despite its growing popularity, not all who lived in the Jazz Age were keen on the sound of jazz music, and especially of jazz clubs. By the advent of the 20th century, campaigns to censor the ""devil's music"" started to appear, prohibiting when and where jazz clubs could be built. For example, a Cincinnati home for expectant mothers won an injunction to prevent construction of a neighboring theater where jazz will be played, convincing a court that the music was dangerous to fetuses. By the end of the 1920s, at least 60 communities across the nation enacted laws prohibiting jazz in public dance halls.[15] Prohibition in 1920 fostered the emergence of the underground, gangster-run jazz clubs. These venues served alcohol, hired black musicians, and allowed whites, blacks and audiences of all social classes to mingle socially for the first time.[15] Although the underground jazz clubs encouraged the intermingling of races in the Jazz Age, there were other jazz clubs, such as the Cotton Club in New York, that were white-only. By the 1940s, jazz music as a form of popular music was on the decline, and so was the popularity of jazz clubs. In the early 1940s, bebop-style performers began to shift jazz from danceable popular music towards a more challenging ""musician's music."" Since bebop was meant to be listened to, not danced to, it could use faster tempos. Drumming shifted to a more elusive and explosive style and highly syncopated music.[16] While bebop did not draw the huge crowds that had once flocked to Swing-era dance clubs, the bebop style was based around small combos such as the jazz quartet. With these smaller combos on stage, smaller clubs could afford to pay the ensembles even with much smaller clubs than were common in the 1930s heyday of the Cotton Club. Soul Jazz was a development of hard bop which incorporated strong influences from blues, gospel and rhythm and blues to create music for small groups, often the organ trio of Hammond organ, drummer and tenor saxophonist. Unlike hard bop, soul Jazz generally emphasized repetitive grooves and melodic hooks, and improvisations were often less complex than in other Jazz styles. It often had a steadier ""funk"" style groove, which was different from the swing rhythms typical of much hard bop. Soul Jazz proved to be a boon to Jazz clubs, because since organ trios were based around the powerful Hammond organ, a three-piece organ trio could fill a nightclub with the same full sound that in previous years would have required a five- or six-piece band. The 1980s saw something of a reaction against the fusion and free jazz that had dominated the 1970s. Trumpeter Wynton Marsalis emerged early in the decade, and strove to create music within what he believed was the tradition, rejecting both fusion and free jazz and creating extensions of the small and large forms initially pioneered by such artists as Louis Armstrong and Duke Ellington, as well as the hard bop of the 1950s. Whether Marsalis' critical and commercial success was a cause or a symptom of the reaction against Fusion and Free Jazz and the resurgence of interest in the kind of jazz pioneered in the 1960s (particularly Modal Jazz and Post-Bop) is debatable; nonetheless there were many other manifestations of a resurgence of traditionalism, even if fusion and free jazz were by no means abandoned and continued to develop and evolve. Well into the 1980s, the clubs where it is performed in these countries provide meeting places for political dissidents, however, attendance of these clubs is minuscule compared to the popularity of jazz clubs during the Jazz Age. Known as the ""birthplace of jazz,"" New Orleans is home to some of the oldest and most famous jazz clubs in the United States,[17] including:"
Jim Crow laws,/wiki/Jim_Crow_laws," The Jim Crow laws were state and local laws introduced in the Southern United States in the late 19th and early 20th centuries that enforced racial segregation, ""Jim Crow"" being a pejorative term for an African American.[1] Such laws remained in force until 1965.[2] Formal and informal segregation policies were present in other areas of the United States as well, even if several states outside the South had banned discrimination in public accommodations and voting.[3][4] Southern laws were enacted by white ""Redeemers""-dominated state legislatures to disenfranchise and remove political and economic gains made by African Americans during the Reconstruction era.[5] Such continuing racial segregation was also supported by the successful Lily-White Movement of Southern Republicans.[6] In practice, Jim Crow laws mandated racial segregation in all public facilities in the states of the former Confederate States of America and in some others, beginning in the 1870s. Jim Crow laws were upheld in 1896 in the case of Plessy vs. Ferguson, in which the Supreme Court laid out its ""separate but equal"" legal doctrine concerning facilities for African Americans. Moreover, public education had essentially been segregated since its establishment in most of the South after the Civil War in 1861–1865. Companion laws excluded almost all African Americans from the vote in the South and deprived them of any representation government. Although in theory, the ""equal"" segregation doctrine governed public facilities and transportation too, facilities for African Americans were consistently inferior and underfunded compared to facilities for white Americans; sometimes, there were no facilities for the black community at all.[7][8] Far from equality, as a body of law, Jim Crow institutionalized economic, educational, political and social disadvantages and second class citizenship for most African Americans living in the United States.[7][8][9] After the National Association for the Advancement of Colored People (NAACP) was founded in 1909, it became involved in a sustained public protest and campaigns against the Jim Crow laws, and the so-called ""separate but equal"" doctrine. In 1954, segregation of public schools (state-sponsored) was declared unconstitutional by the U.S. Supreme Court in the landmark case Brown v. Topeka Board of Education.[10][11][12] In some states, it took many years to implement this decision, while the Warren Court continued to rule against Jim Crow legislation in other cases such as Heart of Atlanta Motel, Inc. v. United States (1964).[13] In general, the remaining Jim Crow laws were overturned by the Civil Rights Act of 1964 and the Voting Rights Act of 1965. The earliest known use of the phrase ""Jim Crow law"" can be dated to 1884 in a newspaper article summarizing congressional debate.[14] The term appears in 1892 in the title of a New York Times article about Louisiana requiring segregated railroad cars.[15][16] The origin of the phrase ""Jim Crow"" has often been attributed to ""Jump Jim Crow"", a song-and-dance caricature of black people performed by white actor Thomas D. Rice in blackface, first performed in 1828. As a result of Rice's fame, Jim Crow had become by 1838 a pejorative expression meaning ""Negro"". When southern legislatures passed laws of racial segregation directed against African Americans at the end of the 19th century, these statutes became known as Jim Crow laws.[15] In January 1865, an amendment to the Constitution abolishing slavery in the United States was proposed by Congress and ratified as the Thirteenth Amendment on December 18, 1865.[17] During the Reconstruction era of 1865–1877, federal laws provided civil rights protections in the U.S. South for freedmen, African Americans who were former slaves, and the minority of black people who had been free before the war. In the 1870s, Democrats gradually regained power in the Southern legislatures[18] as violent insurgent paramilitary groups, such as the Ku Klux Klan, White League, and Red Shirts disrupted Republican organizing, ran Republican officeholders out of town, and lynched Black voters as an intimidation tactic to suppress the Black vote.[19] Extensive voter fraud was also used. In one instance, an outright coup or insurrection in coastal North Carolina led to the violent removal of democratically elected Republican party executive and representative officials, who were either hunted down or hounded out. Gubernatorial elections were close and had been disputed in Louisiana for years, with increasing violence against black Americans during campaigns from 1868 onward.[20] The Compromise of 1877 to gain Southern support in the presidential election resulted in the government withdrawing the last of the federal troops from the South. White Democrats had regained political power in every Southern state.[21] These Southern, white, ""Redeemer"" governments legislated Jim Crow laws, officially segregating the country's population. Jim Crow laws were a manifestation of authoritarian rule specifically directed at one racial group.[22] Black people were still elected to local offices throughout the 1880s in local areas with large black populations, but their voting was suppressed for state and national elections. States passed laws to make voter registration and electoral rules more restrictive, with the result that political participation by most black people and many poor white people began to decrease.[23][24] Between 1890 and 1910, ten of the eleven former Confederate states, beginning with Mississippi, passed new constitutions or amendments that effectively disenfranchised most black people and tens of thousands of poor white people through a combination of poll taxes, literacy and comprehension tests, and residency and record-keeping requirements.[23][24]Grandfather clauses temporarily permitted some illiterate white people to vote but gave no relief to most black people. Voter turnout dropped dramatically through the South as a result of these measures. In Louisiana, by 1900, black voters were reduced to 5,320 on the rolls, although they comprised the majority of the state's population. By 1910, only 730 black people were registered, less than 0.5% of eligible black men. ""In 27 of the state's 60 parishes, not a single black voter was registered any longer; in 9 more parishes, only one black voter was.""[25] The cumulative effect in North Carolina meant that black voters were eliminated from voter rolls during the period from 1896 to 1904. The growth of their thriving middle class was slowed. In North Carolina and other Southern states, black people suffered from being made invisible in the political system: ""[W]ithin a decade of disfranchisement, the white supremacy campaign had erased the image of the black middle class from the minds of white North Carolinians.""[25] In Alabama, tens of thousands of poor whites were also disenfranchised, although initially legislators had promised them they would not be affected adversely by the new restrictions.[26] Those who could not vote were not eligible to serve on juries and could not run for local offices. They effectively disappeared from political life, as they could not influence the state legislatures, and their interests were overlooked. While public schools had been established by Reconstruction legislatures for the first time in most Southern states, those for black children were consistently underfunded compared to schools for white children, even when considered within the strained finances of the postwar South where the decreasing price of cotton kept the agricultural economy at a low.[27] Like schools, public libraries for black people were underfunded, if they existed at all, and they were often stocked with secondhand books and other resources.[8][28] These facilities were not introduced for African Americans in the South until the first decade of the 20th century.[29] Throughout the Jim Crow era, libraries were only available sporadically.[30] Prior to the 20th century, most libraries established for African Americans were school-library combinations.[30] Many public libraries for both European-American and African-American patrons in this period were founded as the result of middle-class activism aided by matching grants from the Carnegie Foundation.[30] In some cases, progressive measures intended to reduce election fraud, such as the Eight Box Law in South Carolina, acted against black and white voters who were illiterate, as they could not follow the directions.[31] While the separation of African Americans from the white general population was becoming legalized and formalized during the Progressive Era (1890s–1920s), it was also becoming customary. Even in cases in which Jim Crow laws did not expressly forbid black people from participating in sports or recreation, a segregated culture had become common.[15] In the Jim Crow context, the presidential election of 1912 was steeply slanted against the interests of African Americans.[32] Most black Americans still lived in the South, where they had been effectively disfranchised, so they could not vote at all. While poll taxes and literacy requirements banned many poor or illiterate people from voting, these stipulations frequently had loopholes that exempted European Americans from meeting the requirements. In Oklahoma, for instance, anyone qualified to vote before 1866, or related to someone qualified to vote before 1866 (a kind of ""grandfather clause""), was exempted from the literacy requirement; but the only men who had the franchise before that year were white or European-American. European Americans were effectively exempted from the literacy testing, whereas black Americans were effectively singled out by the law.[33] Woodrow Wilson was a Democrat elected from New Jersey, but he was born and raised in the South, and was the first Southern-born president of the post-Civil War period. He appointed Southerners to his Cabinet. Some quickly began to press for segregated workplaces, although the city of Washington, D.C., and federal offices had been integrated since after the Civil War. In 1913, Secretary of the Treasury William Gibbs McAdoo – an appointee of the President – was heard to express his opinion of black and white women working together in one government office: ""I feel sure that this must go against the grain of the white women. Is there any reason why the white women should not have only white women working across from them on the machines?""[34] The Wilson administration introduced segregation in federal offices, despite much protest from African-American leaders and white progressive groups in the north and midwest.[35] He appointed segregationist Southern politicians because of his own firm belief that racial segregation was in the best interest of black and European Americans alike.[36] At the Great Reunion of 1913 at Gettysburg, Wilson addressed the crowd on July 4, the semi-centennial of Abraham Lincoln's declaration that ""all men are created equal"": How complete the union has become and how dear to all of us, how unquestioned, how benign and majestic, as state after state has been added to this, our great family of free men![37] In sharp contrast to Wilson, a Washington Bee editorial wondered if the ""reunion"" of 1913 was a reunion of those who fought for ""the extinction of slavery"" or a reunion of those who fought to ""perpetuate slavery and who are now employing every artifice and argument known to deceit"" to present emancipation as a failed venture.[37] Historian David W. Blight observed that the ""Peace Jubilee"" at which Wilson presided at Gettysburg in 1913 ""was a Jim Crow reunion, and white supremacy might be said to have been the silent, invisible master of ceremonies"".[37] In Texas, several towns adopted residential segregation laws between 1910 and the 1920s. Legal strictures called for segregated water fountains and restrooms.[37] The exclusion of African Americans also found support in the Republican lily-white movement.[38] The Civil Rights Act of 1875, introduced by Charles Sumner and Benjamin F. Butler, stipulated a guarantee that everyone, regardless of race, color, or previous condition of servitude, was entitled to the same treatment in public accommodations, such as inns, public transportation, theaters, and other places of recreation. This Act had little effect in practice.[39] An 1883 Supreme Court decision ruled that the act was unconstitutional in some respects, saying Congress was not afforded control over private persons or corporations. With white southern Democrats forming a solid voting bloc in Congress, due to having outsize power from keeping seats apportioned for the total population in the South (although hundreds of thousands had been disenfranchised), Congress did not pass another civil rights law until 1957.[40] In 1887, Rev. W. H. Heard lodged a complaint with the Interstate Commerce Commission against the Georgia Railroad company for discrimination, citing its provision of different cars for white and black/colored passengers. The company successfully appealed for relief on the grounds it offered ""separate but equal"" accommodation.[41] In 1890, Louisiana passed a law requiring separate accommodations for colored and white passengers on railroads. Louisiana law distinguished between ""white"", ""black"" and ""colored"" (that is, people of mixed European and African ancestry). The law had already specified that black people could not ride with white people, but colored people could ride with white people before 1890. A group of concerned black, colored and white citizens in New Orleans formed an association dedicated to rescinding the law. The group persuaded Homer Plessy to test it; he was a man of color who was of fair complexion and one-eighth ""Negro"" in ancestry.[42] In 1892, Plessy bought a first-class ticket from New Orleans on the East Louisiana Railway. Once he had boarded the train, he informed the train conductor of his racial lineage and took a seat in the whites-only car. He was directed to leave that car and sit instead in the ""coloreds only"" car. Plessy refused and was immediately arrested. The Citizens Committee of New Orleans fought the case all the way to the United States Supreme Court. They lost in Plessy v. Ferguson (1896), in which the Court ruled that ""separate but equal"" facilities were constitutional. The finding contributed to 58 more years of legalized discrimination against black and colored people in the United States.[42] In 1908, Congress defeated an attempt to introduce segregated streetcars into the capital.[43] White Southerners encountered problems in learning free labor management after the end of slavery, and they resented African Americans, who represented the Confederacy's Civil War defeat: ""With white supremacy being challenged throughout the South, many whites sought to protect their former status by threatening African Americans who exercised their new rights.""[45] White Southerners used their power to segregate public spaces and facilities in law and reestablish social dominance over black people in the South. One rationale for the systematic exclusion of African Americans from southern public society was that it was for their own protection. An early 20th-century scholar suggested that allowing black people to attend white schools would mean ""constantly subjecting them to adverse feeling and opinion"", which might lead to ""a morbid race consciousness"".[46] This perspective took anti-black sentiment for granted, because bigotry was widespread in the South after slavery became a racial caste system. Justifications for white supremacy were provided by scientific racism and negative stereotypes of African Americans. Social segregation, from housing to laws against interracial chess games, was justified as a way to prevent black men from having sex with white women and in particular the rapacious Black Buck stereotype.[47] In 1944, Associate Justice Frank Murphy introduced the word ""racism"" into the lexicon of U.S. Supreme Court opinions in Korematsu v. United States, 323 U.S. 214 (1944).[48] In his dissenting opinion, Murphy stated that by upholding the forced relocation of Japanese Americans during World War II, the Court was sinking into ""the ugly abyss of racism"". This was the first time that ""racism"" was used in Supreme Court opinion (Murphy used it twice in a concurring opinion in Steele v Louisville & Nashville Railway Co 323 192 (1944) issued that day).[49] Murphy used the word in five separate opinions, but after he left the court, ""racism"" was not used again in an opinion for two decades. It next appeared in the landmark decision of Loving v. Virginia, 388 U.S. 1 (1967). Numerous boycotts and demonstrations against segregation had occurred throughout the 1930s and 1940s. The National Association for the Advancement of Colored People (NAACP) had been engaged in a series of litigation cases since the early 20th century in efforts to combat laws that disenfranchised black voters across the South. Some of the early demonstrations achieved positive results, strengthening political activism, especially in the post-World War II years. Black veterans were impatient with social oppression after having fought for the United States and freedom across the world. In 1947 K. Leroy Irvis of Pittsburgh's Urban League, for instance, led a demonstration against employment discrimination by the city's department stores. It was the beginning of his own influential political career.[50] After World War II, people of color increasingly challenged segregation, as they believed they had more than earned the right to be treated as full citizens because of their military service and sacrifices. The civil rights movement was energized by a number of flashpoints, including the 1946 police beating and blinding of World War II veteran Isaac Woodard while he was in U.S. Army uniform. In 1948 President Harry S. Truman issued Executive Order 9981, ending racial discrimination in the armed services.[51] That same year, Silas Herbert Hunt enrolled in the University of Arkansas, effectively starting the desegregation of education in the South.[52] As the civil rights movement gained momentum and used federal courts to attack Jim Crow statutes, the white-dominated governments of many of the southern states countered by passing alternative forms of resistance.[53] Historian William Chafe has explored the defensive techniques developed inside the African-American community to avoid the worst features of Jim Crow as expressed in the legal system, unbalanced economic power, and intimidation and psychological pressure. Chafe says ""protective socialization by black people themselves"" was created inside the community in order to accommodate white-imposed sanctions while subtly encouraging challenges to those sanctions. Known as ""walking the tightrope"", such efforts at bringing about change were only slightly effective before the 1920s. However, this did build the foundation for later generations to advance racial equality and de-segregation. Chafe argued that the places essential for change to begin were institutions, particularly black churches, which functioned as centers for community-building and discussion of politics. Additionally, some all-black communities, such as Mound Bayou, Mississippi and Ruthville, Virginia served as sources of pride and inspiration for black society as a whole. Over time, pushback and open defiance of the oppressive existing laws grew, until it reached a boiling point in the aggressive, large-scale activism of the 1950s civil rights movement.[54] The NAACP Legal Defense Committee (a group that became independent of the NAACP) – and its lawyer, Thurgood Marshall – brought the landmark case Brown v. Board of Education of Topeka, 347 U.S. 483 (1954) before the U.S. Supreme Court under Chief Justice Earl Warren.[10][11][12] In its pivotal 1954 decision, the Warren Court unanimously (9–0) overturned the 1896 Plessy decision.[11] The Supreme Court found that legally mandated (de jure) public school segregation was unconstitutional. The decision had far-reaching social ramifications.[55] Racial integration of all-white collegiate sports teams was high on the Southern agenda in the 1950s and 1960s. Involved were issues of equality, racism, and the alumni demand for the top players needed to win high-profile games. The Atlantic Coast Conference (ACC) of flagship state universities in the Southeast took the lead. First they started to schedule integrated teams from the North. Finally, ACC schools – typically under pressure from boosters and civil rights groups – integrated their teams.[56] With an alumni base that dominated local and state politics, society and business, the ACC schools were successful in their endeavor – as Pamela Grundy argues, they had learned how to win: In 1955, Rosa Parks refused to give up her seat on a city bus to a white man in Montgomery, Alabama. This was not the first time this happened – for example, Parks was inspired by 15-year-old Claudette Colvin doing the same thing nine months earlier[58] – but the Parks act of civil disobedience was chosen, symbolically, as an important catalyst in the growth of the post-1954 civil rights movement; activists built the Montgomery bus boycott around it, which lasted more than a year and resulted in desegregation of the privately run buses in the city. Civil rights protests and actions, together with legal challenges, resulted in a series of legislative and court decisions which contributed to undermining the Jim Crow system.[59] The decisive action ending segregation came when Congress in bipartisan fashion overcame Southern filibusters to pass the Civil Rights Act of 1964 and the Voting Rights Act of 1965. A complex interaction of factors came together unexpectedly in the period 1954–1965 to make the momentous changes possible. The Supreme Court had taken the first initiative in Brown v. Board of Education (1954), declaring segregation of public schools unconstitutional. Enforcement was rapid in the North and border states, but was deliberately stopped in the South by the movement called Massive Resistance, sponsored by rural segregationists who largely controlled the state legislatures. Southern liberals, who counseled moderation, were shouted down by both sides and had limited impact. Much more significant was the civil rights movement, especially the Southern Christian Leadership Conference (SCLC) headed by Martin Luther King Jr. It largely displaced the old, much more moderate NAACP in taking leadership roles. King organized massive demonstrations, that seized massive media attention in an era when network television news was an innovative and universally watched phenomenon.[60] SCLC, student activists and smaller local organizations staged demonstrations across the South. National attention focused on Birmingham, Alabama, where protesters deliberately provoked Bull Connor and his police forces by using young teenagers as demonstrators – and Connor arrested 900 on one day alone. The next day Connor unleashed billy clubs, police dogs, and high-pressure water hoses to disperse and punish the young demonstrators with a brutality that horrified the nation. It was very bad for business, and for the image of a modernizing progressive urban South. President John F. Kennedy, who had been calling for moderation, threatened to use federal troops to restore order in Birmingham. The result in Birmingham was compromise by which the new mayor opened the library, golf courses, and other city facilities to both races, against the backdrop of church bombings and assassinations.[61] In summer 1963, there were 800 demonstrations in 200 southern cities and towns, with over 100,000 participants, and 15,000 arrests. In Alabama in June 1963, Governor George Wallace escalated the crisis by defying court orders to admit the first two black students to the University of Alabama.[62] Kennedy responded by sending Congress a comprehensive civil rights bill, and ordered Attorney General Robert F. Kennedy to file federal lawsuits against segregated schools, and to deny funds for discriminatory programs. Martin Luther King launched a huge march on Washington in August 1963, bringing out 200,000 demonstrators in front of the Lincoln Memorial, at the time the largest political assembly in the nation's history. The Kennedy administration now gave full-fledged support to the civil rights movement, but powerful southern congressmen blocked any legislation.[63] After Kennedy was assassinated, President Lyndon B. Johnson called for immediate passage of Kennedy civil rights legislation as a memorial to the martyred president. Johnson formed a coalition with Northern Republicans that led to passage in the House, and with the help of Republican Senate leader Everett Dirksen with passage in the Senate early in 1964. For the first time in history, the southern filibuster was broken and the Senate finally passed its version on June 19 by vote of 73 to 27.[64] The Civil Rights Act of 1964 was the most powerful affirmation of equal rights ever made by Congress. It guaranteed access to public accommodations such as restaurants and places of amusement, authorized the Justice Department to bring suits to desegregate facilities in schools, gave new powers to the Civil Rights Commission; and allowed federal funds to be cut off in cases of discrimination. Furthermore, racial, religious and gender discrimination was outlawed for businesses with 25 or more employees, as well as apartment houses. The South resisted until the last moment, but as soon as the new law was signed by President Johnson on July 2, 1964, it was widely accepted across the nation. There was only a scattering of diehard opposition, typified by restaurant owner Lester Maddox in Georgia.[65][66][67][68] In January 1964, President Lyndon Johnson met with civil rights leaders. On January 8, during his first State of the Union address, Johnson asked Congress to ""let this session of Congress be known as the session which did more for civil rights than the last hundred sessions combined."" On June 21, civil rights workers Michael Schwerner, Andrew Goodman, and James Chaney disappeared in Neshoba County, Mississippi, where they were volunteering in the registration of African American voters as part of the Freedom Summer project. The disappearance of the three activists captured national attention and the ensuing outrage was used by Johnson and civil rights activists to build a coalition of northern and western Democrats and Republicans and push Congress to pass the Civil Rights Act of 1964.[69] On July 2, 1964, Johnson signed the historic Civil Rights Act of 1964.[69][70] It invoked the Commerce Clause[69] to outlaw discrimination in public accommodations (privately owned restaurants, hotels, and stores, and in private schools and workplaces). This use of the Commerce Clause was upheld by the Warren Court in the landmark case Heart of Atlanta Motel v. United States 379 US 241 (1964).[71] By 1965, efforts to break the grip of state disenfranchisement by education for voter registration in southern counties had been underway for some time, but had achieved only modest success overall. In some areas of the Deep South, white resistance made these efforts almost entirely ineffectual. The murder of the three voting-rights activists in Mississippi in 1964 and the state's refusal to prosecute the murderers, along with numerous other acts of violence and terrorism against black people, had gained national attention. Finally, the unprovoked attack on March 7, 1965, by county and state troopers on peaceful Alabama marchers crossing the Edmund Pettus Bridge en route from Selma to the state capital of Montgomery, persuaded the President and Congress to overcome Southern legislators' resistance to effective voting rights enforcement legislation. President Johnson issued a call for a strong voting rights law and hearings soon began on the bill that would become the Voting Rights Act.[72] The Voting Rights Act of 1965 ended legally sanctioned state barriers to voting for all federal, state and local elections. It also provided for federal oversight and monitoring of counties with historically low minority voter turnout. Years of enforcement have been needed to overcome resistance, and additional legal challenges have been made in the courts to ensure the ability of voters to elect candidates of their choice. For instance, many cities and counties introduced at-large election of council members, which resulted in many cases of diluting minority votes and preventing election of minority-supported candidates.[73] In 2013, the Roberts Court, in Shelby County v. Holder, removed the requirement established by the Voting Rights Act that Southern states needed Federal approval for changes in voting policies. Several states immediately made changes in their laws restricting voting access.[74] The Jim Crow laws and the high rate of lynchings in the South were major factors that led to the Great Migration during the first half of the 20th century. Because opportunities were very limited in the South, African Americans moved in great numbers to cities in Northeastern, Midwestern, and Western states to seek better lives. African American athletes faced much discrimination during the Jim Crow era with White opposition leading to their exclusion from most organized sporting competitions. The boxers Jack Johnson and Joe Louis (both of whom became world heavyweight boxing champions) and track and field athlete Jesse Owens (who won four gold medals at the 1936 Summer Olympics in Berlin) gained prominence during the era. In baseball, a color line instituted in the 1880s had informally barred black people from playing in the major leagues, leading to the development of the Negro leagues, which featured many fine players. A major breakthrough occurred in 1947, when Jackie Robinson was hired as the first African American to play in Major League Baseball; he permanently broke the color bar. Baseball teams continued to integrate in the following years, leading to the full participation of black baseball players in the Major Leagues in the 1960s.[citation needed] Although sometimes counted among Jim Crow laws of the South, statutes such as anti-miscegenation laws were also passed by other states. Anti-miscegenation laws were not repealed by the Civil Rights Act of 1964, but were declared unconstitutional by the U.S. Supreme Court (the Warren Court) in a unanimous ruling Loving v. Virginia (1967).[69][75][76] Chief Justice Earl Warren wrote in the court opinion that ""the freedom to marry, or not marry, a person of another race resides with the individual, and cannot be infringed by the State.""[76] The Sixth Amendment to the United States Constitution grants criminal defendants the right to a trial by a jury of their peers. While federal law required that convictions could only be granted by a unanimous jury for federal crimes, states were free to set their own jury requirements. All but two states, Oregon and Louisiana, opted for unanimous juries for conviction. Oregon and Louisiana, however, allowed juries of at least 10–2 to decide a criminal conviction. Louisiana's law was amended in 2018 to require a unanimous jury for criminal convictions, effective in 2019. Prior to that amendment, the law had been seen as a remnant of Jim Crow laws, because it allowed minority voices on a jury to be marginalized. In 2020, the Supreme Court found, in Ramos v. Louisiana, that unanimous jury votes are required for criminal convictions at state levels, thereby nullifying Oregon's remaining law, and overturning previous cases in Louisiana.[77] In 1971, the U.S. Supreme Court (the Burger Court), in Swann v. Charlotte-Mecklenburg Board of Education, upheld desegregation busing of students to achieve integration. Interpretation of the Constitution and its application to minority rights continues to be controversial as Court membership changes. Observers such as Ian F. Lopez believe that in the 2000s, the Supreme Court has become more protective of the status quo.[78] There is evidence that the government of Nazi Germany took inspiration from the Jim Crow laws when writing the Nuremberg Laws.[79] Ferris State University in Big Rapids, Michigan, houses the Jim Crow Museum of Racist Memorabilia, an extensive collection of everyday items that promoted racial segregation or presented racial stereotypes of African Americans, for the purpose of academic research and education about their cultural influence.[80]"
Jim Crow economy,/wiki/Jim_Crow_economy,"The term Jim Crow economy applies to a specific set of economic conditions in the United States during the period when the Jim Crow laws were in effect to force racial segregation; however, it should also be taken as an attempt to disentangle the economic ramifications from the politico-legal ramifications of ""separate but equal"" de jure segregation, to consider how the economic impacts might have persisted beyond the politico-legal ramifications. It includes the intentional effects of the laws themselves, effects that were not explicitly written into laws, and effects that continued after the laws had been repealed. Some of these impacts continue into the present. The primary differences of the Jim Crow economy, compared to a situation like apartheid, revolve around the alleged equality of access, especially in regard to land ownership and entry into the competitive labor market; however, those two categories often relate to ancillary effects in all other aspects of life. Frequently sources will mention the Jim Crow economy, and then proceed to discuss only what is specific to the topic being broached by a particular author; however, unlike the laws passed to restrict access to services and education, the laws that governed the economy were often written in race-neutral terms, with inequality stemming from enforcement decisions. The economic impacts of Jim Crow are also intertwined with changes in the overall economy of the United States, from the Civil War through the 20th century. There is a temporal rhythm to the economic impacts of Jim Crow; from the Reconstruction onward, social trends preceded policy changes that, in turn, preceded economic changes. During the decade following the Civil War, the freed slaves made gains in political participation, land ownership, and personal wealth; but, those gains were somewhat temporary, perhaps because the mood of the federal policy-makers changed from punishing secessionists, to repatriating them. In the decades following the closure of the Freedmen's Bureau, in the South, black political participation was curtailed, the potential for acquiring new land was diminished, and ultimately Plessy v. Ferguson would usher in the Jim Crow era. By the end of the first decade of the 20th century, not only was African American progress halted, it was regressing. Leading up to and following World War I, the agrarian economy of the South was in dire straits, beginning a slow shift to urbanization and limited industrialization; this period also saw the beginning of the Great Migration. The 1930s saw increasing urbanization and industrialization in the South; and, federal policies of the time, such as the National Industrial Recovery Act and the Fair Labor Standards Act, attempted to force economic parity between the South and the rest of the nation (Wright 1987:171). By the time of the passing of the Civil Rights Act of 1964, the scientific racism that had underlain much of the justification for the Jim Crow era legal racism had been discredited, the South had substantially closed its wealth gap with the rest of the nation, and America was both urbanized and industrialized. However, the African American struggle to earn economic parity, that had made progress during the first half century of the postbellum era, had largely been reversed during the second half. Legally, equality was assured, but that did little to actually promulgate equal conditions in daily life. Some of the gains in the South's economic relation to the rest of the U.S. can be explained by population shifts to other regions; so, it may have had as much to do with spreading poverty around, as spreading wealth around.[further explanation needed] In the period when agriculture had formed the basis of the economy, land and labor were intimately tied together in the ownership of farmland; in the shift to urban industrialization, neither land tenure, nor labor opportunities were necessarily improved for African Americans. Thus, to understand the Jim Crow economy it is required to look to the social and political climate prior to the implementation of the laws, and to the economic inertia that continued to impact people's lives after the repeal of the laws. In the decades following the Civil War, there were steady increases in African American ownership of farmland in the South, from 3 million acres (12,000 km2) in 1875, to 8 million acres (32,000 km2) in 1890, 12 million acres (49,000 km2) at the turn of the century, and peaking at 12,800,000 acres (52,000 km2) in 1910 (Reynolds 2002:4). Other estimates suggest that total black ownership of land in the South may have been as much as 15 million acres (61,000 km2) within a half century after emancipation (Mitchell 2000:507). There were also setbacks, due to property being taken illegally; in the first 30 years of the 20th century, 24,000 acres (97 km2) were taken, from 406 separate landowners (Darity Jr. & Frank 2003:327). By 1930, the number of black owned farms was 3% lower than what it had been at the turn of the century (Woodman 1997:22). After being freed, there were 2 main ways for African Americans to acquire land in the South: either buy it from a private landowner, or stake a claim to public land offered by the federal government under laws like the Southern Homestead Act of 1866, and by state governments, such as South Carolina's Land Commission. The Southern Homestead Act opened up the transfer of public land in the states of Alabama, Arkansas, Florida, Louisiana, and Mississippi, with the hope of providing land to freedmen by limiting the claims to 80 acres (320,000 m2) for the first 2 years (Pope 1970:203). The results were less purchasers than had been hoped for, largely because the recently freed slaves did not have the material means to settle unimproved property, and only 4,000 of the 11, 633 total claims were registered by freedmen (Pope 1970:205). Within the South, the Southern Homestead Act was seen as further punishment of attempting to secede; this was substantiated, by the repeal of 1876, when old enmities gave way to the promise of federal revenues (Gates 1940:311). After the Act was repealed, cash sales of public lands were reopened to large-scale purchasers; the repeal was reversed in 1888, but prior to that point more than 5,500,000 acres (22,000 km2) of land in the 5 public land states of the South were sold off to land speculators and timber harvesters (Gates 1936:667). South Carolina's Land Commission was a unique case of a Reconstruction-era state government organization that was formed explicitly for the purpose of selling bonds to fund the purchase of non-operating plantations and selling the land to small farm operators over a 10-year repayment schedule at 7% annual interest (Bethel 1997:20). From 1868 to 1879, the Land Commission sold farmland to 14,000 African American families (Bethel 1997:27). Another well documented sample of African American property ownership in a non-public land state comes from census and tax records in Georgia. In the year following the end of the Civil War, black property owners accumulated approximately 10,000 acres (40 km2) of land, with a value of about $22,500; however, on average, African Americans in Georgia held a total wealth of less than $1 per person (Higgs 1982:728). Between 1880 and 1910, Georgia's African Americans increased their average wealth, from $8 per person to $26.59, with some setbacks occurring around the turn of the century; however, relative to white Georgians, that amounted to an increase from 2% to 6% of total wealth held (Higgs 1982:729). By expanding the defined territory of the South to 16 states (including Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, and West Virginia), in 1910, there were 175,000 black farm owners compared to 1.15 million white farm owners (Higgs 1973:150). Discounting the states of Delaware, Kentucky, Maryland, Oklahoma, Texas, Virginia, and West Virginia, the average white-owned farm was nearly twice the size of the average black-owned farm (Higgs 1973:162). Land ownership was an important source of capital for both groups, but the ability to use the land with maximal productivity was not equally afforded to both groups. From the antebellum period up to the mid-1880s, all land owners were highly dependent on credit from merchant transporters of cotton; however, as the transportation infrastructure improved the white land owners were able to use their greater land holdings to attract credit directly from Northern financiers, and were thus able to usurp the position of the merchant transporters that furnished necessary staple goods to cotton growers (Woodman 1977:547). Drawing from a representative sample of 4,695 farms in 27 counties in Alabama, Georgia, Mississippi, North Carolina, and South Carolina, regarding the 1879-1880 cotton crop, white owners were able to leave more than 4 times the amount of land fallow, had nearly twice the value in farming implements, and were more than a third more likely to have access to fertilizer than were the black land owners (Ransom & Sutch 1973:141). Thus, African Americans were laboring harder for lower crop returns, and putting the long-term productivity of their land in greater jeopardy (Ransom & Sutch 1973:142). Between 1900 and 1930, in the South, 4.7% of black farm owners became tenant farmers; while 9.5% of white farmers were reduced from owners to tenants during that period, that amounted to only 46.6% of all white farmers being tenants compared to 79.3% of all black farmers (Woodman 1997:9). Moreover, there were fewer opportunities to acquire land, as white owners refused to sell land to black purchasers regardless of the price being offered, and there was little legal recourse when property was lost due to extra-legal practices (Higgs 1973:165). In any case, the availability of funds was greatly reduced by the failure of government-initiated lending institutions like the Freedman's Savings and Trust Company; and, lending organizations founded by benevolent societies often found themselves too overextended to withstand moderate levels of default on loans, such as the failure of the True Reformers Savings Banks in 1910 (Heen 2009:386). Lending organizations outside the South, backed by Northern capitalists, were mostly unwilling to make loans supporting African American land purchase, out of concern that the development of a class of black landowners would result in increased demands from Northern industrial workers (Ezeani 1977:106). With new land being unobtainable, and existing land only able to be subdivided so far before becoming unusable as farmland, the progeny of the land owning generation were pressured to move to Southern cities, or outside the South completely (Bethel 1997:98;101). When the U.S. Became involved with World War I, Northern cities became the focus of out-migration, and Northern industry became the employer of many former farmers (Tolnay et al.:991). The South was much slower to industrialize; and, where predominantly white land owners retained large tracts of farmland, and where the population of black laborers remained high, agriculture continued as the economic base (Roscigno & Tomaskovic-Devey 1996:576). African American movement into urban centers had begun just after the end of the Civil War; and, by 1870, the black population, in cities greater than 4,000, was increased by 80%, compared to only a 13% increase in the white population (Kellogg 1977:312). In contrast to the antebellum urban settlement pattern, cities that rose to prominence in the postbellum years tended to be more highly segregated (Groves & Muller 1975:174). To provide an example of monetary value, in Georgia, African American holdings of urban property increase from a value of $1.2 million in 1880 to $8.8 million in 1910, even though the properties were often in the least desirable locations; however, at the end of World War I, much of that property was sold off to white buyers, as African Americans started moving to Northern cities in large numbers (Higgs 1982:730-731). There were no explicit racial zoning ordinances in Southern cities prior to 1910; however, the individuals who developed and sold real estate in these areas often refused to sell to African American purchasers, outside of the prescribed areas (Kellogg 1982:41). In fact, the National Association of Realtors could take disciplinary action against a realtor for selling property to a person of a different race than those who presently lived in a particular neighborhood (Herrington et al.:163-164). The impact was greatest on those who migrated to cities early on; for those who migrated to the North, after 1965, there is evidence that they moved into neighborhoods that were the least segregated by race (Tolnay et al.:999). The initial pattern, starting in the 19th century, was to allow the original enclave neighborhoods to become overcrowded, while individual property owners subdivided acreages in low-lying areas at the urban periphery or close to industrial areas that employed unskilled laborers (Groves & Muller 1975:170). Starting with Baltimore in 1910, a number of cities throughout the South starting implementing racial zoning codes; although these were overturned by the Buchanan v. Warley Supreme Court decision, in 1917, many large and small cities simply changed from overtly racial zoning to instituting zoning based on existing neighborhood composition (Silver 1997). In Alabama, ""Birmingham continued illegally to enforce a racial zoning code until 1951"" (Silver 1997:38). Many growing cities and towns enacted their own Jim Crow ordinances; and, as they grew, they planned low-cost housing in areas with less access to public services, often using transportation corridors and natural features as buffer zones (Lee 1992:376-377). This practice was not restricted to the South; for example, in 1940s Detroit, a 6 ft (1.8 m). high concrete wall was erected to divide the Eight Mile-Wyoming area from neighboring white developments (Hayden 2003:111-112). These policies did not just impact the poor and undereducated; for example, around 1950, a cooperative housing development, that housed mainly faculty from Stanford University, limited availability to non-whites to 10%, in order to preserve financing for mortgages (Arrow 1998:92). The first consideration in the availability of labor is the overall distribution of the African American population. In 1870, 85.3% of all African Americans lived in the South, by 1910 that number dropped to 82.8%, by 1950 the number had dwindled to 61.5%, and by 1990 it was down to 46.2% living in Alabama, Arkansas, Florida, Georgia, Kentucky, Louisiana, Mississippi, North Carolina, South Carolina, Tennessee, Texas, or Virginia (Shelley & Webster 1998:168). In 1900, African Americans represented 34.3% of the South's overall population, in 1910 they still comprised 31.6% of the population; however, by 1950, they were only 22.5% of the total population, and that number dropped to 21% in 1960 (Nicholls 1964:35). Within the South, the African American urban population went from 8.8% in 1870 to 19.7% in 1910, while the white urban population went from 7.7% to 19.5% in that same time period; however, in 1920, 25.4% of whites and 23.5% of blacks were in urban areas, a slight change in the pace of urbanization that only occurred in the South (Roback 1984:1190). For the United States, as a whole, the African American population went from 79% rural in 1910, to 85% urban in 1980 (Aiken 1985:383). From 1870 to 1880, the relative rates of out-migration for whites and blacks were fairly similar; however, in the decade from 1880 to 1890 black out-migration slowed relative to whites in Alabama (42.3%), Mississippi (17.8%), and Tennessee (72%), and in the decade from 1890 to 1900 the same relative decline began in Arkansas (9.3%), Georgia (45%), and Kentucky (73.9%), in total numbers (Roback 1984:1188-1189). In the decade of World War I both groups were leaving the South, with whites leaving at a slightly higher rate; but in the decade of World War II, the South lost 1.58 million blacks, and only 866,000 whites (Wright 1987:174). In the decade from 1950 to 1960, the net out-migration was 1.2 million blacks, to only 234,000 whites; but, from 1960 to 1970 the picture changed dramatically, still losing 1.38 million blacks, but gaining 1.8 million whites. Starting in the decade of 1970–1980, there was a net influx of both groups, but with a markedly higher rate for whites, at 3.56 million to only 206,000. The raw numbers mask that the median education level of African Americans migrating out of the South was 6.6 years, up to 1960; whereas, by that same time, slightly more than a third of the white males in the South, with more than 5 years of college, had been born outside that region (Wright 1987:173). Thus, another factor that is masked by the raw numbers is that the areas African Americans were moving into were already experiencing black unemployment rates of up to 40%, and where there were few employers that utilized unskilled and undereducated labor, at all (Wright 1987:175). Under convict leasing, those who were convicted of a crime had their labor sold to employers by the prison system; in this case, the control over the prisoner was transferred to the employer, who had little concern for the well-being of the convict beyond the term of the lease (Roback 1984:1170). Ordinary debt peonage could affect any farmer working under the crop lien system, whether due to crop failure or merchant monopoly; however, the criminal-surety system functioned in a similar way, as the worker had little control over determining when their debt was to be considered repaid (Roback 1984:1174-1176). During the civil rights era, ""economic coercion"" was used to prevent participation, by denying credit, causing evictions, and canceling insurance policies (Bobo & Smith 1998:208). In 1973, only 2.25% of 5 million U.S. businesses were owned by African Americans; furthermore, 95% of those businesses employed fewer than 9 people, and two-thirds generated gross annual receipts of less than $50,000 (Bailey 1973:53). In the most extreme analysis, the level of urban residential segregation, along the unidirectional economic dependency of African American communities, presents the possibility that they may be treated as a ""national collectivity of internal colonies"" (Bailey 1973:61). From this perspective, small black-owned businesses are seen as the ""ghetto domestic sector,"" white-owned businesses that operate within the internal colonies are seen as the ""ghetto enclave sector,"" and the black laborers that work outside the community are seen as the ""ghetto labor-export sector"" (Bailey 1973:62). The idea of a black internal colony makes it especially notable that the Jim Crow era was brought to a close not only by the internal influences of the civil rights movement, but also from external pressures brought by international trading partners and decolonized developing nations (Cable & Mix 2003:198). The second consideration is how laws governing contract enforcement, enticement, emigrant agents, vagrancy, convict leasing, and debt peonage function to immobilize labor and restrict competition in a system where agriculture was the dominant consumer of labor. The South was overwhelmingly based on agricultural production through the postbellum years, only seeing substantial increases in industrial manufacturing starting in the 1930s; and, for those who did not own farmland, the dominant forms of employment were: farm laborer, sharecropper, share-renter, and fixed renter. Throughout this period there were some large landholders that used a set wage for farm laborers; however, the general lack of banks in the South made this arrangement problematic (Parker 1980:1024-1025). Using a set wage for laborers without contracts presented the problem of either overpaying during periods when labor demands were low, or risking the loss of the laborer during the peak of harvest season (Roback 1984:1172). Thus, the dominant pattern was to contract labor for an entire season which, when combined with the lack of liquid capital, favored the development of sharecroppers who received a share of the profits from the sale of the crops at the end of the season, or share-renters who paid a share of their crops as rent at the end of the season (Parker 1980:1028-1030). Whether white or black, the wage earned by the tenant farmer was relatively equal (Higgs 1973:151). Moreover, the tenant and the planter class landowner shared in the inherent risks of uncertain crop production; thus, external capital was invested in the merchant transporter who furnished staple goods in return, rather than in the agriculturalists directly (Parker 1980:1035). By the last decade of the 19th century, the planter class had recovered from the Civil War enough to both keep Northern industrialist manufacturing interests out of the South, and to take the role of merchant themselves (Woodman 1977:546). As the planter class came back to prominence, the rural and urban middle class lost power, and the poor rural tenant farmers were set in opposition based both on race, and the inherent superiority of the wealthy landowner (Nicholls 1964:25). It was in this social climate that the Jim Crow laws began to appear, amidst the Populist challenges of the tenant farmers of both races; thus, the laws may be seen as a tactic to drive a wedge between the members of the lowest social class, by using obvious physical traits to define the opposing sides (Roscigno & Tomaskovic-Devey 1996:568). Outside of laws that specifically addressed the issue of race, other laws that impacted the tenant farmer were often differentially enforced, to the detriment of African Americans. Enticement laws, and emigrant agent laws were geared toward immobilizing labor by preventing other employers from trying to lure employees away with promises of better wages; in the case of enticement the laws limited competition between landowners to the beginning of each contract season, and the emigrant agent laws created limitations on employers trying to lure out of the region altogether (Roback 1984:1166-1167;1169). Contract enforcement laws were contingent on demonstration of an intent to defraud the contractor, but often failure to live up to the terms of the contract were treated as intentional; these laws were addressed in the Supreme Court decision of Bailey v. Alabama. Vagrancy laws functioned to keep workers from exiting the labor force entirely, and were often used to forcibly ensure that every able body was engaged in some form of work; in some cases, African Americans were made into misdemeanants, through vagrancy laws, just on the basis of traveling outside the territory where they were personally known (Roback 1984:1168). In any case, African Americans were often disadvantaged in obtaining work contracts outside the areas where they were personally known, due to employers not wanting to pay the cost of having to check on their claims of specific knowledge or skills germane to a task (Ransom & Sutch 1973:139). The third consideration is how the overall transition from an agriculture-based economy to an urban, industrial economy. In the South, industrial growth started with labor-intensive, unskilled industries; for example, manufacturing employment increased from 14.5% in 1930 to 21.3% in 1960, but the increase was largest for non-durable goods (Nicholls 1964:26-27). For black males, in the South, agricultural employment dropped from 43.6% in 1940, to 4.9% in 1980; in that same time period, manufacturing employment rose from 14.2%, to 26.9% (Heckman & Payner 1989:148). There was also more pressure for African American women to work outside the home, often for low wages in the domestic service sector; for example, in the late 1930s, female domestics earned $3–8 per week, sometimes a bit less in the South (Thernstrom & Thernstrom 1999:35). For black females, throughout the South, manufacturing employment rose from 3.5% in 1940, to 17.2% in 1980; for that same time period, personal service employment decreased from 65.8%, to 13.7% (Heckman & Payner 1989:1989). One study, looking at non-agricultural employment from 1920 to 1930, determined that black males were losing jobs not to industrial mechanization, but to white males (Anderson & Halcoussis 1996:12). One of the major sources of wealth transfer is inheritance (Darity Jr. & Nicholson 2005:81). Race-based life insurance rates began in the early 1880s, and included higher rates, reduced benefits, and no commission for the insurance agent on policies written for African Americans. When state laws were passed to prevent race-based differential insurance rates, companies simply stopped selling insurance to black clients in those states (Heen 2009:369). When customers that had existing policies tried to purchase additional coverage from their local agent, at times when the company had stopped soliciting policies in that area, they were told they could travel to a regional office to make their purchase (Heen 2009:390-391). From 1896, scientific racism was used as the basis for declaring black clients as substandard risks, which also affected the ability of black-owned insurance companies to secure capital to provide their own policies (Heen 2009:387). By 1970, the black-owned insurance companies that had remained in business found themselves targeted for take over by white insurance companies that hoped to increase their number of black employees by acquiring smaller companies (Heen 2009:389). In the first decade of the 21st century, major insurance companies like Metropolitan Life, Prudential, American General, and John Hancock Life were still settling court cases brought by policy holders that had purchased their policies during the Jim Crow era (Heen 2009:360-361). Another economic impact of death is seen when the deceased does not have a will, and land is bequeathed to multiple people, under intestacy law, as tenancies in common (Mitchell 2000:507-508). Frequently, the recipients of such property do not realize that if one of the common owners wishes to sell their share, then the entire estate can be put up for partition sale. Most state statutes suggest that partition in kind be preferred over partition sale, except where properties cannot be divided equitably for the parties involved; however, many courts opt to require properties be put up for partition sale because the monetary value of the land is higher as a single parcel than a number of subdivided parcels, and also, to some extent, because the utility value of rural land is higher if it can be used a single productive unit (Mitchell 2000:514-515;563). This means that a land developer can purchase one person's share of a tenancy in common, and then use their position to force a partition sale of the entire property. Thus, a person who has inherited a common share of a property that they do not personally use, might be inclined to sell their share thinking that they are only selling the rights to a portion of the property, and wind up initiating the displacement of other inheritors that are actually living on the property. African American estate planning is thought to be minimal in rural, economically depressed areas, and developers are known to target properties in those areas (Mitchell 2000:517). An economic analysis, conducted at the end of the 1970s, concluded that even if the freed slaves had been given the 40 acres and a mule that had been promised by the Freedman's Bureau, it still would not have been enough to entirely close the wealth gap between whites and blacks, to that point in time (DeCanio 1979:202-203). In 1984, the median wealth for black households was $3,000, compared to $39,000 for white households (Bobo & Smith 1998:188). By 1993, the median wealth for black households was $4,418, compared to $45,740 for white households (Darity Jr. & Nicholson 2005:79). The research that underlies public program policy decisions continues to be guided by sensationalistic ""failure studies"" that focus on communities as liabilities, rather than identifying positive community aspects that programs could build upon as assets (Woodson 1989:1028;1039). Counting owners and tenants, there were 925,708 black farmers in 1920; in 2000, there were about 18,000 black farmers, which is roughly 11,000 less than the number of black farm owners in 1870 (Mitchell 2000:527-528). As the recent decision of Pigford v. Glickman has shown, there are still race-based biases in way government entities like the United States Department of Agriculture decide how to disburse farm credit. By federal regulation, the local commissions that make the decisions must be elected from current farm owners; in two cases unrelated to the Pigford decision, five different county commissioners were found to have wrongly denied disaster assistance to African American farmers (Mitchell 2000:528-529). Additionally, black farmers trying to obtain credit to purchase farmland being lost by black owners ""experienced delays"" while funding was being extended to white borrowers (Reynolds 2002:16). African American residential centralization, which started in the postbellum and Great Migration periods, continues to have a negative impact on employment rates (Herrington et al.:169). In fact, ""one third of African Americans live in areas so intensely segregated that they are almost completely isolated from other groups in society"" (Mitchell 2000:535). The unemployment effects of residential centralization are twice as problematic in metropolitan areas with total populations over 1 million (Weinberg 2000:116). A one standard deviation reduction in residential centralization could reduce unemployment by about a fifth; and, a complete elimination of residential centralization could reduce unemployment by almost half for high school educated males, and nearly two-thirds for college educated males and females (Weinberg 2000:126)."
Juan Crow,/wiki/Juan_Crow,"Juan Crow is political terminology that was coined by journalist Roberto Lovato.[1][2][3][4][5][6] It first gained popularity when he used it in an article for The Nation magazine in 2008.[7] ""Call it Juan Crow: the matrix of laws, social customs, economic institutions and symbolic systems enabling the physical and psychic isolation needed to control and exploit undocumented immigrants."" Lovato utilized the term to criticize immigration enforcement laws by analogizing them to Jim Crow laws, and has since become popular among immigration activists. In recent years, the term Juan Crow has also been used to discuss the historical discrimination against Mexican Americans in the U.S. as analogous to the treatment of African Americans in the Jim Crow era, specifically as related to mob violence and segregation in schools.[8][9][10][11] The term Juan Crow was first used to refer to immigration enforcement statutes in the United States that penalize illegal immigration and deny services to undocumented people living in the U.S. unlawfully.[12][13][14] Laws in Arizona,[15]Alabama,[12]Georgia,[7] and Texas[16][17] have been considered Juan Crow laws. California's Proposition 187 was considered a Juan Crow law by immigration activists. It required citizenship screening of residents and denied social services like health care and public education to undocumented immigrants.[18] The term Juan Crow has also been used to refer to historical instances of mob violence, as well as de facto discrimination that specifically targeted Mexicans and people of Mexican descent.[8][19][20][21] Between the years 1848 and 1928, mob violence against people of Mexican descent totaled 547 lynchings.[22] Texas holds the highest tally with 232 victims.[22] Other Southwest states, which include California, Arizona, and New Mexico, range between 25 and 143 lynching murders.[22][23] In the 1850s, after the Mexican-American war, Anglo-Americans were concerned about the potentiality of Mexican-Americans responding to Mexican newspapers that called for the Reconquista (reconquering).[8] Consequently, Anglo-Americans advocated for the systemic inequity of Mexican-Americans through social exclusion and lynchings.[8] The mistreatment persisted for several decades, with the Texas Rangers acting as enforcers and overseeing 232 Mexican-American men to violent attacks by mob violence between 1848 and 1928.[8][24] Mexican-Americans were often victims of lynching by Anglo-American society, but there were also occurrences of Mexicans lynching Mexicans.[22][23] In particular, Mexican-Americans of higher class status who were aligned with Anglo ranchers participated in such acts.[22][23] The culture's acceptance of lynching impacted Mexican standards during the 19th and 20th centuries.[22][23] Mexican Americans were not only hung, but mob violence included other forms of brutality such as shooting, burning people alive, physical mutilation, and other deadly acts of persecution.[22][23][25] During the 1870s and 1880s, the use of the derogatory term “greaser” promoted the Texas Rangers to carry out a campaign against the Mexican populace of the Rio Grande Valley.[8] They believed that by instilling fear, they could more effectively suppress the Mexican population.[8] In 1918, a group of Anglo ranchers and the Texas Rangers arrived at a village in Presidio County, Porvenir, where 140 refugees, including women, children, and men, resided.[25][8][24] Despite no evidence of weapons or stolen goods, thirteen Mexican men and two teenage boys were killed on suspicion of banditry.[25][8][24] The Porvenir massacre, as described by historian Miguel A. Levario, exposed the violence committed by the Rangers against Mexicans.[8] With a dual identity, the Texas Rangers are an emblem of Texan pride from an Anglo perspective.[24] They enhanced the quality of life for colonists by actively confronting and defeating Indigenous peoples, outlaws, and Mexicans.[24] However, for their Mexican victims, they are a source of terror and oppression.[24] The segregation of Mexican American students in academia is a disputed topic with various perspectives.[26] Mexicans, who were then considered to be white, were never legally segregated and normally went to white schools, but some children were placed in special education within the white school system due to language. It is being asserted as racial segregation by some historians today. The state did not officially sanction their discrimination.[26][27] Some scholars argue it was de facto segregation from the local customs that intentionally separated Mexican American students.[26] In contrast, others express it was de jure segregation as school officials enforced their policies.[26] Although legally classified as ""White,"" some historians argue that Mexican Americans were socially perceived as ""colored"" and subject to segregation in schools and communities.[26][27] Despite the lack of state-sanctioned segregation laws, it was a prevalent trend in the American Southwest.[26][27]"
USS Kitty Hawk riot,/wiki/USS_Kitty_Hawk_riot," The USS Kitty Hawk riot was a racial conflict between white and black sailors aboard the United States Navy aircraft carrier Kitty Hawk on the night of 12–13 October 1972, while positioned at Yankee Station off the coast of North Vietnam during the Vietnam War. In the early days of the Vietnam War, African-American service members represented less than five percent of personnel in the United States Navy.[1] The draft enticed men of all races to enlist in the Navy as a way to avoid heavy combat. This resulted in stiff competition, allowing Navy recruiters to enlist only the top performers on the Armed Forces Qualification Test. This was known as ""Qualitative Recruitment""—recruiting the ""highest quality"" sailors, of whom nearly all happened to be white, as the quality of the education that white candidates had received was far superior to that of the black candidates. This made it improbable for black candidates to ever really compete with their white counterparts.[1] By 1971, after President Richard Nixon sought to create an all-volunteer military and the U.S. had begun to disengage from Vietnam, the number of men drafted dramatically decreased, and demand to join the Navy decreased with it. The Navy recruitment quota fell by more than fifty percent from 1971 to 1972, which resulted in the Navy needing black men to achieve its recruitment goals.[1] Black men joined the Navy at a high rate, increasing their overall representation to twenty percent. Due to scoring lower on their qualification exams, blacks were more likely to be placed in less desirable jobs. Whites were often promoted to the more desirable jobs and accounted for 99% of the Navy's officers.[1][2][3] By October 1972, the majority of the black enlisted sailors on the aircraft carrier Kitty Hawk had been serving for less than a year. Of approximately 4,500 sailors on the ship, less than seven percent were black.[1]Racial tensions were reportedly high on the ship. Politically, black sailors tended to support the advancement of social minorities in the Navy, views which conflicted with the obstacles they were faced with due to their lack of education. This created hostility on the ship and compounded the frustration the sailors felt from being at sea for nearly eight months.[1] The first racial incident occurred at Subic Bay Naval Base, in Olongapo, Philippines. Racial segregation was enforced in Olongapo – the white section was known as ""The Strip,"" while the black section was known as ""The Jungle.""[4] On the night of 8 October 1972, a fight between black and white sailors broke out at the base enlisted men's club. The situation escalated when a black sailor took to the stage and began voicing his opposition to the ""white man's war"" and advocating ""black power"", leading to a white sailor throwing a glass at the black sailor's head. The brawl spread throughout the club and ultimately was broken up by base Marines.[5] The black sailors were told not to go back to the EM Club. Around 12:30 am on 9 October, another incident occurred when Dwight Horton, a black airman en route to Kitty Hawk, was arrested for fighting with two white petty officers. Horton asserted that the noncommisioned officers beat him. He argued he could not fight back because his arm was in a cast. When he returned to Kitty Hawk, Horton told the other black sailors about what happened, which further agitated them.[6] On 10 October 1972, the black sailors decided to disrupt a favorite hangout of white sailors on The Strip – the Sampaguita Club – to retaliate against Horton's treatment. That night was designated ""Soul Night"" at the club, which was the only night black sailors were welcome. Around 9:00 pm, a petty scuffle began when a black sailor punched a white shore patrolman for tapping him on the shoulder. To keep the situation from escalating, fifteen additional shore patrolmen were summoned to monitor the club. At around 1:00 am, ten black sailors walked on stage and began ""dapping"" each other – exchanging physical gestures of greeting common in African-American communities – which provoked the white sailors in the audience. As the white sailors began berating the black sailors with epithets, the black sailors in the crowd voiced their solidarity.[7] Meanwhile, outside the club, Horton arrived and threw a punch at another shore patrolman, distracting the patrolmen standing guard. White sailors began throwing beer bottles at the black sailors on stage, resulting in another brawl between black and white sailors that was again broken up by base Marines.[7] The following morning the sailors returned to Kitty Hawk, bloodied and bruised from the previous night, and the ship went to sea.[8] On the afternoon of 12 October, while Kitty Hawk was participating in Operation Linebacker off the coast of North Vietnam, three black sailors went on deck. The three were approached by two Marines who told them, ""You blacks can't walk in over twos."" When the black sailors ignored them and kept walking, one of the Marines used a nightstick to put one of the sailors, Perry Pettus, into a stranglehold.[9] When Captain Marland Townsend Jr. of Kitty Hawk learned of the incident, he apologized to the three black sailors. Word of the incident made its rounds among black sailors, who were already incensed by events at Subic Bay. Thirty minutes after flight operations, one of those black sailors, 18-year-old Airman Apprentice Terry Avinger, went to the mess deck to eat and requested two sandwiches. A white mess cook refused and limited Avinger to one sandwich. Avinger then reached across the food line and took another sandwich, which resulted in a shouting match between him and the mess cook.[1] Things escalated after another white mess cook, organizing food trays, stepped on a black sailor's foot.[1] Upset about what transpired, Avinger went to a bunk area where black sailors regularly got together and expressed his frustration about the way they were being subjugated by whites on the ship, telling them he regretted ""that he didn't just beat the racist cracker's ass right there."" He railed that ""black sailors on the Kitty Hawk had had enough and it was time to stand up for themselves.""[1] The black sailors then went into the ship's passageway and armed themselves with makeshift weapons – broom handles, wrenches, a foam fog nozzle and pieces of pipe. They then began beating white sailors and vandalized some of the ship's compartments.[1] Around 8:00 p.m., a white cook called for the Marine detachment on board. When the white Marines arrived, they ordered the black sailors to the aft mess. The black sailors thought that the Marines were corralling them there in order to beat or kill them.[2][10] This resulted in a stand-off between the two groups. News of what was happening reached Kitty Hawk's half‐black/half‐Native executive officer, Commander Ben Cloud, who had been aboard the ship for eight weeks. Informed that the situation was potentially deadly, Cloud went on the ship's communication system and ordered the violence to stop, pleading for the black sailors to go to the aft mess and for the Marines to stand down and go to the forecastle.[11] Cloud was unaware that Captain Townsend had also been briefed on what was happening and was on his way to the mess deck.[1] Cloud went to the mess deck to talk to the black sailors for about an hour, trying to calm them down and assure them that he could be trusted: ""For the first time, you have a brother who is an executive officer. My door is always open."" Their anger subsiding, the black sailors gave a Black Power salute in solidarity to Cloud, who returned the salute. The black sailors celebrated, feeling that they had someone in a position of authority who was sympathetic to their treatment on board.[1] Cloud then dismissed the sailors and told them to get back to work. Townsend arrived on the mess deck and, witnessing Cloud's handling of the situation, disagreed with his method. He left the mess deck and summoned the Marine detachment, ordering them to increase patrols in the black compartments.[1] Despite the de-escalation, tensions were still high. Groups of between five and twenty-five black sailors continued to roam Kitty Hawk, attacking whites at random throughout the night. Sleeping sailors were pulled from their racks and beaten with fists, chains, wrenches and broom handles, with many also shouting epithets such as, ""Kill the white trash!” The mess cook who had earlier confronted Avinger was found and beaten after a mock trial.[1] Cloud again intervened when he saw some black sailors heading to the forecastle, where the Marines had been ordered to go. By Cloud's own admission, ""he believed that had he not been black he would have been killed on the spot."" Cloud talked to the sailors for two hours, appealing to them not as a senior officer but ""as one black to another.""[1] By 2:30 am, Cloud had calmed the black sailors and persuaded them to relinquish their weapons. About forty sailors went to the mess deck to eat, play cards and listen to music along with a few white sailors. At 3:00 a.m., Townsend told Cloud he did not want large groups of blacks congregating in the mess hall, and likened the gathering to a ""victory party.""[2][12] Townsend and Cloud dispersed the group and met with any sailors who were still upset in the forecastle until 5:00 a.m. Many white sailors aboard the massive ship were unaware that the incident had occurred, and began to hear rumors when they awoke. Becoming increasingly angry, about 150 white sailors began to arm themselves and congregated in a berthing compartment, readying themselves for what they thought would be an outright racial battle for control of Kitty Hawk.[12] Hearing of the discord, Cloud went to address the group, who dismissed him as being ""nothing more than a nigger, like the rest of them.""[12] When Cloud pulled rank on them and threatened them with legal action if they proceeded, the white sailors dispersed. Cloud reported the incident to Townsend and then continued to talk to concerned sailors – both white and black – throughout the morning, reducing the threat of white retaliation.[12] By 7:58 a.m., the confrontation had completely ended, and the Kitty Hawk resumed bombing North Vietnam.[12] In total, the incident left forty white sailors and six black sailors injured, including three who had to be evacuated to onshore medical facilities.[13] Six weeks after the incident, Kitty Hawk returned to San Diego, California, where twenty-seven black sailors were arrested and charged. No white sailors were arrested.[2][10] Twenty-one of those charged requested courts-martial. Lawyers for the black sailors stressed the bias shown in the pre-trial report against the black sailors, stating that it only contained testimony from prosecution witnesses.[11] By December, Congress had begun investigating the incident and called Townsend and Cloud to testify. Most of those who requested a court martial were also invited to testify, but they all declined and no subpoenas were issued to force them to do so. In January 1973, before a Navy court-martial, Cloud testified that the fighting erupted when Marines, on orders to break up groups of three or more sailors, only enforced the order against black sailors.[14] He further testified that he had been threatened by black and white sailors alike, and that during the fighting between black sailors and Marines he witnessed a white sailor seemingly directing Marines toward black sailors. He noted that Townsend requested that the white sailor be identified, ""but this was not done.""[14] In February, on behalf of seventeen of the black sailors, the National Association for the Advancement of Colored People (NAACP) brought a complaint against a prosecutor for racial prejudice in an attempt to get the charges against the black sailors dismissed. The complaint also accused Michael A. Laurie, a white sailor who had been a key government witness, for perjury. Citing tape recordings of Laurie admitting that white sailors had ""exaggerated"" the violence of black sailors; then later affirming that he had lied about black sailors when he was asked outright.[15] Laurie elaborated that, despite not seeing any black sailors actually hit any white sailors, white sailors would say that they did. Laurie also demonstrated racist leanings when he expressed his regret for not being armed on the night of the riot since it would have allowed him to have killed ""at least 30 of them [black sailors].""[16][15] In April 1973, the courts martial concluded with a total of twenty-seven trials.[13] Four sailors were convicted of rioting, with two of those pleading guilty in exchange for reduced sentences. Fourteen were convicted of assault. Four were found not guilty of all charges. Five sailors had the charges dropped against them, and seven were sentenced to the brig. Most were given a demotion in rank. The events on Kitty Hawk inspired other ship riots and protests in the months that followed.[17] In October, around the same time as the events aboard Kitty Hawk, a series of isolated interracial attacks occurred aboard USS Constellation. On November 3, as Constellation was sailing toward San Diego, black sailors staged a sit-down strike in the enlisted men's mess deck. The strike was chaotic, but one central grievance was that six black sailors were to be given general discharges rather than honorable discharges. Captain Ward avoided meeting directly with the men because, according to a public relations officer, that would have implied a ""recognition of some sort of union"" and a ""breakdown of the chain of command."" Seaman Edward A. Martinez was elected as a representative, but attempts at mediation with an officer, Commander Yacabucci, did not succeed in defusing the situation. At the dock in San Diego on Nov. 4, there was another sit-down protest, and 120 sailors who did not return to the ship were charged with being ashore without leave, receiving light punishments. The Navy avoided describing the events as a mutiny.[18] The Navy officially defined the incident aboard Kitty Hawk as a race riot. However, only four sailors were convicted of rioting, with two of those pleading guilty in exchange for reduced sentences. Fourteen were convicted of assault. Four were found not guilty of all charges. Five sailors had the charges dropped against them, and seven were sentenced to the brig. Most were given a demotion in rank.[13] Roy Wilkins, executive director of the NAACP, called the Navy's handling of the incident a ""despicable perversion of justice"" of the black sailors who were victims of ""a spurious effort to discredit them, categorize them, and keep them in menial, low-paying jobs.""[16] Many black officers also expressed that the riot was inevitable because the Navy was inept at treating black sailors as sailors rather than as blacks, which created differences in the way black sailors were treated over issues such as ""promotion, assignments, interracial relationships.""[2][3] However, despite these accounts, Floyd Hicks, the Chairman of the House Armed Services Subcommittee, determined that the incident ""consisted of unprovoked attacks"" by blacks against whites.[2][10][14] The subcommittee wrote that ""the riot on Kitty Hawk consisted of unprovoked assaults by a very few men, most of whom were below-average mental capacity, most of whom had been aboard for less than one year, and all of whom were black. This group, as a whole, acted as 'thugs' which raises doubt as to whether they should ever have been accepted into military service in the first place.""[19][17] The Subcommittee's final report concluded: The subcommittee has been unable to determine any precipitous cause for rampage aboard U.S.S. Kitty Hawk. Not only was there not one case wherein racial discrimination could be pinpointed, but there is no evidence which indicated that the blacks who participated in that incident perceived racial discrimination, either in general or any specific, of such a nature as to justify belief that violent reaction was required ... The members of the subcommittee did not find and are unaware of any instances of institutional discrimination on the part of the Navy toward any group of persons, majority or minority ... Black unity, the drive toward togetherness on the part of blacks, has resulted in a tendency on the part of black sailors to polarize. This results in a grievance of one black, real or fancied, becoming the grievance of many ... The Navy's recruitment program for most of 1972 which resulted in the lowering of standards for enlistment, accepting a greater percentage of mental category IV and those in the lower half of category III, not requiring recruits in these categories to have completed their high school education, and accepting these people without sufficient analysis of their previous offense records, has created many of the problems the Navy is experiencing today."
Legacy preferences,/wiki/Legacy_preferences," Legacy preference or legacy admission is a preference given by an institution or organization to certain applicants on the basis of their familial relationship to alumni of that institution. It is most controversial in college admissions,[3] where students so admitted are referred to as legacies or legacy students. The practice is particularly widespread in the college admissions in the United States; almost three-quarters of research universities and nearly all liberal arts colleges grant legacy preferences in admissions.[4] Schools vary in how broadly they extend legacy preferences, with some schools granting this favor only to children of undergraduate alumni, while other schools extend the favor to extended family, including: children, grandchildren, siblings, nephews, and nieces of alumni of undergraduate and graduate programs.[5] A 2005 analysis of 180,000 student records obtained from nineteen selective colleges and universities found that, within a set range of SAT scores, being a legacy raised an applicant's chances of admission by 19.7 percentage points.[6] Legacy preferences are controversial, as the legacy students tend to be less qualified and less racially diverse than non-legacy students.[7] However, legacy students are economically beneficial to universities, as they are perceived to be more likely to donate to their university after graduation and have parents who are perceived to be more generous donors.[7] Legacy preferences are particularly prevalent at Ivy League universities and other selective private universities in the United States.[8] In the United States, legacy admissions in universities date back to the 1920s. Elite schools used legacy admissions to maintain spots for White Anglo-Saxon Protestants amid fears that Jews, Catholics and Asians were increasingly taking spots at the schools.[9][10] A 1992 survey found that of the top seventy-five universities in the U.S. News & World Report rankings, only one (the California Institute of Technology) had no legacy preferences at all; the Massachusetts Institute of Technology also affirmed that it does not practice legacy admissions.[11] Legacy preferences were almost ubiquitous among the one hundred top-ranked liberal arts colleges as well. The only liberal arts college in the top one hundred that explicitly said it did not use legacy preferences was Berea. Beginning in the 2010s, several top schools ended legacy preferences, including Johns Hopkins University in 2014,[12]Pomona College in 2017,[13][14]Amherst College in 2021,[15] and Wesleyan University in 2023.[16][17] Currently, the Ivy League institutions are estimated to admit 10% to 15% of each entering class using legacy admissions.[18] For example, in the 2008 entering undergraduate class, the University of Pennsylvania admitted 41.7% of legacies who applied during the early decision admissions round and 33.9% of legacies who applied during the regular admissions cycle, versus 29.3% of all students who applied during the early decision admissions round and 16.4% of all who applied during the regular cycle.[19] In 2009, Princeton admitted 41.7% of legacy applicants—more than 4.5 times the 9.2% rate of non-legacies. Similarly, in 2006, Brown University admitted 33.5% of alumni children, significantly higher than the 13.8% overall admissions rate. In short, Ivy League and other top schools typically admit legacies at two to five times their overall admission rates.[20] Among top universities, the University of Notre Dame and Georgetown University are known to weigh legacy status heavily in their application processes.[21] A 2019 National Bureau of Economic Research working paper by Peter Arcidiacono found that 43% of students admitted to Harvard College were either athletes, legacies, members of the ""Dean's"" or ""Director's"" lists of relations of donors or prominent figures, or children of university employees (""ALDCs""); fewer than 16% of ethnic minority Harvard undergraduate admits were ALDCs.[23] Arcidiacono also found that almost 70% of Harvard legacy applicants were white.[24] A similar study at an elite college found that legacies were almost twice as likely to be admitted as non-legacies and that legacy preferences increased the admission rates for white and wealthy students to the greatest degree. [25] The advantages that colleges offer legacy students extend well beyond admission preferences. Many colleges have various mechanisms for coaching legacies through the admissions process and for advising them about strategies for constructing successful applications, including notifying legacies of the advantage that they can gain by applying early. Some universities have alumni councils that provide legacies with special advising sessions, pair these prospective students with current legacy students, and generally provide advice and mentoring for legacy applicants. Some universities employ admissions counselors dedicated solely to legacy applicants, and it is common to provide scholarships or tuition discounts earmarked especially for legacies and for legacies to be charged in-state tuition fees when they are out-of-state residents.[19] In cases where legacies are rejected, some universities offer legacy admissions counseling and help with placement at other colleges. Such students are often encouraged to enroll at a lesser ranked school for one or two years to prove themselves and then to reapply as transfer students. Because rankings by U.S. News & World Report and other media take into account only the SAT scores and high school grades of entering freshmen, a college can accept poor achieving legacies as transfer students without hurting its standing. Harvard caters to the children of well-connected alumni and big donors through the ""Z-list."" Z-listers are often guaranteed admittance while in high school but are obliged to take a year off between high school and Harvard, doing whatever they wish in the interim.[26] Former Harvard University president Lawrence Summers has stated, ""Legacy admissions are integral to the kind of community that any private educational institution is."" In the 1998 book The Shape of the River: Long-Term Consequences of Considering Race in College and University Admissions, authors William G. Bowen, former Princeton University president, and Derek Bok, former Harvard University president, found ""the overall admission rate for legacies was almost twice that for all other candidates."" While the preference is quite common in elite universities and liberal arts colleges, it is quite controversial, with 75% of Americans opposing the preference.[27] Wesleyan University announced in 2023 that it would no longer lend preference to 'legacy' applicants.[28] Economists are divided over implications of the practice.[29] A 2019 study of leading economists by the University of Chicago Booth School of Business (IGM Forum) found that 76% of economists responding surveyed either ""strongly agreed"" or ""agreed"" that legacy preferences crowds out applicants with greater academic potential.[30] The economists were divided as to whether the existence of legacy admissions meant that universities had a less beneficial ""net effect"" on society than if there were no legacy admissions: 2% strongly agreed, 29% agreed, 40% were uncertain, 19% disagreed, and none strongly disagreed. (10% did not respond).[30] Panelist David Autor commented that ""There are clear costs + benefits, But the optics are terrible, which degrades public faith in ostensibly meritocratic institutions.""[30] Many economists noted that the effect of legacy admissions (or ending legacy admissions) was difficult to determine, given the unclear relationship (elasticity) between donations and admission of children and the unclear effects of legacy admissions on donations and class size/higher education capacity.[30] Some studies suggest legacy admissions practices marginally increase donations from alumni,[31][32] though other analyses have disputed this conclusion.[33] At some schools, legacy preferences have an effect on admissions comparable to other programs such as athletic recruiting or affirmative action. One study of three selective private research universities in the United States showed the following effects (admissions disadvantage and advantage in terms of SAT points on the 1600-point scale): Although it may initially appear that non-Asian students of color are the most favored of all the groups in terms of college admissions, in practice, widespread legacy preferences have reduced acceptance rates for black, Latino, and Asian-American applicants because the overwhelming majority of legacy students are white. According to a 2008 study, Duke's legacies are more likely to be white, Protestant, American citizens, and private high school graduates than the overall student body. In 2000-2001, of 567 alumni children attending Princeton, 10 were Latino and 4 were black. Similarly, a 2005 study reported that half of the legacy applicants to selective colleges boasted family incomes in the top quartile of American earnings, compared to 29% of non-legacy students.[35] In 2003, Texas A&M—which no longer practices legacy admissions—enrolled 312 white students and only 27 Latino and 6 black students who would not have been admitted if not for their family ties.[36] Since 1983, there have been formal complaints to the Education Department's Office for Civil Rights (OCR) that Asian-American applicants are being rejected in favor of students with lesser credentials.[37] In 1990, the OCR determined that Harvard had admitted legacies at twice the rate of other applicants, that in several cases legacy status ""was the critical or decisive favor"" in a decision to admit an applicant, and that legacy preferences help explain why 17.4% of white applicants were admitted compared with only 13.2% of Asian-American applicants during the previous decade. The OCR also found that legacies on average were rated lower than applicants who were neither legacies nor athletes in every important category (excluding athletic ability) in which applicants were judged.[38] In the 1990s, the University of California's Board of Regents voted to ban the use of affirmative action preferences throughout the system, and legacy privilege was abandoned across the University of California system soon after.[39] The Supreme Court upheld race-conscious admissions policies in its 2003 Grutter v. Bollinger decision, involving the University of Michigan's law school. The only significant criticism of legacy preferences from the Court came from Justice Clarence Thomas, the sole member of the Supreme Court who grew up in poverty.[40] While the majority of Americans have been shown to strongly oppose legacy admissions, its beneficiaries hold key positions in Congress and the judiciary, protecting this practice from political and legal challenge.[41] While many schools say that a main reason for legacy preferences is to increase donations,[42] at an aggregate (school-wide) level the decision to prefer legacies has not been shown to increase donations.[43] However, in some instances, while alumni donations may go up if a child is intending on applying, donations fall if that child is rejected.[44] In 2008, alumni donations accounted for 27.5% of all donations to higher education in the U.S.[19] Because private universities in the U.S. rely heavily on donations from alumni, critics argue that legacy preferences are a way to indirectly sell university placement. Opponents accuse these programs of perpetuating an oligarchy and plutocracy as they lower the weight of academic merit in the admissions process in exchange for a financial one. Legacy students tend to be the white and wealthy, contributing to socioeconomic inequality. Supporters of the elimination of all non-academic preferences point out that many European universities, including highly selective institutions[45] such as Oxford, Cambridge, UCL and London School of Economics do not use legacy, racial, or athletic preferences in admissions decisions.[46][47] There are also legal arguments against legacy preferences. In public schools, legacy preferences may violate the Nobility Clause and the Equal Protection Clause of the U.S. Constitution by creating a hereditary privilege and discriminating on the basis of ancestry.[48] Legacy preferences in both public and private universities may be illegal under the Civil Rights Act of 1866 (now codified in Section 1981 of the U.S. Code). At Harvard, legacies have higher median SAT test scores and grades than the rest of admitted students.[33] According to The Atlantic, ""While some research indicates that legacy admits go on to earn lower average grades than their peers, plenty are strong applicants.""[33] In a paper published in Economics Letters, economist James Monks compared the academic performance of legacy students to that of non-legacy students from 27 private and selective colleges. Monks finds that legacies perform at least as well as their nonlegacy counterparts.[49] In admission data reviewed by The Daily Princetonian in 2023, the student newspaper found that legacy students had higher a GPA than non-legacy students except at the highest income levels, and were more likely to go into non-profit work after admission, and less likely to go on to graduate school.[50]"
Lloyd–La Follette Act,/wiki/Lloyd%E2%80%93La_Follette_Act,"The Lloyd–La Follette Act of 1912 began the process of protecting civil servants in the United States from unwarranted or abusive removal by codifying ""just cause"" standards previously embodied in presidential orders. It defines ""just causes"" as those that would promote the ""efficiency of the service."" August 24, 1912, § 6, 37 Stat. 555, 5 U.S.C. § 7511 The Act further states that ""the right of employees ... to furnish information to either House of Congress, or to a committee or Member thereof, may not be interfered with or denied."" 5 U.S.C. § 7211 Under the leadership of Republican Senator Robert M. La Follette, Sr.,[1] the United States Congress passed the Act with the intention of conferring job protection rights on federal employees they had not previously had. Prior to this, there was no such statutory inhibition on the authority of the government to discharge a federal employee, and an employee could be discharged with or without cause for conduct which was not protected under the First Amendment. James Tilghman Lloyd a Democratic congressman from Missouri, led the effort to pass the bill in the House of Representatives. The act was passed after the Theodore Roosevelt (in 1902) and Taft (in 1909) administrations prohibited federal employees from communicating with Congress without authorization from their superiors. This language was later placed in the Civil Service Reform Act of 1978 and codified in 5 U.S.C. § 7211. The purpose of this Act was to allow Congress to obtain uncensored, essential information from federal employees. Congress intended to allow the federal workers direct access to Congress in order to register complaints about conduct by their supervisors and to report corruption or incompetence.[2] In Arnett v. Kennedy[3] the Supreme Court addressed questions about the Act. It held that the Act's standard of employment protection, which describes as explicitly as is feasible in view of the wide variety of factual situations where employees' statements might justify dismissal for ""cause"" the conduct that is ground for removal, is not impermissibly vague or overbroad in regulating federal employees' speech. One of the primary purposes of the Act was to protect those who criticize superiors from official retribution. Senator La Follette gave the following example of an abuse sought to be cured by the bill: The cause for [the employee's] dismissal was that he gave publicity to the insanitary conditions existing in some part of the post-office building in Chicago where the clerks were required to perform their services. ... [H]e furnished some facts to the press of Chicago, and the publication was made of the conditions. They were simply horrible. ... The public health officers of Chicago, as soon as their attention was called to the conditions, condemned the situation as they found it; and yet this young man, one of the brightest fellows I have met, was removed from the service because, he had given publicity to these outrageous conditions. 48 Congressional Record, Vol. -1806, Page 10731 (1912). The Act was thus the first federal law enacted specifically to protect whistleblowers. The history and scope of the Act was further described by the Supreme Court of the United States in Bush v. Lucas, 462 U.S. 367, 103 S.Ct. 2404 (1983).[4] Congressional attention to the problem of politically-motivated removals was again prompted by the issuance of Executive Orders by Presidents Roosevelt and Taft that forbade federal employees to communicate directly with Congress without the permission of their supervisors. ... These ""gag orders,"" enforced by dismissal, were cited by several legislators as the reason for enacting the Lloyd–La Follette Act in 1912, 37 Stat. 555, § 6.FN20 That statute ... explicitly guaranteed that the right of civil servants ""to furnish information to either House of Congress, or to any committee or member thereof, shall not be denied or interfered with."" FN22 As the House Report explained, this legislation was intended ""to protect employees against oppression and in the right of free speech and the right to consult their representatives."" FN23 In enacting the Lloyd–La Follette Act, Congress weighed the competing policy considerations and concluded that efficient management of government operations did not preclude the extension of free speech rights to government employees.FN24 Footnote 20. See 48 Cong.Rec. 4513 (1912) (remarks of Rep. Gregg) (""[I]t is for the purpose of wiping out the existence of this despicable 'gag rule' that this provision is inserted. The rule is unjust, unfair, and against the provisions of the Constitution of the United States, which provides for the right of appeal and the right of free speech to all its citizens."") A number of the bill's proponents asserted that the gag rule violated the First Amendment rights of civil servants. See, e.g., id., at 4653 (remarks of Rep. Calder) (1912); id., at 4738 (remarks of Rep. Blackmon); id., at 5201 (remarks of Rep. Prouty); id., at 5223 (remarks of Rep. O'Shaunessy); id., at 5634 (remarks of Rep. Lloyd); id., at 5637-5638 (remarks of Rep. Wilson); id., at 10671 (remarks of Sen. Ashurst); id., at 10673 (remarks of Sen. Reed); id., at 10793 (remarks of Sen. Smith); id., at 10799 (remarks of Sen. La Follette). Footnote 22. This provision was accompanied by a more specific guarantee that membership in any independent association of postal employees seeking improvements in wages, hours, and working conditions, or the presentation to Congress of any grievance, ""shall not constitute or be cause for reduction in rank or compensation or removal of such person or groups of persons from said service."" Footnote 23. H.R.Rep. No. 388, 62d Cong., 2d Sess. 7 (1912). Footnote 24. Members of the House, which originated § 6, suggested that it would improve the efficiency and morale of the civil service. ""It will do away with the discontent and suspicion which now exists among the employees and will restore that confidence which is necessary to get the best results from the employees."" 48 Cong.Rec. 4654 (1912) (remarks of Rep. Calder); see id., at 5635 (remarks of Rep. Lloyd). The Senate Committee initially took a different position, urging in its report that the relevant language, see id., at 10732 (House version) be omitted entirely: As to the last clause in section 6, it is the view of the committee that all citizens have a constitutional right as such to present their grievances to Congress or Members thereof. But governmental employees occupy a position relative to the Government different from that of ordinary citizens. Upon questions of interest to them as citizens, governmental employees have a right to petition Congress direct. A different rule should prevail with regard to their presentation of grievances connected with their relation to the Government as employees. In that respect good discipline and the efficiency of the service requires that they present their grievances through the proper administrative channels."" S.Rep. No. 955, 62d Cong.2d Sess. 21 (1912). As Sen. Bourne explained, ""it was believed by the committee that to recognize the right of the individual employee to go over the head of his superior and go to Members of Congress on matters appertaining to his own particular grievances, or for his own selfish interest, would be detrimental to the service itself; that it would absolutely destroy the discipline necessary for good service."" 48 Cong.Rec. 10676 (1912). This view did not prevail. After extended discussion in floor debate concerning the right to organize and the right to present grievances to Congress, id., at 10671-10677, 10728-10733, 10792-10804, the committee offered and the Senate approved a compromise amendment to the House version, guaranteeing both rights at least in part, which was subsequently enacted into law. Id., at 10804; 37 Stat. 555. In 1997, the Justice Department argued that Congress does not have a constitutional right to obtain information from civil servants through unauthorized disclosures. Based on its analysis of disclosure laws and its stance on separation of powers, Justice argued that Congress cannot vest ""in executive branch employees a right to provide classified information to members of Congress without official authorization.""[5] In 1997, Congress adopted an anti-gag rule. The government-wide prohibition on the use of appropriated funds to pay the salary of any federal official who prohibits or prevents or threatens to prohibit or prevent a federal employee from contacting Congress first appeared in the Treasury and General Government Appropriations Act, 1998, Pub. L..mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px;white-space:nowrap}Tooltip Public Law (United States) 105–61 (text) (PDF), 111 Stat. 1318, (1997). In 1997, the Senate passed a prohibition that applied only to the Postal Service, while the House of Representatives passed a government-wide prohibition. The conference report adopted the House version, and a government-wide prohibition has been included in every Treasury-Postal appropriations act since fiscal year 1998.[6] This provision has its antecedents in several older pieces of legislation, including the Treasury Department Appropriation Act of 1972, the Lloyd–La Follette Act of 1912, and the Civil Service Reform Act of 1978.[7] In 2006, Rep. John Conyers included the Lloyd–La Follette Act in a list of 26 laws that he contends President George W. Bush violated.[8][9]"
Marriage of enslaved people (United States),/wiki/Marriage_of_enslaved_people_(United_States),"Marriage of enslaved people in the United States was generally not legal before the American Civil War (1861–1865). Enslaved African Americans were considered chattel legally, and they were denied human or civil rights until the United States abolished slavery with the passage of the Thirteenth Amendment to the United States Constitution. Both state and federal laws denied, or rarely defined, rights for enslaved people.[1] [Slaves] are men, but they must not read the work of God; they have no right to any reward for their labor; no right to their wives; no right to their children; no right to themselves! The law makes them property and affords them no protection, and what are the Christian people of this country doing about it? Nothing at all! —Francis William Kellogg[2] Slave codes, federal and state laws that controlled African Americans' legal status and condition, started with legislation in 1705. They were treated like other forms of property, like farm equipment, cows, and horses. Enslaved people were prohibited from entering civil contracts and could not legally own or receive real or personal property. Their enslavers legally owned anything an enslaved person possessed. They were denied civil and political rights and the ability to plan their own time and movement. After a number of slave rebellions it was made illegal to teach enslaved people to read and write.[1] The Supreme Court of the United States supported the principle that enslaved people were not entitled to constitutional protection because, as chattel property, they were not citizens of the United States in the case of Dred Scott v. Sandford in 1856.[2] In the Northern United States, some states legalized marriages between enslaved people. In New York, bondsmen and women were allowed to marry, and their children were legitimate with the passage of the Act of February 17, 1809. Tennessee was the only slave state that allowed for marriage among enslaved people with the owner's consent. Instead of being chattel, Tennessee recognized the personhood of enslaved people, who had a legal status of being ""agent[s] of their owners"".[3] Enslaved men and women entered into relationships with one another based on the knowledge that meaningful relationships were important to their survival. Initially, enslaved people formed relationships according to the customs of West Africa.[4] There was an expectation of love, affection, and loyalty.[5] ""Marriage"" between enslaved people reflected a chosen emotional bond and a committed marital relationship. Being in a quasi-marital relationship affected one's status in the community and helped define the nature of the relationship among those in black and white communities.[5] One of the strongest arguments against slavery is that it made managing marital and family relationships challenging.[4] Unions involving an enslaved person or people were not legally binding.[6] Couples who were emancipated might have their marriage solemnized, which made their children legitimate.[7] Clerks were prohibited from issuing marriage licenses or recording marriages. In some places, ministers were prohibited from performing marriage ceremonies.[8] A long-term relationship with an enslaved person was often called a marriage, but it was a contubernium or quasi-marital relationship.[8] Unlike white couples, enslaved people did not have the protection of the law, the sanctity of the church, or the greater community's support to foment successful marriages. Because they were considered to be chattel, they had no legal standing. Their enslavers made decisions about their lives, which meant they did not have a sense of permanence when entering a committed, intimate relationship.[9] The church did not sanction quasi-marriages and thus was at odds with the teachings of the Christian church regarding the roles of wives, husbands, and children.[10] The longer an enslaved couple and their children were together, the more likely enslavers would separate them. This was particularly the case after the Act Prohibiting Importation of Slaves went into effect on January 1, 1808, coupled with the cotton economy that drove the acquisition of enslaved people in the Deep South (at about the same time that tobacco farms in the Upper South transitioned to an economy based upon crops like wheat and corn that required fewer enslaved workers).[11] Interstate slave trade increased to meet the varying needs of planter's crops. For instance, sugar plantations primarily operated with enslaved males because the work was so strenuous. The domestic slave trade disrupted one-third of first marriages by separating partners. Relationships also suffered when family members were hired-out to other enslavers for the long term.[12] Within African American communities, couples who entered into unions were considered married.[13] Marriages could be established as simply getting enslavers' permission and sharing a cabin.[14] If they shared vows, the wording had to be modified. The vow, ""To have and to hold, in sickness and in health... til death do you part"", was revised to reflect enslavers' legal right to separate them. For instance, ""til death do us part"" was revised to ""til death or buckra part you"", the term ""buckra"" meaning ""the white man"", or ""til death or distance do you part"".[15] Rarely, domestic servants might have formal marriage ceremonies performed by a black plantation preacher or a white minister. After honoring the couple, a feast and dance might follow. Such events were rare in general, and when they occurred were more likely held for house servants.[14] Jumping the broom was a ceremony ritual conducted for an enslaved couple. The practices varied. In some cases, the broom was held about a foot off the ground, and each partner jumped backward over the broom. Another practice was to have two brooms; each partner jumped over a broom while holding hands. The ritual helped couples feel ""more married"".[14] Enslavers controlled quasi-marital unions and could decide the fate of husbands, wives, and children at any moment.[4] Enslavers decided whether families lived together, if they were sold away from one another, or if or when they could see one another.[13][14] If the couples lived on different plantations, they were said to have a ""broad"" or an ""abroad"" marriage.[14][16] Even though they committed to one another, they were not necessarily allowed to live together.[17] Depending upon the distance, they might visit one another on the weekend or stay with each other nightly. For enslavers, broad marriages could make the management of productivity and the control of resistance more challenging if they became increasingly independent.[14] Enslavers might encourage marriages between black men and women on their plantations. A wealthy owner might buy the spouse of a broad marriage so that they would live together on their estate.[14] Enslavers learned that it was in their best interest for their enslaved workers to be married and have families. It meant that enslaved people would be mollified and less likely to run away.[4] An enslaver could permit a couple to have a relationship, essentially providing approval to breed, which would increase the number of people they enslaved and make more money for the enslaver.[18] Enslaved women, whether married or not, were subject to rape by their owner, who benefited financially by fathering several children with greater control as the biological father.[19] Historian Eugene Genovese argues that enslavers understood the strength of enslaved peoples' marital and family ties: ""Evidence of the slaveholders' awareness of the importance of family to the slaves may be found in almost any well-kept set of plantation records. Masters and overseers normally listed their slaves by households and shaped disciplinary procedures to take full account of family relationships. The sale of a recalcitrant slave might be delayed or avoided because it would cause resentment among his family of normally good workers. Conversely, a slave might be sold as the only way to break his influence over valuable relatives.""[20] Some men and women lived with their children in nuclear families. In most cases, enslaved fathers did not live with their families. In many ways, enslaved couples assumed typically female and male roles within the relationships, except that since their children and wife were subject to enslavers' whims, men had less control in the care of their family than free men with free family members.[21] In the 19th century, Alexis de Tocqueville found there was a ""profound and natural antipathy between the institution of marriage and that of slavery"" because a man could not be an authority figure to his wife and children. He could not control their fate, what work they performed, or their privileges.[22] Enslaved men hunted, fished, and raised crops, poultry, and livestock to feed their families. They might also perform ""overwork"" tasks that provided their families with a better standard of living.[23] According to the law (partus sequitur ventrem), children born to an enslaved woman were the property of her enslaver.[14] In many jurisdictions, once enslaved people in long-term relationships were emancipated or manumitted, their marriages were recorded, and their children were deemed legitimate.[24] She would take me upon her knee and, pointing to the forest trees which were then being stripped of their foliage by the winds of autumn, would say to me, my son, as yonder leaves are stripped from off the trees of the forest, so are the children of the slaves swept away from them by the hands of cruel tyrants; and her voice would tremble with deep emotion, while the tears would find their way down her saddened cheeks. On those occasions she fondly pressed me to her heaving bosom, as if to save me from so dreaded a calamity, or to feast on the enjoyments of maternal feeling while se yet retained possession of her child. When he was 15 years of age, Henry Box Brown was separated from his mother, father, brothers, and sisters upon his enslaver's death. He was sent to work at a tobacco factory in Richmond, Virginia owned by the son of his former owner. Brown had believed that he was to be freed upon his enslaver's death.[26] The Thirteenth Amendment emancipated enslaved people, who were thus no longer considered chattel. The Civil Rights Act of 1866 defined the rights of free people to own, sell, or lease personal and real property, enter into contracts, and be entitled to fundamental human rights. They could also marry.[2] After the Civil War, states defined how to evaluate whether long-term couples were married and what rights they had as married couples within their jurisdiction.[27] After the end of the Civil War, freed men and women searched for family members that enslavers had separated from them. In their search, they walked long distances and contacted many agencies.[4][28] They followed the routes of former slave traders, contacted churches, and reached out to Freedmen's Bureau to locate their spouses that they might not have seen for years.[28] One of the fundamental rights that freed men and women chose to exercise was the right to marry, resulting in a plethora of African American marriages soon after the end of the war. There were a few, though, that felt that they were being forced to marry by missionaries or were concerned about obligations that they might be unknowingly taking on.[4] President Andrew Johnson hired Major General Oliver Otis Howard on May 30, 1865, to be the commissioner of the Freeman's Bureau, to aid in the Reconstruction of the District of Columbia, Confederate states and free states that bordered slave states. They worked in camps established by the military for formerly enslaved people. Associate commissioners were responsible for hiring officers to record former slave marriages. Ordained ministers provided records of marriages that they had performed and solemnized other marriages.[14] The Bureau recorded marriages and preserved marital records of former enslaved men and women in registers, certificates, marriage licenses, and other records. States and other organizations also formalized long-term relationships.[14] Ironically, blacks were prevented from obtaining legal marriages while enslaved, but they were ""disproportionately punished"" if they lived together without being married once they were free. Marriage had become a moral and legal requirement for blacks in American society.[29] Unaware of legal ramifications, some African Americans who had been in quasi-marriages were prosecuted for choosing a different partner to legally marry once they were free.[30] Ellen and William Craft were both born into slavery and were separated from their parents at a young age. They cared for one another, but they would not enter into a marriage that would mean that she would bear a child who would be born into slavery. They hatched a plan to seek freedom. Ellen had a fair complexion, like her father, so she dressed up as a male enslaver who traveled with William, who played the role of her slave. They successfully fled slavery and lived as man and wife.[31] Charlotte and Dick Green were integral to the successful operation of Bent's Fort on the Santa Fe Trail. Still, they remained enslaved until Dick was rewarded for his participation in the military party sent out to avenge the death of Governor Charles Bent of the territory of New Mexico. The Greens were freed by William Bent, brother of Charles, in 1847. They then headed east back to Missouri.[32] Emeline and Samuel Hawkins, an enslaved woman and a freed man and sharecropper, considered themselves a married couple. They lived together with their children. Their two eldest children were sold away from the family in 1839. Samuel had tried unsuccessfully to purchase the freedom of his wife. Four more children were threatened with being sold away from the family. With Samuel Burris, Samuel Hawkins planned his family's escape. The family made it to Byberry Township, Pennsylvania, where they changed their last name to Hackett. They reunited with their eldest sons, Chester and Samuel, who were apprentices in the area.[33] Sarah Ann and Benjamin Manson were an enslaved couple from Wilson County, Tennessee. They lived as man and wife since 1843 and had sixteen children. They were legally married on April 19, 1866, and received a marriage certificate from the Freedmen's Bureau. The certificate symbolized their right to live together as a family.[14][34] Since the mid-1970s, some historians—like Herbert Gutman, John Blassingame, Jacqueline Jones, Ann Malone, and Eugene Genovese—have contended that most slave children grew up in homes with both parents. Newer scholarship and review of enslaved person and census records in Loudoun County, Virginia has shown a greatly diminished role of the husband and father in enslaved families, who were unable to be the leader for the family, according to Brenda E. Stephenson. Often, mothers headed the family on plantations and had ""abroad"" spouses who lived on other plantations. Consequently, an enslaved man might have intimate relationships with more than one woman.[23] According to Herbert Gutman, a slave register from a South Carolina plantation over almost 100 years shows that there were long-standing marriages between enslaved men and women. He found examples of long-term marriages in other states, like Virginia, northern Louisiana, North Carolina, and Alabama. [35] Enslavers further separated families by trading them to the Deep South and Southwest in the years preceding the Civil War. When enslavers split families apart, historical records showed a significant effort among Blacks to reunite families. It remains an open question among historians regarding the extent to which the interstate slave trade destroyed slave families.[23]"
Bill May (synchronized swimmer),/wiki/Bill_May_(synchronized_swimmer)," William May (born January 17, 1979)[1][2] is an American synchronized swimmer. Performing primarily in duets, May won several national and international events. Because of his sex, May was not allowed to compete in the 2004 Summer Olympics. May was born in Syracuse, New York.[1] He became interested in synchronized swimming when he was 10 years old while living in Syracuse, after watching his sister in a beginning class. He explained why he became involved, ""I did competitive swimming there and the synchro class was right after it, so we couldn't go home until she was done. It was either try it with her or sit outside the pool and watch her, so my mom told me to try it, just to be in the water and be doing something.""[3] May began taking lessons and later performed with a local team, the Syracuse Synchro Cats. After the Synchro Cats disbanded, he performed with the Oswego Lakettes.[4][5] In 1996, at age 16, May moved to Santa Clara, California. He tried out for the Santa Clara Aquamaids, one of the top synchronized swimming programs in the United States, and was accepted into the junior A squad. May eventually was promoted to the Aquamaid's top ""A"" squad.[3][4] Teaming with partner Kristina Lum, May won the duet event at the 1998 US national championships. The pair then won the silver medal in the event at the 1998 Goodwill Games. Because May is male, he was barred from competing in the 1999 Pan American Games.[3] The International Swimming Federation (FINA) allowed May to compete in its sanctioned events. In 1999, May finished first in duet at the Swiss Open and French Open. He won the Grand Slam at the 2000 Jantzen Nationals and was named the US Synchronized Swimming Athlete of the Year in 1998 and 1999.[3][6] May was not allowed to compete at the 2004 Summer Olympics because Synchronized swimming at the Summer Olympics consisted of two women's only events. In 2008, May performed in Cirque du Soleil's water-based show, O. He was paid $100 for each performance. In addition, he later showed Natalie Fletcher, an up-and-coming synchronized swimmer who would go on to win 13-15 nationals, around the set of Cirque du Soleil, an excursion organized by Tammy McGregor, former Olympian.[5][7] With the mixed duet added to the World Championship program, Bill May returned to the sport after a 10-year retirement.[8] On July 26, 2015, he became the first man ever to win Synchronized swimming gold at a major event, such as an Olympic or World Championships, by winning the mixed duet technical routine gold, with his partner Christina Jones at the World Aquatic Championships in Kazan, Russia. Aleksandr Maltsev from Russia would become the second man by defeating May and his partner Kristina Lum with his partner Darina Valitova in the mixed duet free routine on July 30, 2015. May and Lum would finish second and win the silver. Maltsev and Valitova had been a close second to May and Jones in the technical routine event. Lum had been May's duet partner in 1998, where they also made in winning the duet at the U.S. Synchronized Swimming Championships, and a silver at the 1998 Goodwill Games. In July 2023, at age 44, May became the first American man to win a world team medal, joining the U.S. team that claimed silver in the acrobatic routine. It was the first U.S. medal in artistic swimming since 2007. This is the first year that men were permitted to compete in artistic team events at the World Championships, taking place in Fukuoka, Japan. Men will also be able to compete at the Olympics in a team event for the first time next year in Paris.[9]"
The Memphis 13,/wiki/The_Memphis_13,"The Memphis 13 are the group of young children who integrated the schools of Memphis, Tennessee. On October 3, 1961, 13 African-American first grade students were enrolled in schools that were previously all white. The schools that the students attended were Bruce, Gordon, Rozelle, and Springdale elementary schools.[1] The students attended the following schools: Bruce Elementary (Dwania Kyles, Harry Williams, Michael Willis); Gordon Elementary (Alvin Freeman, Sharon Malone, Sheila Malone, Pamela Mayes); Rozelle Elementary (Joyce Bell, E.C. Freeman, Leandrew Wiggins, Clarence Williams); Springdale Elementary (Deborah Ann Holt; Jacqueline Moore).[2] When these students desegregated Memphis City Schools, there was no violence like the violence witnessed in other parts of the South. There was neither a great deal of news coverage nor a great deal of public discussion about what was going on. Rev. Samuel Kyles was the chairman of the local NAACP's education committee at the time noted that the decision to use first-graders instead of high school students was intentional. Kyles believed that first graders were not tainted and therefore were better suited to integrate the schools.[1] The story of the Memphis 13 has been made into a documentary by Professor Daniel Kiel, a professor at the University of Memphis Cecil C. Humphreys School of Law. The documentary is 35 minutes and consists of interviews with the 13 students and their family members about their experiences and feelings during the time.[3][4] In October 2015, historical markers were placed at the four schools students attended.[2] The students have been honored as part of the City of Memphis ""Be the Dream"" awards program at Mason Temple in January 2016[5] and were honored at screenings of the documentary at Harvard Law School,[6] the Little Rock Reel Civil Rights Film Festival,[7] Thurgood Marshall School of Law, and the University of Mississippi School of Law.[8]"
Mormonism and polygamy,/wiki/Mormonism_and_polygamy," Polygamy (called plural marriage by Latter-day Saints in the 19th century or the Principle by modern fundamentalist practitioners of polygamy) was practiced by leaders of the Church of Jesus Christ of Latter-day Saints (LDS Church) for more than half of the 19th century, and practiced publicly from 1852 to 1890 by between 20 and 30 percent of Latter-day Saint families. The Latter-day Saints' practice of polygamy has been controversial, both within Western society and the LDS Church itself. The U.S. was horrified by the practice of polygamy, with the Republican platform at one time referencing ""the twin relics of barbarism—polygamy and slavery.""[1][2]: 438  The private practice of polygamy was instituted in the 1830s by founder Joseph Smith. The public practice of polygamy by the church was announced and defended in 1852 by a member of the Quorum of the Twelve Apostles, Orson Pratt,[3] at the request of church president Brigham Young. Throughout the 19th and early 20th centuries, the LDS Church and the United States were at odds over the issue: as the church defended the practice as a matter of religious freedom, while the federal government sought to eradicate it, consistent with prevailing public opinion. Polygamy was probably a significant factor in the Utah War of 1857 and 1858, given Republican attempts to paint Democratic president James Buchanan as weak in his opposition to both polygamy and slavery. In 1862, the United States Congress passed the Morrill Anti-Bigamy Act, which prohibited polygamous marriage in the territories.[3] In spite of the law, Latter-day Saints continued to practice polygamy, believing that it was protected by the First Amendment. In 1879, however, the Supreme Court of the United States upheld the constitutionality of the Morrill Act in Reynolds v. United States,[4] stating: ""Laws are made for the government of actions, and while they cannot interfere with mere religious belief and opinion, they may with practices.""[3] In 1890, when it became clear that Utah would not be admitted to the Union while polygamy was still practiced, church president Wilford Woodruff issued the Woodruff Manifesto, officially terminating the practice of polygamy within the LDS Church.[5] Although this Manifesto did not dissolve existing polygamous marriages, relations with the United States markedly improved after 1890, such that Utah was admitted as a U.S. state in 1896. After the Manifesto, some church members continued to enter into polygamous marriages, but these eventually stopped in 1904 when church president Joseph F. Smith disavowed polygamy before Congress and issued a ""Second Manifesto"", calling for all polygamous marriages in the church to cease, and established excommunication as the consequence for those who disobeyed. Several small ""fundamentalist"" groups, seeking to continue the practice, split from the LDS Church, including the Apostolic United Brethren (AUB) and the Fundamentalist Church of Jesus Christ of Latter-Day Saints (FLDS Church). Meanwhile, the LDS Church continues its policy of excommunicating members found practicing polygamy, and today actively seeks to distance itself from fundamentalist groups that continue the practice.[6] On its website, the church states that ""the standard doctrine of the church is monogamy"" and that polygamy was a temporary exception to the rule.[7][8] Today, various churches and groups from the Latter Day Saint movement continue to practice polygamy.[9] On July 12, 1843, a revelation that Smith said to have received from God was recorded (evidence points to Smith having received the revelation years earlier[citation needed]). The revelation allowed Smith and a few other male church leaders to have more than one wife.[10]: 53 [11] Van Wagoner claims that Smith developed an interest in polygamy after studying parts of the Old Testament in which prophets had more than one wife.[12]: 3  However, it is difficult to know when Smith decided to begin teaching or practicing polygamy.[12]: 3  Many early converts to the religion including Brigham Young, Orson Pratt, and Lyman Johnson, recorded that Joseph Smith was teaching polygamy privately as early as 1831 or 1832. Pratt reported that Smith told some early members in 1831 and 1832 that polygamy was a true principle, but that the time to practice it had not yet come.[13] At the time, the practice was kept secret from non-members and most church members. Throughout his life, Smith publicly denied having multiple wives.[14] During this time, the church publicly disavowed polygamy and only some church members knew about the teachings and practiced polygamy. Joseph Smith publicly condemned and denied his involvement in polygamy and participants were excommunicated for practicing polygamy without instruction and consent from leaders of the church.[15][16] However, church members who received permission began practicing polygamy in the 1840s.[17] The number of members aware of polygamy grew until the church started openly practicing polygamy in the early 1852, eight years after Smith's death.[12]: 4 [10]: 53–54  According to some historians and then-contemporary accounts, by this time, polygamy was openly taught and practiced.[10]: 185  The doctrine authorizing polygamy was canonized and first published in the 1876 version of the church's Doctrine and Covenants.[18] There were two types of LDS polygamous marriages: eternity-only and time-and-eternity. Eternity-only polygamous marriages applied only in the afterlife and time-and-eternity marriages applied both in mortal life and in the afterlife.[19] Most likely, Joseph Smith did not have sexual relations with all of his wives as some were eternity-only marriages.[20][21] Polygamy was taught as being essential for salvation.[10]: 186  Polygamy was seen as ""more important than baptism"" {reference?} and the practice of polygamy was required before the Second Coming of Christ. Brigham Young said that any male member of the church who was commanded to practice polygamy and refused would be damned.[22]: 112  Other leaders of the church taught that men who refused to have multiple wives were not obeying God's commandments and that they should step down from their priesthood callings.[22]: 112–113  Church President Joseph F. Smith also spoke about the necessity of practicing polygamy in order to receive salvation.[22]: 113  Members of the church in St George, Utah report being taught in the late 1800s that there is no ""exaltation"" without polygamy.[22]: 114  In a church-owned newspaper, an article speculates that men and women who refuse to practice polygamy will have a lesser station in the afterlife.[22]: 117  Polygamy was also explained as being a commandment of God that was received by divine revelation and that polygamy was a part of God's plan.[23]: 44  Latter-day Saints believed that a woman could secure her place in heaven by being sealed to a righteous man who held the priesthood. Some women embraced polygamy because of this teaching and their desire to receive divine blessings.[24]: 132  The salvation of women was understood to be dependent on their status as wives.[25]: 98  One reason given for the practice of polygamy is to increase the Mormon population by childbirth.[23]: 44  In the Millennial Star, a newspaper owned and operated by the church, an article teaches that monogamous marriages result in offspring that are physically and mentally lesser than offspring of polygamous marriages.[22]: 117 [10]: 187  An early church leader argued that polygamy has historically been the main form of marriage and that polygamy is the most moral form of marriage.[23]: 44  Polygamy was sometimes explained as a way to prevent men from falling into sexual temptation,[22]: 117  while monogamy was immoral and increased the likelihood of sexual temptation.[23]: 44  Some who practiced polygamy defended it as a religious practice that was taught in the Bible.[26][23]: 44  Top leaders used the examples of the polygamy of God the Father and Jesus Christ in defense of it and these teachings on God and Jesus' polygamy were widely accepted among Mormons by the late 1850s.[27][28][22]: 84  In 1853, Jedediah M. Grant—who later become a First Presidency member—stated that the top reason behind the persecution of Christ and his disciples was due to their practice of polygamy.[29][27] Two months later, apostle Orson Pratt taught in a church periodical that ""We have now clearly shown that God the Father had a plurality of wives,"" and that after her death, Mary (the mother of Jesus) may have become another eternal polygamous wife of God.[30] He also stated that Christ had multiple wives—Mary of Bethany, Martha, and Mary Magdalene—as further evidence in defense of polygamy.[31][27] In the next two years the apostle Orson Hyde also stated during two general conference addresses that Jesus practiced polygamy[32][27] and repeated this in an 1857 address.[33] In a teaching manual published by the church in 2015, the practice of polygamy is described as a ""test of faith"" that brought Mormons closer to God.[34] Other recent church documents point to an increase in children as being why Mormons believe God commanded them to practice polygamy. An article on the church's website states that early Mormons believed that they would receive blessings from God by obeying the commandment of polygamy.[35] Even among those who accept the views of conventional historians, there is disagreement as to the precise number of wives Smith had: Fawn M. Brodie lists 48,[36]D. Michael Quinn 46,[37] and George D. Smith 38.[38] One historian, Todd M. Compton, documented at least 33 marriages or sealings during Smith's lifetime.[39]Richard Lloyd Anderson and Scott H. Faulring came up with a list of 29 wives of Joseph Smith.[40] It is unclear how many of the wives Smith had sexual relations with. Some contemporary accounts from Smith's time indicate that he engaged in sexual relations with some of his wives.[39][41][42] As of 2007[update], there were at least twelve early Latter Day Saints who, based on historical documents and circumstantial evidence, had been identified as potential Smith offspring stemming from polygamous marriages. In 2005 and 2007 studies, a geneticist with the Sorenson Molecular Genealogy Foundation stated that they had shown ""with 99.9 percent accuracy"" that five of these individuals were in fact not Smith descendants: Mosiah Hancock (son of Clarissa Reed Hancock), Oliver Buell (son of Prescindia Huntington Buell), Moroni Llewellyn Pratt (son of Mary Ann Frost Pratt), Zebulon Jacobs (son of Zina Diantha Huntington Jacobs Smith), and Orrison Smith (son of Fanny Alger).[43] The remaining seven have yet to be conclusively tested, including Josephine Lyon, for whom current DNA testing using mitochondrial DNA cannot provide conclusive evidence either way. Lyon's mother, Sylvia Sessions Lyon, left her daughter a deathbed affidavit telling her she was Smith's daughter.[43] LDS Church president Brigham Young had 51 wives, and 56 children by 16 of those wives.[44] LDS Church apostle Heber C. Kimball had 43 wives, and had 65 children by 17 of those wives.[45] Mormons responded to polygamy with mixed emotions. One historian notes that Mormon women often struggled with the practice and a belief in the divinity of the polygamy commandment was often necessary in accepting it. Records indicate that future church leaders, such as Brigham Young, John Taylor, and Heber C. Kimball, greatly opposed polygamy initially.[46]: 98  Documents left by Mormon women describe personal spiritual experiences that led them to accept polygamy.[22]: 160–161  Another historian notes that some Mormon women expressed appreciation for polygamy and its effects.[24]: 382  An early leader of the church, Orson Pratt, defended polygamy by arguing that the practice was a result of divine revelation and that it was protected under the US Constitution as a religious freedom. Following the public announcement of polygamy, members of the church published pamphlets and literature defending the practice. Mormon missionaries were also directed to defend polygamy.[23]: 44  The majority of Americans who were not members of the church were opposed to polygamy as they saw the practice as a violation of American values and morals.[10]: 192 [47]: 86 [24]: 382  Opponents of polygamy believed that polygamy forced wives into submission to their husbands[48]: 454  and some described polygamy as a form of slavery.[47]: 117  The overall opposition to polygamy led the Republican Party's platform to refer to it as one of the ""relics of barbarianism"".[49] Sensational and often violent novels provided fictional stories about polygamy which fueled the public's dislike for the practice and Mormons.[12]: 39–50  However, some non-Mormons held more positive views of polygamy. For example, after surveying the Utah Territory, Captain Howard Stansbury concluded that most polygamous marriages were successful and there were good feelings between families.[22]: 191  John C Bennett was a former member of the church and close friend of Joseph Smith who was disfellowshipped and later excommunicated for adultery. Following his excommunication, Bennett began to travel around the eastern United States as he lectured about the church. In his lectures, Bennett included claims of sexual misconduct among church leaders, secret rituals, and violence.[24]: 73–74  In 1842, Bennett published a book entitled The History of the Saints: Or, An Exposé of Joe Smith and Mormonism which includes alleged stories of sexual misconduct by Joseph Smith and other church leaders.[50] The church responded to Bennett's claims about Smith by gather affidavits and printing contradictory evidence in newspapers. The women of the Relief Society, encouraged by President Emma Smith, also wrote their experiences that disproved Bennett's statements. They also began a petition in support of Joseph Smith's character which they delivered to the Governor of Illinois.[24]: 74–75  Mormon polygamy was one of the leading moral issues of the 19th Century in the United States, perhaps second only to slavery in importance. Spurred by popular indignation, the U.S. government took a number of steps against polygamy; these were of varying effectiveness.[51][52] Anti-polygamy laws began to be passed ten years after the church publicly announced the practice of polygamy.[10]: 191  The first legislative attempt to discourage polygamy in Utah was presented in the 33rd Congress. The bill was debated in May 1854. The bill included the provision that any man who had more than one wife would not be able to own land in the Utah Territory. This bill was defeated in the House of Representatives after multiple representatives argued that the federal government did not have the authority to legislate morals in the states.[10]: 194–195  As the church settled in what became the Utah Territory, it eventually was subjected to the power and opinion of the United States. Friction first began to show in the James Buchanan administration and federal troops arrived (see Utah War). Buchanan, anticipating Mormon opposition to a newly appointed territorial governor to replace Brigham Young, dispatched 2,500 federal troops to Utah to seat the new governor, thus setting in motion a series of misunderstandings in which the Mormons felt threatened.[53] In 1862, the Morrill Anti-Bigamy Act became law. The Act criminalized the practice of polygamy, unincorporated the church, and limited the church's real estate holdings. The Act was largely understood to be unconstitutional and was only enforced in rare cases.[54]: 422  While, the Act outlawed bigamy in the US territories, it was seen to be largely weak and infective at preventing people from practicing polygamy.[55]: 447–449 [22]: 243–244  However, due to the continuous threat of legislation targeting polygamy and the church, Brigham Young pretended to comply.[54]: 422  On January 6, 1879, the Supreme Court upheld the Morrill Anti-Bigamy Act in Reynolds v. United States.[56]: 93  The Wade, Cragin, and Cullom Bills were anti-bigamy legislation that failed to pass in the US Congress. The bills were all intended to enforce the Morrill Act's prohibition on polygamy with more punitive measures.[57] The Wade Bill of 1866 had the power to dismantle local government in Utah.[58] Three years after the Wade Bill failed, the Cragin Bill, which would have eliminated the right to a jury for bigamy trials, was introduced but not passed.[59] After that, the Cullom Bill was introduced. One of the most concerning parts of the Cullom Bill for polygamists was that, if passed, anyone who practiced any type of non-monogamous relationship would not be able to become a citizen of the United States, vote in elections, or receive the benefits of the homestead laws. The leadership of the church publicly opposed the Cullom Bill. Op-eds in church-owned newspapers declared the bill as unjust and dangerous to Mormons.[60] The introduction of the Cullom Bill led to protests by Mormons, particularly Mormon women. Women organized indignation meetings to voice their disapproval of the bill.[24]: xii  The strong reaction of Mormon women surprised many onlookers and politicians. Outside of the church, Mormon women were seen as weak and oppressed by their husbands and the men of the church. The political activism in support of polygamy of Mormon women was unexpected from a group that had been portrayed as powerless.[61][24]: xii–xvi  Following the failure of the Wade, Cragin, and Collum Bills, the Poland Act was an anti-bigamy prosecution act that was successfully enacted by the 43rd United States Congress. The Poland Act, named after its sponsor in the US House of Representatives, attempted to prosecute Utah under the Morrill Anti-Bigamy act for refusing to stop practicing polygamy. The act stripped away some of Utah's powers and gave the federal government greater control over the territory. Among other powers, the act gave US district courts jurisdiction in the Utah Territory for all court cases[62] The Poland Act was a significant threat to Mormons practicing polygamy as it allowed for men who had multiple wives to be criminally indicted.[63] In February 1882, George Q. Cannon, a prominent leader in the church, was denied a non-voting seat in the U.S. House of Representatives due to his polygamous relations. This revived the issue of polygamy in national politics. One month later, the Edmunds Act was passed by Congress, amending the Morrill Act and made polygamy a felony punishable by a $500 fine and five years in prison. ""Unlawful cohabitation"", in which the prosecution did not need to prove that a marriage ceremony had taken place (only that a couple had lived together), was a misdemeanor punishable by a $300 fine and six months imprisonment.[3] It also revoked the right of polygamists to vote or hold office and allowed them to be punished without due process. Even if people did not practice polygamy, they would have their rights revoked if they confessed a belief in it. In August, Rudger Clawson was imprisoned for continuing to cohabit with wives that he married before the 1862 Morrill Act. In 1887, the Edmunds–Tucker Act allowed the disincorporation of the LDS Church and the seizure of church property; it also further extended the punishments of the Edmunds Act. In July of the same year, the U.S. Attorney General filed suit to seize all church assets.[citation needed] The church was losing control of the territorial government, and many members and leaders were being actively pursued as fugitives. Without being able to appear publicly, the leadership was left to navigate ""underground"".[citation needed] Following the passage of the Edmunds–Tucker Act, the church found it difficult to operate as a viable institution. After visiting priesthood leaders in many settlements, church president Wilford Woodruff left for San Francisco on September 3, 1890, to meet with prominent businessmen and politicians. He returned to Salt Lake City on September 21, determined to obtain divine confirmation to pursue a course that seemed to be agonizingly more and more clear. As he explained to church members a year later, the choice was between, on the one hand, continuing to practice polygamy and thereby losing the temples, ""stopping all the ordinances therein,"" and, on the other, ceasing to practice polygamy in order to continue performing the essential ordinances for the living and the dead. Woodruff hastened to add that he had acted only as the Lord directed.[citation needed] In 1879, the Supreme Court ruled that a defendant cannot claim a religious obligation as a valid defense to a crime and upheld the Morrill Anti-Bigamy Act in Reynolds v. United States.[56]: 93 [64] The Court said that while holding a religious belief was protected under the First Amendment right of freedom of religion, practicing a religious belief that broke the law was not.[65]Reynolds vs. United States was the Supreme Court's first case in which a party used the right of freedom of religion as a defense. The ruling concluded that Mormons could be charged with committing bigamy despite their religious beliefs.[66]: 587  The final element in Woodruff's revelatory experience came on the evening of September 23, 1890. The following morning, he reported to some of the general authorities that he had struggled throughout the night with the Lord regarding the path that should be pursued. The result was a 510-word handwritten manuscript which stated his intentions to comply with the law and denied that the church continued to solemnize or condone polygamous marriages. The document was later edited by George Q. Cannon of the First Presidency and others to its present 356 words. On October 6, 1890, it was presented to the Latter-day Saints at the General Conference and unanimously approved.[citation needed] While many church leaders in 1890 regarded the Manifesto as inspired, there were differences among them about its scope and permanence. Contemporary opinions include the contention that the manifesto was more related to an effort to achieve statehood for the Utah territory.[67] Some leaders were reluctant to terminate a long-standing practice that was regarded as divinely mandated. As a result, over 200 polygamous marriages were performed between 1890 and 1904.[68] It was not until 1904, under the leadership of church president Joseph F. Smith, that the church completely banned new polygamous marriages worldwide.[69] Not surprisingly, rumors persisted of marriages performed after the 1890 Manifesto, and beginning in January 1904, testimony given in the Smoot hearings made it clear that polygamy had not been completely extinguished.[citation needed] The ambiguity was ended in the General Conference of April 1904, when Smith issued the ""Second Manifesto"", an emphatic declaration that prohibited new polygamous marriages and proclaimed that offenders would be subject to church discipline.[citation needed] It declared that any who participated in additional plural marriages, and those officiating, would be excommunicated from the church. Those disagreeing with the Second Manifesto included apostles Matthias F. Cowley and John W. Taylor, who both resigned from the Quorum of the Twelve. Cowley retained his membership in the church, but Taylor was later excommunicated.[citation needed] Although the Second Manifesto ended the official practice of new polygamous marriages, existing ones were not automatically dissolved. Many Mormons, including prominent church leaders, maintained their polygamy into the 1940s and 1950s.[70] In 1943, the First Presidency learned that apostle Richard R. Lyman was cohabitating with a woman other than his legal wife. As it turned out, in 1925 Lyman had begun a relationship which he defined as a polygamous marriage. Unable to trust anyone else to officiate, Lyman and the woman exchanged vows secretly. By 1943, both were in their seventies. Lyman was excommunicated on November 12, 1943. The Quorum of the Twelve provided the newspapers with a one-sentence announcement, stating that the ground for excommunication was violation of the law of chastity.[citation needed] Over time, many of those who rejected the LDS Church's relinquishment of polygamy formed small, close-knit communities in areas of the Rocky Mountains. These groups continue to practice ""the Principle"". In the 1940s, LDS Church apostle Mark E. Petersen coined the term ""Mormon fundamentalist"" to describe such people.[72] Fundamentalists either practice as individuals, as families, or as part of organized denominations. Today, the LDS Church objects to the use of the term ""Mormon fundamentalists"" and suggests using the term ""polygamist sects"" to avoid confusion about whether the main body of Mormon believers teach or practice polygamy.[73]The Fundamentalist Church of Jesus Christ of Latter-Day Saints (also referred to as the FLDS Church) continues to practice polygamy.[74] Critics of polygamy in the early LDS Church claim that polygamy produced unhappiness in some wives.[75] LDS historian Todd Compton, in his book In Sacred Loneliness, described various instances where some wives in polygamous marriages were unhappy with polygamy.[39] Critics of polygamy in the early LDS Church claim that church leaders established the practice of polygamy in order to further their immoral desires for sexual gratification with multiple sexual partners.[76] Critics point to the fact that church leaders practiced polygamy in secret from 1833 to 1852, despite a written church doctrine (Doctrine and Covenants 101, 1835 edition) renouncing polygamy and stating that only monogamous marriages were permitted.[77] Critics also cite several first-person accounts of early church leaders attempting to use the polygamy doctrine to enter into illicit relationships with women.[78][79] Critics also assert that Joseph Smith instituted polygamy in order to cover up an 1835 adulterous affair with a neighbor's daughter, Fanny Alger, by taking Alger as his second wife.[80] Compton dates this marriage to March or April 1833, well before Joseph was accused of an affair.[81] However, historian Lawrence Foster dismisses the marriage of Alger to Joseph Smith as ""debatable supposition"" rather than ""established fact"".[82] Critics of polygamy in the early LDS Church claim that church leaders sometimes used polygamy to take advantage of young girls for immoral purposes.[88] Historian George D. Smith studied 153 men who took multiple wives in the early years of the Latter Day Saint movement, and found that two of the girls were thirteen years old, 13 girls were fourteen years old, 21 were fifteen years old, and 53 were sixteen years old.[89] Historian Todd Compton documented that Joseph Smith married one girl who was fourteen-years old (possibly two); according to Compton, ""it is unlikely that the marriage was consummated"".[90] Historian Stanley Hirshon documented cases of girls aged 10 and 11 being married to old men.[91] The mean age of marriage for women was lower in Mormon polygamy than in New England and the Northeastern states (the societies in which Smith and many early converts to the movement had lived). This was partly caused by the practice of polygamy, and Compton concludes that ""Early marriage and very early marriage were… accepted"" in early Mormonism. These marriages were frequently ""dynastic"" in purpose, meant to join people to the families of leaders, motivated by the significance of marriage for the nineteenth-century Latter-day Saint understanding of the afterlife. According to Compton, the ""valid parallel"" for Mormon early marriages is the ""American and European history of elite early marriages that were not consummated until the marriage participants were much older"". Compton ""find[s] dynastic marriages of teenage girls problematic, even if sexual consummation is delayed"".[92] If some men have several wives and the numbers of men and women are approximately equal, some men will necessarily be left without wives. In the sects that still practice polygamy today, such men, known as lost boys are often driven out so as not to compete with high-ranked polygamous men.[93] philosophy, sociology, psychology, and secularity 
 
 *^  Membership worldwide; generally church-reported; with an occasional exception
 †^ Once larger"
The New Jim Crow,/wiki/The_New_Jim_Crow,"The New Jim Crow: Mass Incarceration in the Age of Colorblindness is a book by Michelle Alexander, a civil rights litigator and legal scholar. The book discusses race-related issues specific to African-American males and mass incarceration in the United States, but Alexander noted that the discrimination faced by African-American males is prevalent among other minorities and socio-economically disadvantaged populations. Alexander's central premise, from which the book derives its title, is that ""mass incarceration is, metaphorically, the New Jim Crow"".[1] Though the conventional point of view holds that systemic racial discrimination mostly ended with the civil rights movement reforms of the 1960s, Alexander posits that the U.S. criminal justice system uses the War on Drugs as a primary tool for enforcing traditional, as well as new modes of discrimination and oppression.[2] These new modes of racism have led to not only the highest rate of incarceration in the world, but also a disproportionately large rate of imprisonment for African American men. Were present trends to continue, Alexander writes, the United States would imprison one third of its African American population. When combined with the fact that whites are more likely to commit drug crimes than people of color, the issue becomes clear for Alexander: ""the primary targets of [the penal system's] control can be defined largely by race"".[3] This ultimately leads Alexander to argue that mass incarceration is ""a stunningly comprehensive and well-disguised system of racialized social control that functions in a manner strikingly similar to Jim Crow"".[4] The culmination of this social control is what Alexander calls a ""racial caste system"", a type of stratification wherein people of color are kept in an inferior position. Its emergence, she believes, is a direct response to the civil rights movement. It is because of this that Alexander argues for issues with mass incarceration to be addressed as issues of racial justice and civil rights. To approach these matters as anything but would be to fortify this new racial caste. Thus, Alexander aims to mobilize the civil rights community to move the incarceration issue to the forefront of its agenda and to provide factual information, data, arguments and a point of reference for those interested in pursuing the issue. Her broader goal is the revamping of the prevailing mentality regarding human rights, equality and equal opportunities in America, to prevent future cyclical recurrence of what she sees as ""racial control under changing disguise"".[1] According to the author, what has been altered since the collapse of Jim Crow is not so much the basic structure of US society, as the language used to justify its affairs. She argues that when people of color are disproportionately labeled as ""criminals"", this allows the unleashing of a whole range of legal discrimination measures in employment, housing, education, public benefits, voting rights, jury duty, and so on.[5] Alexander explains that it took her years to become fully aware and convinced of the phenomena she describes, despite her professional civil rights background. She expects similar reluctance and disbelief on the part of many of her readers. She believes that the problems besetting African American communities are not merely a passive, collateral side effect of poverty, limited educational opportunity or other factors, but a consequence of purposeful government policies. Alexander has concluded that mass incarceration policies, which were swiftly developed and implemented, are a ""comprehensive and well-disguised system of racialized control that functions in a manner strikingly similar to Jim Crow"".[6] Alexander contends that in 1982 the Reagan administration began an escalation of the War on Drugs, purportedly as a response to a crack cocaine crisis in black ghettos, which was (she claims) announced well before crack cocaine arrived in most inner city neighborhoods. During the mid-1980s, as the use of crack cocaine increased to epidemic levels in these neighborhoods, federal drug authorities publicized the problem, using scare tactics to generate support for their already-declared escalation.[7] The government's successful media campaign made possible an unprecedented expansion of law enforcement activities in America's urban neighborhoods, and this aggressive approach fueled widespread belief in conspiracy theories that posited government plans to destroy the black population. (Black genocide)[citation needed] In 1998, the Central Intelligence Agency (CIA) acknowledged that during the 1980s the Contra faction—covertly supported by the US in Nicaragua—had been involved in smuggling cocaine into the US and distributing it in US cities. Drug Enforcement Administration efforts to expose these illegal activities were blocked by Reagan officials, which contributed to an explosion of crack cocaine consumption in America's urban neighborhoods. More aggressive enforcement of federal drug laws resulted in a dramatic increase in street level arrests for possession. Disparate sentencing policies (the crack cocaine v. powdered cocaine penalty disparity was 100-1 by weight and remains 18-1 even after recent reform efforts) meant that a disproportionate number of inner city residents were charged with felonies and sentenced to long prison terms, because they tended to purchase the more affordable crack version of cocaine, rather than the powdered version commonly consumed in the suburbs.[8][9] Alexander argues that the War on Drugs has a devastating impact on inner city African American communities, on a scale entirely out of proportion to the actual dimensions of criminal activity taking place within these communities. During the past three decades, the US prison population exploded from 300,000 to more than two million, with the majority of the increase due to drug convictions.[10] This led to the US having the world's highest incarceration rate. The US incarceration rate is eight times that of Germany, a comparatively developed large democracy.[11] Alexander claims that the US is unparalleled in the world in focusing enforcement of federal drug laws on racial and ethnic minorities. In the capital city of Washington, D.C., three out of four young African American males are expected to serve time in prison.[12] While studies show that quantitatively Americans of different races consume illegal drugs at similar rates,[13][verification needed] in some states black men have been sent to prison on drug charges at rates twenty to fifty times those of white men.[14] The proportion of African American men with some sort of criminal record approaches 80% in some major US cities, and they become marginalized, part of what Alexander calls ""a growing and permanent undercaste"".[15][16] Alexander maintains that this undercaste is hidden from view, invisible within a maze of rationalizations, with mass incarceration its most serious manifestation. Alexander borrows from the term ""racial caste"", as it is commonly used in scientific literature, to create ""undercaste"", denoting a ""stigmatized racial group locked into inferior position by law and custom"". By mass incarceration she refers to the web of laws, rules, policies and customs that make up the criminal justice system and which serve as a gateway to permanent marginalization in the undercaste. Once released from prison, new members of this undercaste face a ""hidden underworld of legalized discrimination and permanent social exclusion"".[17] According to Alexander, crime and punishment are poorly correlated, and the present US criminal justice system has effectively become a system of social control unparalleled in any other Western democracy, with its targets largely defined by race. The rate of incarceration in the US has soared, while its crime rates have generally remained similar to those of other Western countries, where incarceration rates have remained stable. The current rate of incarceration in the US is six to ten times greater than in other industrialized nations, and Alexander maintains that this disparity is not correlated to the fluctuation of crime rates, but can be traced mostly to the artificially invoked War on Drugs and its associated discriminatory policies.[18] The US embarked on an unprecedented expansion of its juvenile detention and prison systems.[19][20] Alexander notes that the civil rights community has been reluctant to get involved in this issue, concentrating primarily on protecting affirmative action gains, which mainly benefit an elite group of high-achieving African Americans. At the other end of the social spectrum are the young black men who are under active control of the criminal justice system (currently in prison, or on parole or probation)—approximately one-third of the young black men in the US. Criminal justice was not listed as a top priority of the Leadership Conference on Civil Rights in 2007 and 2008, or of the Congressional Black Caucus in 2009. The National Association for the Advancement of Colored People (NAACP) and the American Civil Liberties Union (ACLU) have been involved in legal action, and grassroots campaigns have been organized, however Alexander feels that generally there is a lack of appreciation of the enormity of the crisis. According to her, mass incarceration is ""the most damaging manifestation of the backlash against the Civil Rights Movement"", and those who feel that the election of Barack Obama represents the ultimate ""triumph over race"", and that race no longer matters, are dangerously misguided.[21] Alexander writes that Americans are ashamed of their racial history, and therefore avoid talking about race, or even class, so the terms used in her book may seem unfamiliar to many. Americans want to believe that everybody is capable of upward mobility, given enough effort on his or her part; this assumption forms a part of the national collective self-image. Alexander points out that a large percentage of African Americans are hindered by the discriminatory practices of an ostensibly colorblind criminal justice system, which end up creating an undercaste where upward mobility is severely constrained.[citation needed] Alexander believes that the existence of the New Jim Crow system is not disproved by the election of Barack Obama and other examples of exceptional achievement among African Americans, but on the contrary the New Jim Crow system depends on such exceptionalism. She contends that the system does not require overt racial hostility or bigotry on the part of another racial group or groups. Indifference is sufficient to support the system. Alexander argues that the system reflects an underlying racial ideology and will not be significantly disturbed by half-measures such as laws mandating shorter prison sentences. Like its predecessors, the new system of racial control has been largely immune from legal challenge. She writes that a human tragedy is unfolding, and The New Jim Crow is intended to stimulate a much-needed national discussion ""about the role of the criminal justice system in creating and perpetuating racial hierarchy in the United States"".[22] Alexander states in the book: ""I was careful to define ""mass incarceration"" to include those who were subject to state control outside of prison walls, as well as those who were locked in literal cages.""[23] The scope of Alexander's definition of ""incarceration"" includes people who have been arrested (but not tried), people on parole and people who have been released but labelled as ""criminals"". Alexander's definition is intentionally much broader than the subset of individuals currently in physical detention. Darryl Pinckney, writing in The New York Review of Books, called the book one that would ""touch the public and educate social commentators, policymakers, and politicians about a glaring wrong that we have been living with that we also somehow don't know how to face... [Alexander] is not the first to offer this bitter analysis, but NJC is striking in the intelligence of her ideas, her powers of summary, and the force of her writing"".[24] Jennifer Schuessler, writing in The New York Times, notes that Alexander presents voluminous evidence in the form of both statistics and legal cases to argue that the tough-on-crime policies begun under the Nixon administration and amplified under Reagan's war on drugs have devastated black America, where nearly one-third of black men are likely to spend time in prison during their lifetimes, and where many of these men will be second-class citizens afterwards. Schuessler also notes that Alexander's book goes further, by asserting that the increase in incarceration was a deliberate effort to roll back civil rights gains, rather than a true response to increased rates of violent crime. Schuessler notes that the book has galvanized both black and white readers, some of whom view the work as giving voice to deep feelings that the criminal justice system is stacked against blacks, while others might question its portrayal of anti-crime policies as primarily motivated by racial animus.[25] Forbes wrote that Alexander ""looks in detail at what economists usually miss"", and ""does a fine job of truth-telling, pointing the finger where it rightly should be pointed: at all of us, liberal and conservative, white and black"".[26] The book received a starred review in Publishers Weekly, saying that Alexander ""offers an acute analysis of the effect of mass incarceration upon former inmates"" who will be legally discriminated against for the rest of their lives, and described the book as ""carefully researched, deeply engaging, and thoroughly readable"".[27] James Forman Jr. argues that though the book has value in focusing scholars (and society as a whole) on the failures of the criminal justice system, it obscures African-American support for tougher crime laws and downplays the role of violent crime in the story of incarceration.[28] John Pfaff, in his book Locked In: The True Causes of Mass Incarceration and How to Achieve Real Reform, criticizes Alexander's assertion that the Drug War is responsible for mass incarceration. Among his findings are that drug offenders make up only a small part of the prison population, and non-violent drug offenders an even smaller portion; that people convicted of violent crimes make up the majority of prisoners; that county and state justice systems account for the large majority of American prisoners and not the federal system that handles most drug cases; and, subsequently, ""national"" statistics tell a distorted story when differences in enforcement, conviction, and sentencing are widely disparate between states and counties.[29] The Brookings Institution reconciles the differences between Alexander and Pfaff by explaining two ways to look at the prison population as it relates to drug crimes, concluding ""The picture is clear: Drug crimes have been the predominant reason for new admissions into state and federal prisons in recent decades"" and ""rolling back the war on drugs would not, as Pfaff and Urban Institute scholars maintain, totally solve the problem of mass incarceration, but it could help a great deal, by reducing exposure to prison.""[30] The 10th Anniversary Edition (2020) was discussed with Ellen DeGeneres on The Ellen Show on network TV, and reviewed on the front page of the New York Times Book Review section on January 19, 2020. The New Jim Crow was listed in The Chronicle of Higher Education as one of the 11 best scholarly books of the 2010s, chosen by Stefan M. Bradley.[31] a..mw-parser-output .citation{word-wrap:break-word}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}^ The persistently lingering result of the lack of land reform, of the fact that the former slaves were not granted any of the property on which they had long labored (unlike many European serfs, emancipated and economically empowered to various degrees by that time,[32] their American counterparts ended up with nothing), is the present extremely inequitable distribution of wealth in the United States along racial lines. 150 years after the Civil War, the median wealth of a black family is a small fraction of the median wealth of a white family.[33] b.^ According to Ruth W. Grant of Duke University, the author of the book Strings Attached: Untangling the Ethics of Incentives (Princeton University Press 2011, ISBN 978-0-691-15160-1), the expediency-based plea bargain process, in which 90 to 95% of felony prosecutions never go to trial, but are settled by the defendant pleading guilty, undermines the purpose and challenges the legitimacy of the justice system. Justice won't take place, because ""either the defendant is guilty, but gets off easy by copping a plea, or the defendant is innocent but pleads guilty to avoid the risk of greater punishment"". The question of guilt is decided without adjudicating the evidence-the fundamental process of determining the truth and assigning proportionate punishment does not take place.[34] c.^ Michelle Alexander suggested in a March 2012 New York Times article a possible strategy (she attributed the idea to Susan Burton) for coping with the unjust criminal justice system. If large numbers of the accused could be persuaded to opt out of plea bargaining and demand a full trial by jury, to which they are constitutionally entitled, the criminal justice system in its present form would be unable to continue because of lack of resources (it would ""crash""). This last resort strategy is controversial, as some would end up with extremely harsh sentences, but, it is argued, progress often cannot be made without sacrifice.[35]"
Oregon black exclusion laws,/wiki/Oregon_black_exclusion_laws,"The Oregon black exclusion laws were attempts to prevent black people from settling within the borders of the settlement and eventual U.S. state of Oregon. The first such law took effect in 1844, when the Provisional Government of Oregon voted to exclude black settlers from Oregon's borders. The law authorized a punishment for any black settler remaining in the territory to be whipped with ""not less than twenty nor more than thirty-nine stripes"" for every six months they remained.[1] Additional laws aimed at African Americans entering Oregon were ratified in 1849 and 1857.[2] The last of these laws was repealed in 1926.[3] The laws, born of pro-slavery and anti-black beliefs,[2][4] were often justified as a reaction to fears of black people instigating Native American uprisings.[5] Early white settlers in the Oregon Country often held both anti-slavery and anti-black beliefs, and many came from states, such as Missouri, which had some version of exclusion laws.[2][4] White settlers believed banning slavery would eliminate political controversy, but feared that settlements of freed slaves would compete for power with white people. In addition, they believed that allowing slavery could lead to the land in Oregon being taken over by large plantations as in the Southern states and force them to compete with bonded labor.[6] One early migrant wrote that Oregon pioneers ""hated slavery, but a much larger number of them hated free negroes worse even than slaves"".[4] In 1843, the Provisional Government of Oregon established a set of organic laws, including a ban on slavery: ""There shall be neither slavery nor involuntary servitude in the said territory otherwise than in the punishment of crimes whereof the party shall have been duly convicted.""[2][7] Enforcement of the law was left unclear.[8] After a vote on June 26, 1844, the first black exclusion law reiterated a ban on all slavery in Oregon territory, and it forced black and mulatto settlers to leave Oregon territory within three years (two years for men) or be whipped ""no more than 39 times"". That section was amended in December 1844 to permit a free slave to be resold on the condition that the slave owner agree to remove them from the territory at the end of the contract, which was held with the provisional government.[2] In effect, that re-established slavery on a temporary basis for three years.[7] The law was repealed in 1845 without any such punishment ever being carried out.[7] On September 21, 1849, the Oregon Territory established its second exclusion law,[5] declaring a ban on ""any negro or mulatto to enter into, or reside"" in Oregon unless already established there. At least four black people were punished under this law, including Jacob Vanderpool, a sailor, and three others who were eventually permitted to stay.[7] An 1850 census showed fewer than 50 black residents in the state of Oregon,[3] including a mixed-race man from Pennsylvania, George Bush, who was forced to move North of the Columbia River after the first exclusion Law was passed.[3]George Washington, another unrelated free man, was the founder of Centralia, Washington.[3] On April 16, 1852, Robin Holmes, a black slave of Nathaniel Ford, brought a case to the territorial Supreme Court, charging that he and his family were being held by Ford illegally. Holmes v. Ford was heard by four judges, culminating in Judge George Henry Williams' June 1853 ruling that slavery was illegal in Oregon. Descendants of Holmes have since stated that Ford had encouraged the lawsuit as a means to bring an end to slavery in the state.[3] On November 7, 1857, Oregon's delegates to the state Constitutional Convention submitted proposals to legalize slavery and to ban black people from the state, including a ban on signing contracts or owning land. The slavery amendment failed, but the exclusion law passed. Oregon was the only state admitted to the Union with such an exclusion law.[7] There are no records that this law was enforced, and the legislature voted down a proposed 1865 law that would authorize sheriffs to deport black residents in their counties.[7] The Cockstock incident was a major factor in the passage of the first black exclusion law.[5] It centered on a fight between a Wasco Native American man, Cockstock, and a free black man, James D. Saules, over ownership of a horse.[2] The argument escalated into a melee that killed three men, and led to rhetoric among white settlers that African Americans could create an uprising among local Native American tribes against settlers.[5] On June 24, 1844, within days of the Cockstock incident, the Oregon Provisional Legislature suspended its rules to allow Peter Burnett to propose a bill ""for the prevention of slavery"" without reference to a committee. The bill was read twice and voted into law the following day. The bill contained methods of enforcement for the prevention of slavery, which had already been banned in the territory.[2] These laws included a three-year limit on all free black people, and required freed slaves to leave the state within two years, if male, and three years, if female. The initial law proposed ""no more than 20"" lashings by whip for slaves found in violation of the law, which was amended in December 1844.[2] A week after the law's passage, Burnett wrote in a personal letter that the bill was intended to ""keep clear of that most troublesome class of population.""[2] Decades later, Burnett publicly described the exclusion law as intended to prevent disenfranchised black people from being exposed to politically empowered white people, which he wrote ""reminds them of their inferiority"", and suggested that their presence was ""injurious to the dominant class itself, as such a degraded and practically defenseless condition offers so many temptations to tyrannical abuse"".[2] In his History of Oregon, William Gray described the law as ""inhuman""; Burnett argued that Gray misrepresented the law.[9] The law had an unknown impact on black people in the state, and no records suggest it was ever directly enforced.[1] However, its threatened enforcement against African American settler George Bush led the Bush-Simmons party, which arrived shortly after the law's adoption, to cut a new spur of the Oregon Trail northward across the Columbia River into disputed, British-controlled territory, where they founded the first U.S. settlement on the Puget Sound in present-day Washington State.[10][11] Concern over the potential applicability of the exclusion law (which was understood to forbid black land ownership) to Washington lands prior to the creation of a separate Washington Territory later led Congress to pass a private law confirming Bush's title over his lands.[10][12][13] Though the official text of the original law has been lost, it was reprinted in several sources at the time. The law as described contained eight sections, and two amendments were added in December 1844.[2][1] The Oregon exclusion law prohibited free black men and women in the territory, though jurisdiction for the law was limited to the region south of the Columbia river.[3] Section 1: That slavery and involuntary servitude shall be forever prohibited in Oregon. Section 2: That in all cases where slaves shall have been, or shall hereafter be, brought into Oregon, the owners of such slaves respectively shall have the term of three years from the introduction of such slaves to remove them out of the country. Section 3: That if such owners of slaves shall neglect or refuse to remove such slaves from the country within the time specified in the preceding section, such slaves shall be free. Section 4: That when any free negro or mulatto shall have come to Oregon, he or she (as the case may be), if of the age of eighteen or upward, shall remove from and leave the country within the term of two years for males, and three for females, from the passage of this act, and that if any free negro or mulatto shall hereafter come to Oregon, if of the age aforesaid, he or she shall quite and leave the country within the term of two years for males and three for females, from his or her arrival in the county. Section 5: That if such free negro or mulatto be under the age aforesaid, the terms of time specified in the preceding section shall begin to run when he or she arrive at such age. Section 6: That if such free negro or mulatto shall fail to quit the country, as required by this act, he or she may be arrested upon a warrant issued by some justice of the peace, and, if guilty upon trial before such justice, shall receive upon his or her bare back not less than twenty nor more than thirty-nine stripes, to be inflicted by the constable of the proper county. Section 7: That if any free negro or mulatto shall fail to quit the country within the term of six months after receiving such stripes, he or she shall again receive the same punishment once in every six months, until he or she shall quit the country. Section 8: That when any slave shall obtain his or her freedom, the time specified in the fourth section shall begin to run from the time when such freedom shall be obtained. Be it enacted by the Legislative Committee of Oregon as follows: Section 1: That the sixth and seventh sections of said act are hereby repealed. Section 2: That if any such free negro or mulatto shall fail to quit and leave the country, as required by the act to which this is amendatory, he or she may be arrested upon a warrant issued by some justice of the peace, and if guilty upon trial before such justice ... the said justice shall issue his order to any officer competent to execute process, directing said officer to give ten days' public notice, by at least four written or printed advertisements, that he will publicly hire out such free negro or mulatto from the country for the shortest term of service, shall enter into a bond with good and sufficient security to Oregon, in a penalty of at least one thousand dollars, binding himself to remove said negro or mulatto out of the country within six months after such service shall expire,; which bond shall be filed in the clerk's office in the proper county, and upon failure to perform the conditions of said bond, the attourney prosecuting for Oregon shall commence a suit upon a certified copy of such bond in the circuit court against such delinquent and his sureties. In September 1849, the legislature passed another exclusion law, with a preamble arguing that ""it would be highly dangerous to allow free negroes and mulattos to reside in the Territory or to intermix with the Indians, instilling in their minds feelings of hostility against the white race"".[2] The 1849 law ordered any black people entering the territory to leave within 40 days.[5][4] It was applied in 1851 to Jacob Vanderpool, a West Indian who had migrated to Oregon City. A white resident of the city brought a case against Vanderpool, who was arrested and ordered to leave Oregon within 30 days.[5] The family of Abner Hunt Francis were ordered out of the state within 40 days, but were allowed to stay after a petition was signed by 225 citizens. The petition formed the basis of a failed coalition to amend the exclusion law to allow for good behavior bonds by black settlers. The Francis family moved to Canada in 1861.[5] A petition was also used in 1854 to prevent the deportation of Morris Thomas and Jane Snowden.[5] Oregon's congressional delegate, Samuel Thurston, while seeking to limit federal land grants to white people, described the law to congress: The negroes associate with the Indians and intermarry, and, if their free ingress is encouraged or allowed, there would a relationship spring up between them and the different tribes, and a mixed race would ensue inimical to the whites; and the Indians being led on by the negro who is better acquainted with the customs, language, and manners of the whites, than the Indian, these savages would become much more formidable than they otherwise would, and long and bloody wars would be the fruits of the commingling of the races.[4] The law was repealed in 1854.[4] In 1857, after Oregon voters had voted for statehood, they subsequently called for a constitutional convention. The emergent constitution contained 185 sections, 172 of which were taken from various other state constitutions, with the additions primarily being racial exclusion or finance related.[14] The document enshrined an exclusion law into Section 35 of the Bill of Rights within the Oregon State Constitution.[15] The article read as follows: No free negro or mulatto not residing in this state at the time of the adoption of this constitution, shall come, reside or be within this state or hold any real estate, or make any contracts, or maintain any suit therein; and the legislative assembly shall provide by penal laws for the removal by public officers of all such negroes and mulattoes, and for their effectual exclusion from the state, and for the punishment of persons who shall bring them into the state, or employ or harbor them.[15] John McBride, later a state senator, described the amendment: ""It was largely an expression against any mingling of the white with any of the other races, and upon a theory that as we had yet no considerable representation of other races in our midst, we should do nothing to encourage their introduction. We were building a new state on virgin ground; its people believed it should encourage only the best elements to come to us, and discourage others.""[16][17] The question of slavery itself was put to a popular vote, with the public voting against slavery (by a vote of 7,727 to 2,645) but in favor of racial exclusion policies (by a vote of 8,640 to 1,081).[8][4] The final constitution barred ""negroes, mulattos and Chinamen"" from voting or owning land in the state.[8] Oregon's racially discriminatory state constitutional amendment, Section 35, was legally invalidated after the Civil War by the ratification of the 14th Amendment to the federal Constitution in 1868. However, Section 35 remained formally on the books for another 58 years. In 1925, the Oregon legislature proposed the formal repeal of Section 35, adopted as House Joint Resolution 8 (1925). The measure was referred to Oregon voters as a 1926 ballot initiative which was approved with 62.5% in favor.[18][dead link][19] Measure 14 in 2002, approved by a vote of 71–29, removed references to the 1857 referendum from the constitution.[20] From 1850 to 1860, Oregon saw its black population increase by just 75, compared to an increase of 4,000 in neighboring California.[4] Oregon's black exclusion laws have been linked to a below-average black population – two percent – into the present day.[16][4] Historian Cheryl Brooks has argued that Oregon's small black population has made it difficult for Oregonians to recognize racial discrimination problems in the state.[4]"
Jordan Palmer (social activist),/wiki/Jordan_Palmer_(social_activist),"Jordan Palmer is an American social activist, Kentucky politician, civil rights activist, entrepreneur,[1] and the founder of the Kentucky Equality Federation. Palmer is from Hazard, Kentucky and is credited for having Kentucky Constitutional Amendment 1 (banning same-sex marriage), being struck down by a Kentucky judge, pushing the first hate crime convictions under the Matthew Shepard and James Byrd Jr. Hate Crimes Prevention Act, and holding the first equality protests against a sitting governor and members of the Kentucky House of Representatives.[2][3] Palmer was the first openly gay person to run for the Kentucky Senate. Palmer is the son of a Church of Christ minister.[4] In early 2006 Jordan founded Kentucky Equal Rights (later renamed Kentucky Equality Federation by a majority vote of its members) to advance the interests of gay, lesbian, bisexual, and transgender people in the Commonwealth of Kentucky. Palmer also served as on the Board of Directors and the Executive Vice President of Development of Marriage Equality USA based in San Francisco, California from 2007-2009. In February 2009 Palmer organized gay and lesbian couples going to county court clerks offices in Kentucky to apply for a same-sex marriage license only to be denied. Palmer told the media his intention was to raise awareness of Kentucky's ban on same-sex marriages.[5][6] The action was condemned by the Family Foundation of Kentucky, which Palmer dismissed as ""another example of how so-called family organizations are some of the most useless, money-hungry scams in the world with their bizarre and all-encompassing 'gay fetish.'""[7] Palmer later organized churches refusing to sign marriage licenses until gay marriage was recognized in Kentucky.[8] On September 10, 2013 the Kentucky Equality Federation sued the Commonwealth of Kentucky in Franklin Circuit Court claiming Kentucky's 2004 Constitutional Amendment banning same-sex marriage violated sections of the commonwealth's constitution.[9] Case # 13-CI-1074 was assigned by the Franklin County Court Clerk[10] (the location of the Kentucky State Capitol). The lawsuit was conceived by President Jordan Palmer,[11] written and signed by Vice President of Legal Jillian Hall, Esq. Palmer stated to the media that: Kentucky added a facially unconstitutional amendment to its constitution via a ballot initiative process. Thus, the attempt to abrogate constitutional sensibilities in favor of a ballot initiative, as was done for Section 233A of the Kentucky Constitution in 2004, is against the very notion of equal protection as guaranteed to each and all of Kentucky's population. This should be held as true as a matter of law by the Courts, regardless of the ballot's outcome.[12] Governor Steve Beshear's legal representatives initially responded by citing foreign laws, and that gay and lesbian people could not reproduce as a reason to quash the lawsuit.[13] On April 16, 2015, Kentucky Equality Federation v. Beshear also known as Kentucky Equality Federation v. Commonwealth of Kentucky was ruled on by Franklin County Circuit Court Judge Thomas D. Wingate. Judge Wingate sided with Kentucky Equality Federation against the Commonwealth and struck down Kentucky Constitutional Amendment banning same-sex marriages. Judge Wingate also struck down all laws passed by the Kentucky General Assembly. At the request of Governor Steve Beshear's legal representation, the Judge also placed a stay on the order pending a ruling from a Kentucky appellate court (such as the Kentucky Court of Appeals or Kentucky's court of last resort, the Kentucky Supreme Court) or the U.S. Supreme Court.[14] The lawsuit was a significant victory for the Kentucky Equality Federation and the same-sex marriage civil rights movement. Kentucky's statutory and constitutional bans on same-sex marriage void and unenforceable for violating Plaintiff and Plaintiff's Members Constitutional Rights"", ruled Judge Wingate.[15] Palmer served on the board of directors of Marriage Equality USA from 2006-2009. Palmer worked across the state of California to fight 2008 California Proposition 8. During and after the No Prop 8 Campaign, Palmer told change.org and Time Magazine that he was concerned that the organization didn’t utilize the grassroots community to its potential and recognizing the harm associated with a campaign run by political consultants without sufficient accountability or transparency to the larger community.[16][17] When Kevin Pennington was attacked in the Appalachian mountains, Palmer demanded the U.S. Department of Justice prosecute his assailants under the U.S. Matthew Shepherd and James Byrd Jr. Hate Crimes Prevention Act.[18] Palmer succeeded and was active in the preparation of the trail.[19] David Jason Jenkins, of Cumberland, and Anthony Ray Jenkins, of Partridge, was indicted and convicted in U.S. District Court in London, KY.[20][21] After the trial, the U.S. Attorney's Office for the Eastern District of Kentucky promised to step-up hate crime conviction, a move hailed as progress by Palmer.[22] Palmer stepped down in 2012 from the Kentucky Equality Federation.[23] In 2014, he ran running for Kentucky State Senate, being the first openly gay person in Kentucky to do so.[24] Palmer lost to the incumbent and returned to lead the Kentucky Equality Federation and its member organizations. On July 14, 2011, Matthew Vanderpool, the first openly gay person to run for the Kentucky House of Representatives, and Palmer's roommate, committed suicide. Palmer was also the campain manager for Vanderpool.[25] Palmer continues to be an advocate for equality across Kentucky and the United States.[26] On July 09, 2021, Palmer threatened to sue the Leslie County Board of Education in Southeastern Kentucky for not allowing a lesbian couple to attend prom together and painting over a pride flag.[27]"
Pornography in the United States,/wiki/Pornography_in_the_United_States," Pornography has existed since the origins of the United States, and has become more readily accessible in the 21st century. Advanced by technological development, it has gone from a hard-to-find ""back alley"" item, beginning in 1969 with Blue Movie by Andy Warhol, the Golden Age of Porn (1969–1984) and home video, to being more available in the country and later, starting in the 1990s, readily accessible to nearly anyone with a computer or other device connected to the Internet. The U.S. has no current plans to block explicit content from children and adolescents, as many other countries have planned or proceeded to do. Attempts made to suppress it include: outright bans, prohibitions of its sale, censorship or rating schemes that restrict audience numbers, and claims that it is prostitution and thereby subject to regulations governing prostitution. Legal decisions affecting production and consumption of pornography include those relating to its definition, its relationship with prostitution, the definition of obscenity, rulings about personal possession of pornography, and its standing in relation to freedom of expression rights. American advocates for pornography often cite the First Amendment to the United States Constitution, which guarantees freedom of speech; however, under the Miller test established by Miller v. California, anything lacking ""serious literary, artistic, political, or scientific value"" is generally not protected. However, the Supreme Court of the United States held in Ashcroft v. Free Speech Coalition (2002) that child pornography produced without the use of minors is protected by the First Amendment, even if the models appear to be minors but are, in fact, of lawful age, or if the pornography is computer-generated.[1] Several studies have found that the United States has been the largest producer of pornogaphy.[2][3][4] Although pornography dates back thousands of years, its existence in the U.S. can be traced to its 18th-century origins and the influx of foreign trade and immigrants. By the end of the 18th century, France had become the leading country regarding the spread of porn pictures.[5] Porn had become the subject of playing-cards, posters, post cards, and cabinet cards. Prior to this printers were previously limited to engravings, woodcuts, and line cuts for illustrations.[6] As trade increased and more people immigrated from countries with less Puritanical and more relaxed attitudes toward human sexuality, the amount of available visual pornography increased. In 1880, halftone printing was used to reproduce photographs inexpensively for the first time.[7] The invention of halftone printing took pornography and erotica in new directions at the beginning of the 20th century. The new printing processes allowed photographic images to be reproduced easily in black and white. The first porn daguerreotype appeared in 1850 and with the advent of ""moving pictures"" by the Lumière brothers the first porn film was made soon after the public exhibition of their creation. Pornographic film production commenced almost immediately after the invention of the motion picture in 1895. Two of the earliest pioneers were Eugène Pirou and Albert Kirchner. Kirchner directed the earliest surviving pornographic film for Pirou under the trade name ""Léar"". The 1896 film, Le Coucher de la Marie showed Louise Willy performing a striptease. Pirou's film inspired a genre of risqué French films showing women disrobing and other filmmakers realized profits could be made from such films.[8][9] In the United States, one of the Thomas Edison's first efforts using his methods and equipment for making moving pictures was of a nude woman getting up from her bath tub and running away.[10] In the 20th century, the era of ""blue movies"" began with the silent films of the 1920s and continued throughout the post-war era as film technology improved and equipment costs were reduced to a consumer affordable level. Particularly with the introduction of the 8mm and super-8 film gauges, popular for the home movie market. Until the advent of electronic and digital video technology, the mass production of pornographic films was tied directly to the mainstream film industry.[11] Beginning in 1969 with Blue Movie by Andy Warhol, the subsequent Golden Age of Porn and more permissive legislation, a rise of adult theaters in the United States, and many other countries, developed. There was also a proliferation of coin-operated ""movie booths"" in sex shops that displayed pornographic ""loops"" (so called because they projected a movie from film arranged in a continuous loop).[11] By 1982, pornographic film production had switched to the cheaper and more convenient medium of video tape. Many film directors were hesitant to switch because of the different image quality that video tape produced. Those who did make the change benefited from greater profits since consumers preferred the new format. This change moved the films out of the theaters and into people's private homes. This was the end of the age of big budget productions and the beginning of the mainstreaming of pornography. It soon went back to its earthy roots and expanded to cover every fetish possible since video production was inexpensive. Instead of hundreds of pornographic films being made each year, thousands of videos were including compilations of just the sex scenes from various titles.[12][13] In the late 1990s, pornographic films were distributed on DVD. These offered better quality picture and sound than the previous video format and allowed innovations such as ""interactive"" videos that let users choose such variables as multiple camera angles, multiple endings and computer-only DVD content. The introduction and widespread availability of the Internet further changed the way pornography was distributed. Previously videos would be rented or purchased through mail-order, but with the Internet people could watch pornographic movies on their computers, and instead of waiting weeks for an order to arrive, a movie could be downloaded within minutes (or, later, within a few seconds). As of the 2000s, there were hundreds of adult film companies, releasing tens of thousands of productions, recorded directly on video, with minimal sets. Of late, web-cams and web-cam recordings are again expanding the market. Thousands of pornographic actors work in front of the camera to satisfy pornography consumers' demand while often making money per view. By the 2010s, the fortunes of the pornography industry had changed. With reliably profitable DVD sales being largely supplanted by streaming media delivery over the Internet, competition from pirate, amateur, and low-cost professional content on the Internet had made the industry substantially less profitable, leading to it shrinking in size.[14][15] American adult magazines which have the widest distribution do not violate the Miller test and can be legally distributed. Adult magazines have been largely put into mainstream by the pioneer Playboy. However, during the so-called Pubic Wars in the 1960s and 1970s Penthouse established itself as a more explicit magazine. Screw moved the bar toward hardcore when it first came out in 1968 and with Hustler appearing in 1974 the move to hardcore was complete. By the mid-1990s magazines like Playboy had become noncompetitive and even hardcore publications like Penthouse and Hustler struggled. According to Laura Kipnis, a cultural theorist and critic, ""the Hustler body is an unromanticized body—no vaselined lens or soft focus: this is neither the airbrushed top-heavy fantasy body of Playboy, nor the ersatz opulence, the lingeried and sensitive crotch shots of Penthouse, transforming female genitals into objets d'art. It's a body, not a surface or a suntan: insistently material, defiantly vulgar, corporeal"".[16] Many adult magazines in the United States are usually sold wrapped to avoid incidental viewing by minors and are now highlighted by special features or themes. For instance, a primarily softcore magazine, Barely Legal, focuses on models between 18 and 23 years of age. Hustler's Leg World is focused on the female legs and feet. Perfect 10 publishes images of women untouched by plastic surgery or airbrushing. Pornographic bookstores have been subject to U.S. zoning laws.[17] Much of the pornography produced in the United States is in the form of movies and the branch acutely competes with the Internet. The market is very diverse and ranges from the mainstream heterosexual content to the rarefied S/M, BDSM, interracial sex, ethnic, etc. through enduringly popular gay porn. Early American stag films included Wonders of the Unseen World (1927), An Author's True Story (1933), Goodyear (1950s), Smart Alec (1951), and Playmates (1956–58). Breakthrough films, such as 1969's Blue Movie by Andy Warhol, 1972's Deep Throat, 1973's The Devil in Miss Jones and 1976's The Opening of Misty Beethoven by Radley Metzger, launched the so-called ""porno chic"" phenomenon in the United States and enabled the commercialization of the adult film industry. In this period America's most notorious pornographer was Reuben Sturman. According to the U.S. Department of Justice, throughout the 1970s, Sturman controlled most of the pornography circulating in the country. The country now houses over 40 adult movies studios featuring heterosexual scenes,[18] more than any other country. The branch, according to founder and president of Adult Video News Paul Fishbein, involves the manufacturers of adult products, distributors, suppliers, retail store owners, wholesalers, distributors, cable TV buyers, and foreign buyers. The production is concentrated in San Fernando Valley (mainly in Chatsworth, Reseda and Van Nuys) and Las Vegas, where more than 200 adult entertainment companies gather to network and show off their latest wares.[19] The world's largest adult movies studio, Vivid Entertainment, generates an estimated $100 million a year in revenue, distributing 60 films annually[20] and selling them in video stores, hotel rooms, on cable systems, and on the Internet. Vivid's two largest regional competitors are Wicked Pictures and Digital Playground. Boulder Colorado-based New Frontier Media, a leading distributor of adult movies (at NASDAQ since November 2000), is one of the two adult video companies traded publicly, the other one being Spanish Private Media Group. The industry's decision to embrace VHS in the early 1980s, for example, helped to do away with Sony Betamax, despite the latter format's superior quality. Video rentals soared from just under 80 million in 1985 to half-billion by 1993.[21] Suffering at the hands of video warez tended not be publicly stressed by country's film industry.[22] In 1999 there were 711 million rentals of hardcore films.[23] 11,300 hardcore films were released in 2002. In the recent years, according to Fishbein, there are well over 800 million rentals of adult videotapes and DVDs in video stores across the country. Digital Playground said it is choosing the Blu-ray Disc for all of its ""interactive"" films because of its greater capacity.[24] The female demographic is considered to be the biggest catalyst for pornographic cultural crossover.[25] According to Adella O'Neal, a Digital Playground publicist, in 2000 roughly 9% of the company's consumers were women while four years later that figure has bloomed to 53%. American adult pay-per-view television is presently unregulated since it is not technically ""broadcasting"" as defined in the Federal Communications Act. Cable and satellite television networks host about six main adult-related channels. Most of them (particularly Playboy TV, Penthouse TV, and Hustler TV (there is also a ""Hustler Video"", a line of raunchy films created by Larry Flynt)) are maintained by three mainstream porn magazines. In 1999 Playboy Enterprises sold to Vivid Entertainment a small channel which was renamed to Hot Network. Since that Vivid launched two more channels—the Hot Zone and Vivid TV. The viewers paid close to $400 million a year to tune into Vivid's hardcore content and the company soon overtook Playboy as operator of the world's largest adult-TV network. However, after passing the 2000 United States v. Playboy Entertainment Group case Playboy bought all three networks from Vivid in 2001 and folded them into ""Playboy's Spice"" brand. Operators then shunned ""Playboy's Spice Platinum"", a new group of channels with graphic hardcore fare.[26] Some subsidiaries of major corporations are the largest pornography sellers, like News Corporation's DirecTV. Comcast, the nation's largest cable company, once pulled in $50 million from adult programming. Revenues of companies such as Playboy and Hustler were small by comparison.[27] Microsoft has long declined to license development software to game makers whose titles include sexual content. Wal-Mart, America's largest distributor of video games, maintains the policy of selling no games with an AO rating.[28] However, in recent years the pornographic content in video games has been promoted particularly by Playboy. Playboy: The Mansion became the first game built around the ""Playboy"" license.[29] A downloadable mod—""Hot Coffee"" for the game Grand Theft Auto: San Andreas brought attention to the need to discuss the challenges faced in creating games with pornographic content. Meanwhile, Grand Theft Auto: San Andreas was pulled from shelves by Rockstar Games after it became public knowledge that, with the use of a Gameshark cheating device, the scene could be unlocked portraying the protagonist having sex with another character, although in the scene both characters have their clothes on. The game was later sold without the unlockable scene.[30] PlayStation 2 video game God of War (2005), based on Greek mythology, features an event in the first part of the game where protagonist Kratos can have sex with two topless prostitutes, who reside in a bedroom on his boat. Although no sexual acts are depicted (they occur off-screen and are indicated by sound effects), the women are shown topless. The player interacts by performing button and joystick commands that appear on screen which results in an experience reward for the player. This type of sex mini-game became a prominent feature for the God of War series, being included in its sequels God of War II (2007), God of War: Chains of Olympus (2008), God of War III (2010), and God of War: Ghost of Sparta (2010), with the latter being the final game to feature it.[31] The adult sections of American comic book stores frequently carry a large number of translations of Japanese hardcore comics, as well as an increasing number of home imitations.[32] One of the Japanese animation porn movies, which started the American adult video market, was Urotsukidoji. The adult animation market exists primarily through direct sales: mail-order to customers, and wholesale to specialty shops which cater to animation and to comic-book fans.[33] The legal framework in both countries regarding the regulation of obscene and pornographic material is overall rather similar.[34] The Internet maintains a significant part of American adult entertainment, also because the 1997 Reno v. American Civil Liberties Union case specified that the term ""indecent"" has no specific legal meaning in the context of the Internet. More recent federal efforts, such as the CAN-SPAM Act of 2003 expressly addressed the Internet.[35] On May 1, 2000, American Express announced it would no longer cover transactions from adult sites.[36][37] According to the committee to Study Tools and Strategies for Protecting Kids from Pornography and Their Applicability to Other Inappropriate Internet Content, there are over 100,000 subscription sites with adult content in the United States, with each site having multiple web pages. On average, a paid subscription generates $20 to $40 per month in revenue, however, an in-depth analysis is complicated. If a visitor site connects to a pay site and signs up for content, it receives a conversion fee from the larger site. A successful large operation is often an umbrella company serving many markets with pay sites. Around this core and its affiliates is a system of ad-supported service sites.[38] The so-called portable porn market is in its initial stage in the U.S.[39] In 2000 the owners and operators of Playgirl.com and scores of other adult sites were charged by the U.S. Federal Trade Commission with illegally billing thousands of consumers for services that were advertised as free, and for billing other consumers who never visited the web sites at all.[40] Nevadan Voice Media Incorporated, which ran several adult sites, was also charged by the commission. Sites often suffer from unauthorized, non-paying surfers who use stolen passwords, which can use month's worth of bandwidth in a day, costing the site operator hundreds or thousands of dollars' worth of additional bandwidth fees, all for traffic that returns no money at all.[41] The 2002 Paragon Electric Co., Inc. v. Buy This Domain case ruled that linking domain names to pornographic sites is not per se conclusive of bad-faith registration and use, although it does raise that presumption.[42] A common occurrence was the use of domain names similar to known ones, such as whitehouse.com (unrelated to whitehouse.gov), which for some period featured explicit content.[43] The use of expired domains is also common, along with typosquatting, which relies on mistakes such as typos made by Internet users when inputting a website address into a web browser.[44] In 1975, the total retail value of all the hardcore pornography in the U.S. was estimated at $5–10 million.[45] The 1979 Revision of the Federal Criminal Code stated that ""in Los Angeles alone, the pornography business does $100 million a year in gross retail volume"" while ""the average pornography magazine sells for between $6 and $10 each"". According to the 1986 Attorney General's Commission on Pornography, American adult entertainment industry has grown considerably over the past thirty years by continually changing and expanding to appeal to new markets, though the production is considered to be low-profile and clandestine.[46] The total income of modern country's adult entertainment is often rated at $10–13 billion, of which $4–6 billion are legal. The figure is often credited to a study by Forrester Research and was lowered in 1998.[47] In 2007 The Observer newspaper also gave a figure of $13 billion.[48] Other sources, quoted by Forbes (Adams Media Research, Veronis Suhler Communications Industry Report, and IVD), even taking into consideration all possible means (video networks and pay-per-view movies on cable and satellite, web sites, in-room hotel movies, phone sex, sex toys, and magazines) mention the $2.6–3.9 billion figure (without the cellphone component). USA Today claimed in 2003 that websites such as Danni's Hard Drive and Cybererotica.com generated $2 billion in revenue in that year, which was allegedly about 10% of the overall domestic porn market at the time.[49] The adult movies income (from sale and rent) was once estimated by AVN Publications at $4.3 billion but how this figure was determined is unclear. According to the 2001 Forbes data the annual income distribution is like this: The Online Journalism Review, published by the Annenberg School of Communication at the University of Southern California, weighed in with an analysis that favored Forbes' number. The financial extent of adult films, distributed in hotels, is hard to estimate—hotels keep statistics to themselves or do not keep them at all.[51] A CBS News investigation in November 2003 claimed that 50% of guests at the Hilton, Marriott, Hyatt, Sheraton, and Holiday Inn hotel chains purchased adult movies, contributing to 70% of in-room profits. The income of cellphone porn is low, when compared with other countries. The absence of V-chip-style parental controls on other equipment has obviated the need for American consumers to use cellphones to access explicit content.[50] The definition of pornography in the U.S. evolved through decades, from the 1960s. In this period, recognizing ambiguities, the term ""sexually explicit content"" gained use as one of the pornography's euphemisms,[52] but later it was determined that a distinction between pornographic and sexually explicit content is completely artificial.[53][e] In Miller v. California the Supreme Court used the definition of pornography made by Webster's Third New International Dictionary of 1969 (""a depiction (as in a writing or painting) of licentiousness or lewdness: a portrayal of erotic behavior designed to cause sexual excitement"").[54]Black's Law Dictionary followed the Miller test and defined pornography as material that taken as a whole the average person, applying contemporary community standards, would find appealing to the prurient interest. Heinle's Newbury House Dictionary of American English (2003) defined pornography as ""obscene writings, pictures, or films intended to arouse sexual desire"". The Antipornography Civil Rights Ordinance defined pornography as the ""graphic sexually explicit subordination of women, whether in pictures or in words"". The ordinance was ruled unconstitutional by the Federal Appeals Court in American Booksellers v. Hudnut in Indianapolis (1985). Courts in California and New York have clearly rejected the argument that the making of pornography is prostitution.[55] (See California v. Freeman (1988) and People v. Paulino (2005).) The Oregon Supreme Court went even further in State v. Henry (1987) by abolishing the legal definition of obscenity in that state, ruling it violated freedom of speech as defined in the state constitution. Pornography as a legal term at the federal level, except the generic terms ""hardcore pornography""[a] and ""child pornography"",[56][b] has not existed since the 1973 Miller v. California case.[57][58][c] The United States Supreme Court in Miller v. California[59] held that one type of pornography, namely obscenity, does not enjoy First Amendment protection, but recognized that individual communities had different values and opinions on obscenity. The Court defines obscenity in accordance with the Miller test.[60][61] Since then several states have enacted laws that apply that test.[62][d] Relying on the 1930 Smoot–Hawley Tariff Act and under the terms ""obscene"" and ""immoral"", the U.S. Customs and Border Protection prohibits the importation of any pornographic material (19 U.S.C. § 1305a ""Immoral articles; importation prohibited"").[63] Attempts were made in the United States in the 1970s to close down the pornography industry by prosecuting those in the industry on prostitution charges. The prosecution started in the courts in California in the case of People v. Freeman. The California Supreme Court acquitted Freeman and distinguished between someone who takes part in a sexual relationship for money (prostitution) versus someone whose role is merely portraying a sexual relationship on-screen as part of their acting performance. The State did not appeal to the United States Supreme Court making the decision binding in California, where most pornographic films are made today.[11] The term ""pornography"" first appeared in an 1857 British medical dictionary, which defined it as ""a description of prostitutes or of prostitution, as a matter of public hygiene"",[64] therefore pornography by itself was not a widely used term in nineteenth-century America[65] and the term did not appear in any version of American Dictionary of the English Language in its early editions. The dictionary introduced the entry in 1864, defining it primarily as a ""treatment of, or a treatise on, the subject of prostitutes or prostitution"". Early charges used the term ""obscenity"" as well as after Miller v. California, though the term ""pornography"" remained as a reference entry: The censorship of pornographic materials in the United States was enabled by the way courts interpreted the First, and partially Ninth[66] and Fourteenth amendments to the U.S. Constitution. The legal justification also includes the so-called harm principle, as in Canada and the United Kingdom.[67] The absolutist interpretation of the First Amendment as applied to pornography has never been sustained by the Supreme Court.[68] In the Investigation of Literature Allegedly Containing Objectionable Material, issued by the U.S. Congress Select Committee on Current Pornographic Materials in 1953, it was noted that ""perhaps the greatest impediments to the prompt and effective enforcement of existing laws intended to control pornographic materials are the difficulties of establishing a precise interpretation of the word"". During the Warren Court (1953–1969), the first notable court to face the cases of such kind, justices Potter Stewart, Byron White, and Arthur Goldberg shared the opinion that only hardcore pornography was not protected by the First and Fourteenth Amendments.[69] This position was contested notably by U.S. Solicitor General James Lee Rankin (in office 1956–1961),[70] but in Jacobellis v. Ohio Stewart concluded that criminal obscenity laws are constitutionally limited under the First and Fourteenth Amendments to hardcore pornography.[71] Concurring in the 1957 Roth v. United States Justice John Marshall Harlan II wrote that ""even assuming that pornography cannot be deemed ever to cause, in an immediate sense, criminal sexual conduct, other interests within the proper cognizance of the States may be protected by the prohibition placed on such materials."" The 1967 Public Law 90-100 found the traffic in pornography to be ""a matter of national concern"", as well as in obscenity.[72] In this period the Court considered pornography to have two major dimensions. The first can be defined as dealing with sexual representations that are offensive to public morality or taste, which concerned the Court notably in the 1966 Ginzburg v. United States case. The second centers on the effect of pornography on specific individuals or classes, which is the focus of most public discussions and prior Court pornography decisions. This dimension was mentioned only twice in the array of decisions made in 1966. A frustration was expressed notably by Justice Hugo Black in the 1966 Mishkin v. New York: ""I wish once more to express my objections to saddling this Court with the irksome and inevitably unpopular and unwholesome task of finally deciding by a case-by-case, sight-by-sight personal judgment of the members of this Court what pornography (whatever that means) is too hard core for people to see or read.""[73] In the 1974 Hamling v. United States decision the Supreme Court said that just because pornographic materials are for sale and purchased around the country, ""Mere availability of similar materials by itself means nothing more than that other persons are engaged in similar activities.""[74] The 1976 American Heritage Dictionary of the English Language defined that pornography consists of ""written, graphic, or other forms of communication intended to excite lascivious feelings"". Since determining what is pornography and what is ""soft core"" and ""hard core"" are subjective questions to judges, juries, and law enforcement officials, it is difficult to define, since the law cases cannot print examples for the courts to follow.[75] The Rehnquist Court further enhanced the power of community controls on pornography.[76] Current Chief Justice John Roberts told at the confirmation hearing on his nomination: ""Well, Senator, it's my understanding under the Supreme Court's doctrine that pornographic expression is not protected to the same extent at least as political and core speech, and the difficulty that the Court has addressed in these different areas of course is always defining what is or is not pornography and what is entitled to protection under the First Amendment and what is not"".[77] The famous Indianapolis definition of pornography by Dworkin and MacKinnon paralleled their Minneapolis ordinance. The first was rejected by the United States Court of Appeals for the Seventh Circuit for several reasons. The ordinance did not use any of the accepted terms that the Supreme Court had developed over time for determining when material is obscene, including ""prurient interest"", ""offensiveness"", or ""local community standards"".[78] Another concern was the way the women were depicted in the work. If women were referred to in the approved fashion stressing equality, the activity involved would be regardless of how sexually explicit it was.[78] The Court also indicated that if women were referred to in a disapproving way depicting them as subversive or as enjoying humiliation, the activity would be unlawful regardless of the ""literary, artistic or political qualities of the work taken as a whole"".[78] Judge Frank Easterbrook said: ""We accept the premises of this legislation. Depictions of subordination tend to perpetuate subordination. The subordinate status of women in turn leads to affront and lower pay at work, insult and injury at home, battery and rape on the streets.... Yet this simply demonstrates the power of pornography as speech.""[79]Conceptions of Democracy in American Constitutional Argument: The Case of Pornography Regulation by Frank Michelman, issued by Tennessee Law Review (vol. 56, no. 291, 1989) partially consented that ""pornography is political expression in that it promulgates a certain view of women's natures and thus of women's appropriate relations and treatment in society"", but also concluded that the Indianapolis ordinance was precisely designed to suppress that particular view by censoring pornography. Minneapolis ordinance was struck down on the grounds that it was ambiguous and vague, however, despite its failure the proposal influenced other communities across the United States.[78] In May 2005 U.S. Attorney General Alberto Gonzales established an Obscenity Prosecution Task Force.[80] The task force, according to a Department of Justice news release on May 5, was ""dedicated to the investigation and prosecution of the distributors of hard-core pornography that meets the test for obscenity, as defined by the United States Supreme Court."" Under President Bush's and Gonzales' rationales the FBI Adult Obscenity Squad[81] was recruited in August 2005 to gather evidence against ""manufacturers and purveyors"" of adult pornography.[82] The 1970 Lockhart Commission recommended eliminating all criminal penalties for pornography except for pornographic depictions of minors, or sale of pornography to minors.[83] However, prior to 1977, only two states had laws which prohibited the use of children in the production or distribution of pornographic materials or performances.[84] In 1977, the Department of Justice strongly endorsed legislation which banned the production and dissemination of child pornography.[85] These efforts have been unsuccessfully challenged in the 1982 New York v. Ferber case (""The States are entitled to greater leeway in the regulation of pornographic depictions of children..."").[86] Although the states have a different age of consent, in accordance with the Federal Labeling and Record-Keeping Law all models featured in pornographic content should be at least 18 years of age. This kind of material is often labeled as ""adult"" and the appropriate disclaimers are common. They are based on what ""depicts or describes, in terms patently offensive as measured by contemporary community standards, sexual or excretory activities or organs"".[87] It is a federal crime to possess, distribute, or produce non-fictional child pornography and carries large fines and prison sentences of up to 30 years upon conviction and requirement to register as a sex offender.[88] An anti-pornography movement has existed in the United States since before the 1969 Supreme Court decision of Stanley v. Georgia, which held that people could view whatever they wished in the privacy of their own homes, by establishing an implied ""right to privacy"" in U.S. law.[89] This led President Lyndon B. Johnson, with the backing of Congress, to appoint a commission to study pornography.[90] The anti-pornography movement seeks to maintain or restore restrictions and to increase or create restrictions on the production, sale or dissemination of pornography. Jesuit priest Father Morton A. Hill (1917-1985) was a leader of the campaign against pornography in the United States in the 1960s, 1970s and 1980s. He was one of the founders of Morality in Media, which was created in 1962 to fight pornography. Morality in Media was launched by an interfaith group of clergy and Hill was president until his death in 1985. Morality in Media continues with Patrick A. Trueman, a registered federal lobbyist,[91][92] as president.[93] So prominent was Hill on the issue, that in 1969 President Lyndon B. Johnson appointed him to the President's Commission on Obscenity and Pornography. Father Hill and another clergyman on the commission, Dr. Winfrey C. Link, believed that the commission was stacked with supporters of loosening laws on pornography, and issued the Hill-Link Minority Report rebutting the conclusions of the majority report, which held that pornography should be decriminalized as there were no links between it and criminal behavior. The majority report was widely criticized and rejected by Congress.[90] The Senate rejected the commission's findings and recommendations by a 60–5 vote, with 34 abstentions.[94] President Nixon, who had succeeded Johnson in 1969, also emphatically rejected the majority report.[95] The Hill-Link Minority Report, on the other hand, which recommended maintaining anti-obscenity statutes, was read into the record of both the United States Senate and the United States House of Representatives. It was cited by the Burger Court in its 1973 obscenity decisions, including Miller v. California.[96] a .mw-parser-output .citation{word-wrap:break-word}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}^ In the 1969 Stanley v. Georgia case (later followed by the 1971 United States v. Reidel), the Supreme Court ruled that private possession of pornography (except child pornography as determined in 1990 by a 6 to 3 decision[83][97]) in the home was not a crime, nor was it subject to government regulation. Technically why people have a constitutional right to watch hardcore pornography privately has never been explained,[98] but in the 1973 Paris Adult Theatre I v. Slaton Chief Justice Burger wrote: ""We categorically disapprove the theory, apparently adopted by the trial judge, that obscene, pornographic films acquire constitutional immunity from state regulation simply because they are exhibited for consenting adults only. This holding was properly rejected by the Georgia Supreme Court.""[99]Miller v. California held 5–4 that the state may outlaw the showing of hardcore pornographic films, even if the ""adult theatre"" is clearly labeled and warns. b. ^ Sex tourism involving persons under 18 outside the U.S. is also illegal.[100] c. ^ The term ""dial-a-porn"" was used at the federal level e.g. by the Telecommunications Act of 1996,[101] but it was partially voided by federal courts over subsequent years.[102] Another term usage includes the 1996 Denver Area Educational Telecommunications Consortium v. Federal Communications Commission certiorari to the Court of Appeals for the District of Columbia Circuit.[103] d ^ The transportation of pornography in interstate commerce was banned in the 1973 case of United States v. Orito. The Supreme Court upheld zoning restrictions that either quarantine or disperse pornography merchants, leaving it to local officials to determine whether local interests are best served by restricting all porn merchants to a single district. Though the Court has also upheld zoning that prohibits pornographic entertainment within a certain distance of a school, the legitimate primary purpose excludes the preventing of access by minors which can be achieved much more directly by simple restrictions.[104] On May 13, 2002, writing for the majority in Ashcroft v. American Civil Liberties Union, Justice Clarence Thomas stated that using community standards to identify material that could be harmful to minors does not make the law overly broad and therefore unconstitutional under the First Amendment.[105] e ^ Utah uses the legal term ""pornography"" for the same illegal sexually explicit material.[106] f ^ See also the 1977 Splawn v. California and 1978 Pinkus v. United States."
Port Royal School,/wiki/Port_Royal_School,"The Port Royal School is a historic South Carolina school building. It is located at 1214 Paris Avenue in the town of Port Royal. Its original main block is a two-story Colonial Revival structure designed by Wilson and Sompayrac and built in 1911. In 1954 a single-story brick Modern addition was added to the north of this building; it was designed by William Harleston of Halsey & Cummings. A second addition was made in 2002, further extending the 1954 building to the north. The building is believed to be the second-oldest active elementary school building in the state. It also contributed to the area's checkered history of the provision of so-called ""separate but equal"" educational facilities for whites and African Americans: the 1954 addition occurred at a time when the last elementary school for African Americans in Port Royal was closed, requiring the transportation of those students to Beaufort. The Port Royal School remained whites-only until 1964.[2] The school was listed in the National Register of Historic Places in 2014.[1] This article about a property in Beaufort County, South Carolina on the National Register of Historic Places is a stub. You can help Wikipedia by expanding it. This South Carolina school-related article is a stub. You can help Wikipedia by expanding it. This African American–related article is a stub. You can help Wikipedia by expanding it."
Post-racial America,/wiki/Post-racial_America,"Post-racial United States is a theoretical environment in which the United States is free from racial preference, discrimination, and prejudice. One of the earliest uses of the term ""post-racial"" to describe the United States was in an October 5, 1971, article in The New York Times titled ""Compact Set Up for 'Post-Racial' South"".[1] The article reported the establishment of a ""Southern Growth Policies Board"" in Durham, North Carolina, ""by some 70 politicians and professors who believe their region of 60 million citizens has entered an era in which race relations are soon to be replaced as a major concern by population increase, industrial development, and economic fluctuations"".[1] Some Americans saw the presidential candidacy of Barack Obama, and his election in 2008 as the first black president of the United States, as a sign that the nation had, in fact, become post-racial.[2][3] The conservative radio host Lou Dobbs, for example, said in November 2009, ""We are now in a 21st-century post-partisan, post-racial society.""[4] Two months later, Chris Matthews, an MSNBC host, said of President Obama, ""He is post-racial by all appearances. You know, I forgot he was black tonight for an hour.""[5] However, public opinion on whether the United States is post-racial is itself divided starkly by race. In a Washington Post/ABC News poll conducted in December 2014, about 50% of white respondents said they believed that the justice system treats Americans of all races equally, but only 10% of African-Americans said the same.[6] In the spring of 2015, according to a Gallup poll, 13% of black Americans surveyed identified race relations as the most important problem the United States faces, compared with 4% of white Americans.[7] Arguments that the United States is not post-racial frequently emphasize the treatment of African-Americans and other racial minorities in the criminal justice system and in interactions with the police. Killings of unarmed African-Americans, often by police officers, have been widely publicized. In 2015, according to a study by The Guardian, police officers in the United States killed 7.13 black Americans per million, compared with 2.91 white Americans per million.[8] Additionally: Young black men were nine times more likely than other Americans to be killed by police officers in 2015, according to the findings of a Guardian study that recorded a final tally of 1,134 deaths at the hands of law enforcement officers this year. Despite making up only 2% of the total US population, African American males between the ages of 15 and 34 comprised more than 15% of all deaths logged this year by an ongoing investigation into the use of deadly force by police. Their rate of police-involved deaths was five times higher than for white men of the same age.[9] Such killings had a marked effect on public perceptions of race relations in America. The 13 percent of black Americans who called race relations the most pressing problem in the United States in the spring 2015 Gallup poll dwarfed the 3 percent that Gallup reported at the beginning of 2014.[7] And the percentage of white Americans who said race relations were the most important issue rose to 4 percent in 2015 from 1 percent in 2014.[7] In response to high-profile incidents such as the fatal shootings of Michael Brown, Aiyana Jones, Trayvon Martin, Laquan McDonald, Tamir Rice, and Walter Scott, and the death of Freddie Gray from a spinal-cord injury sustained in police custody, academics[3] and journalists[10] have denounced claims that America is post-racial. Ta-Nehisi Coates wrote in The Atlantic in 2015 that the phrase ""post-racial"" was ""usually employed by talk-show hosts and news anchors looking to measure progress in the Obama era.""[10] And Anna Holmes wrote in The New York Times, ""Chattel slavery and the legacies it left behind continue to shape American society. Sometimes it seems as if the desire for a 'post-racial' America is an attempt by white people to liberate themselves from the burden of having to deal with that legacy.""[11] However, others argue that post-racial politics champions aggressive action to deliver economic opportunity and weed out police misconduct, without the divisive framing of racial identity. Under this view, there is no claim that America has attained a fully post-racial society, but it is argued that news selection is skewed toward amplifying racial conflict, events demonstrating racial harmony are dismissed as non-newsworthy, and that such media conflict-bias acts to undermine trust and impede progress. Rather, any true measure of race relations must gauge the everyday daily experiences of Americans in interacting with people of differing backgrounds. An assumption is that the media will cherry-pick the most outrageous, racially-inflammatory events to cover no matter how infrequently they are occurring, and thus misreport progress toward a post-racial ideal. The central tenet of post-racial problem-solving practice is to seek the ""alternative explanation"" when conflict arises (presuming non-racist motives in others), in order to find common ground and creatively resolve the conflict. Examples of post-racial framing in attacking misconduct by the criminal justice system are video recording of all police-citizen interactions, creating a Citizens Review Board with investigative powers, and assigning an independent prosecutor. Or, in the educational sphere, creating charters, academies and school choice to turn around under-performing schools. The divide in public opinion on the status of race in America was reflected in reactions to the Black Lives Matter movement. In response to the ""black lives matter"" rallying cry, some people, including politicians, began using the phrase ""all lives matter"".[12][13][14] After a sheriff's deputy in Harris County, Texas, was fatally shot while pumping gas in August,[15] Sheriff Ron Hickman claimed that the rhetoric of Black Lives Matter activists had contributed to the killing and said, ""We've heard 'black lives matter'. All lives matter. Well, cops' lives matter, too. So why don't we just drop the qualifier and just say 'lives matter', and take that to the bank.'[16] Supporters of the Black Lives Matter movement criticized the ""all lives matter"" phrase, arguing that it minimized the systemic threats faced by African-Americans.[17][18][19] President Obama said in October, ""There is a specific problem that is happening in the African-American community that's not happening in other communities.""[20]Andrew Rosenthal wrote, similarly, in The New York Times, ""The point of 'Black Lives Matter' is that the lives of African-Americans have come under special and deadly threat since before the birth of this country.""[21] Evidence of continued racial divisions in the United States can also be found in demographics. For instance, African-Americans account for less than 15 percent of the total population of Michigan, but more than 82 percent of the population of the state's largest city, Detroit[22] — and Detroit, like many cities whose residents are predominantly black, has ""resegregated schools, dwindling tax bases and decaying public services"".[23] There is a similar dynamic in Louisiana; the state was about 64 percent white as of the 2010 Census,[24] but its largest city, New Orleans, is 60 percent black.[25] Further segregation can be found within New Orleans: the Lower Ninth Ward, for example, is 97 percent black.[26] This was the neighborhood that experienced the most catastrophic flooding after Hurricane Katrina, and the government's response to the disaster has been cited as evidence of the continued presence of racism in the United States.[27][28] Most of the victims were black and poor, and class was a major factor in who survived: Those who lived in areas better protected from flooding, and those who were able to evacuate before the storm, tended to be wealthier.[29] At the time, President George W. Bush acknowledged that this poverty had ""roots in the history of racial discrimination, which cut off generations from the opportunities of America"".[30] The idea that America is post-racial, or close to it, has played a role in at least one United States Supreme Court decision. In Shelby County v. Holder in 2013, the court invalidated a section of the Voting Rights Act of 1965 that had required nine states with particularly severe histories of racial discrimination to obtain federal approval for any change to their election laws.[31] The ruling, written by Chief Justice John G. Roberts Jr., said in part, ""Our country has changed."" It added that in the decades since the Voting Rights Act was passed, ""voting tests were abolished, disparities in voter registration and turnout due to race were erased, and African-Americans attained political office in record numbers. And yet the coverage formula that Congress reauthorized in 2006 ignores these developments, keeping the focus on decades-old data relevant to decades-old problems, rather than current data reflecting current needs.""[32] Similar issues are involved[33] in Fisher v. University of Texas, a challenge to affirmative action policies on which the court ruled in 2016,[34] upholding the race-based admissions policy of the University of Texas. Opponents of post-racialism argue that it ignores racial issues that are perceived as prevalent today. Harvard scholar Lawrence D. Bobo asserted that racism is still prevalent in subtle ways.[35]"
Protected group,/wiki/Protected_group," A protected group, protected class (US), or prohibited ground (Canada) is a category by which people qualified for special protection by a law, policy, or similar authority. In Canada and the United States, the term is frequently used in connection with employees and employment and housing. Where illegal discrimination on the basis of protected group status is concerned, a single act of discrimination may be based on more than one protected class. For example, discrimination based on antisemitism may relate to religion, ethnicity, national origin, or any combination of the three; discrimination against a pregnant woman might be based on sex, marital status, or both.[1] ""Prohibited grounds of discrimination"" (French: motif de distinction illicite) in employment and housing are listed in the federal Canadian Human Rights Act as well as the provincial human rights codes. For examples the federal law lists: race, national or ethnic origin, colour, religion, age, sex, sexual orientation, gender identity or expression, marital status, family status, genetic characteristics, disability, and conviction for an offence for which a pardon has been granted or in respect of which a record suspension has been ordered.[2] Article 14 of the European Convention on Human Rights states that discrimination is prohibited on ""any ground"" (French: sans distinction aucune) but also lists several examples. This protection was expanded by Protocol 12 to the European Convention on Human Rights which states that all law must also applied without discrimination, and not just in housing, employment, and other areas covered by the Convention. This was first litigated in 2009 when the court found in Sejdić and Finci v. Bosnia and Herzegovina that constitutional rules around eligibility to run for office also must be non-discriminatory. US federal law protects individuals from discrimination or harassment based on the following nine protected classes: sex (including sexual orientation and gender identity[3]), race/color, age, disability, national origin, religion/creed, or genetic information (added in 2008).[clarification needed] Many state laws also give certain protected groups special protection against harassment and discrimination, as do many employer policies. Although it is not required by federal law, state law and employer policies may also protect employees from harassment or discrimination based on marital status.[1] The following characteristics are ""protected"" by United States federal anti-discrimination law: Individual states can and do create other classes for protection under state law. Presidents have also issued executive orders which prohibit consideration of particular attributes in employment decisions of the United States government and its contractors. These have included Executive Order 11246 (1965), Executive Order 11478 (1969), Executive Order 13087 (1998), Executive Order 13279 (2003), and Executive Order 13672 (2014)."
Rights and responsibilities of marriages in the United States,/wiki/Rights_and_responsibilities_of_marriages_in_the_United_States,"According to the United States Government Accountability Office (GAO), there are 1,138 statutory provisions[1] in which marital status is a factor in determining benefits, rights, and privileges. These rights were a key issue in the debate over federal recognition of same-sex marriage. Under the 1996 Defense of Marriage Act (DOMA), the federal government was prohibited from recognizing same-sex couples who were lawfully married under the laws of their state. The conflict between this definition and the Due Process Clause of the Fifth Amendment to the Constitution led the U.S. Supreme Court to rule DOMA unconstitutional on June 26, 2013, in the case of United States v. Windsor. DOMA was finally repealed and replaced by the Respect for Marriage Act on December 13, 2022, which retains the same statutory provisions as DOMA and extends them to interracial and same-sex married couples. Prior to the enactment of DOMA, the GAO identified 1,049 federal statutory provisions[2] in which benefits, rights, and privileges are contingent on marital status or in which marital status is a factor. An update was published in 2004 by the GAO covering the period between September 21, 1996 (when DOMA was signed into law), and December 31, 2003. The update identified 120 new statutory provisions involving marital status, and 31 statutory provisions involving marital status repealed or amended in such a way as to eliminate marital status as a factor. The first legally-recognized same-sex marriage occurred in Minneapolis,[3] Minnesota, in 1971.[4] On June 26, 2015, in the case of Obergefell v. Hodges, the Supreme Court overturned Baker v. Nelson and ruled that marriage is a fundamental right guaranteed to all citizens, and thus legalized same-sex marriage nationwide. There are some laws that either benefit or penalize married couples over single people, depending upon their own circumstances: In addition, community-property states frequently have forms of ownership that allow a full basis step-up on one's own share of community property on the death of a spouse (in addition to the normal step-up on spouse's assets). Following the 2004 GAO report at least one bill, the Uniting American Families Act, has been proposed to attempt to remedy some of the differences in rights between same-sex partnerships and marriages."
Sisters in Law (book),/wiki/Sisters_in_Law_(book),"Sisters in Law: How Sandra Day O'Connor and Ruth Bader Ginsburg Went to the Supreme Court and Changed the World is a 2015 non-fiction book by Linda Hirshman. The book examines the legal careers and judicial records of Sandra Day O'Connor and Ruth Bader Ginsburg, the first and second women appointed to the Supreme Court of the United States. The book follows the careers and backgrounds of the first two women to serve on the Supreme Court of the United States — Sandra Day O'Connor and Ruth Bader Ginsburg — and attempts to situate their respective biographies, ascent to the court, and judicial records within the broader context of the women's rights movement. Both women met with sexist challenges to their continued careers upon graduating from law school in the 1950s, but O'Connor acceded to the conservative values of putting her family first, and rose to power within Arizona's Republican Party mostly as a volunteer and socialite, until taking the reins as appointed majority leader in the Arizona State Senate. Ginsburg, with the support of her successful tax-attorney husband, chose the parallel routes of academia and activism, founding and heading the American Civil Liberties Union's Women's Rights Project. In this project, she sought to promote cases that would eradicate any formal inequalities between men and women, focusing to a great degree on challenging laws providing benefits to women, which she saw as promoting stereotypes of women as weak and incapable of independent action. According to Hirshman's analysis, the two women used both their judicial skills and court politics to promote women's rights, though in different ways, as O'Connor remained a staunch Republican and Ginsburg was a classic liberal. Hirshman surveys major cases to prove her thesis that both women justices contributed to massive changes in women's rights, especially lauding Ginsburg, whom she compares with Wolfgang Amadeus Mozart and Jane Austen: “Mozart had, by many accounts, five operatic masterpieces. Jane Austen’s reputation rests on five novels. . . . In five landmark cases over less than a decade, [Ginsburg] largely transformed the constitutional status of women in America.”[1][2] Hirshman also focuses on the relationship between the two justices, using as an example the Virginia Military Institute case, United States v. Virginia, which struck down the long-standing male-only admission policy of the school, and is widely considered ""the jewel in the crown"" of Ginsburg's majority opinions. As the senior justice, O'Connor could have written the opinion, but in an act of generosity, demurred, saying, ""This should be Ruth's.""[2][3] Author Linda Hirshman is a lawyer, philosopher and cultural historian, whose focus is on social movements such as the struggle against sexual and gender violence, gay rights and women's rights. As a lawyer, she focused mostly on labor law, appearing before the Supreme Court in three cases (one win, one loss and one draw), including Garcia v. San Antonio Metropolitan Transit Authority, a landmark case in which the Court held that the Congress has the power under the Commerce Clause of the Constitution to extend the Fair Labor Standards Act of 1938, which requires that employers provide minimum wage and overtime pay to their employees, to state and local governments.[4] Hirshman has also been a professor of philosophy and women's studies at Brandeis University, professor of law at Chicago-Kent College of Law, and visiting professor at Northwestern Law School. Hirshman received a PhD in Philosophy from University of Illinois Chicago, a Juris Doctor from University of Chicago Law School and a Bachelor of Arts from Cornell University. Sisters in Law became a bestseller, appearing on The New York Times[5] and The Washington Post bestseller lists. According to The New York Times critic Linda Greenhouse, ""There is a fascinating book struggling to emerge from the narrative structure Linda Hirshman has imposed on rich material.""[6] She notes that Hirshman is an obvious admirer of Ginsburg, but is less sure how to present conservative O'Connor as a champion of women's rights, which is the thesis of the book, ultimately settling for describing her strategy as one of defense, in which ""she would not permit the courts to roll the equality ball backward,"" while Ginsburg, for her part, ""played offense,"" advocating for change. Kirkus Reviews called Sisters in Law ""[a]n intelligent, evenhanded look at a changing society and its legal foundations.""[7]Publishers Weekly gave the book a positive review, citing in particular ""the unusual step of addressing the influence that Supreme Court law clerks can have on the Court’s decision-making,"" and the ""quality time"" Hirshman spends ""discussing the evolution of the constitutional theories that the justices apply when analyzing such flash-point issues as reproductive rights and workplace sexual harassment."" The review also lauds ""Hirshman’s conversational style and deep analysis of several precedent-setting constitutional cases [that] should appeal to both casual and professional readers.[8] In 2019 a play premiered based on the book, also called Sisters in Law. The book was adapted for the stage by playwright Jonathan Shapiro, and Patricia McGregor directed.[9]Tovah Feldshuh portrayed Ginsburg, and Stephanie Faracy portrayed O'Connor. The Hollywood Reporter deemed that the timing of the production was perfect, given that ""the conundrum of the #MeToo era is that even while women are being empowered with agency, they're still making only 79 cents on the dollar compared to men, reproductive rights are being quietly scaled back and a Supreme Court justice with sexual assault charges leveled against him was seated without a thorough investigation.""[10] The review found the production to be visually strong, and intelligently rendered, as the two characters relitigate major cases that marked their respective careers. However, the review also found that ""Sisters in Law's greatest deficiency is that it's all head and no heart.""[10] According to the Los Angeles Times, the dialog of the play is engaging, marked by ""beautifully paced and illuminating banter"", and positively noted the performances, costume design and set design.[9] Notwithstanding the general praise of the review, reviewer Margaret Gray nevertheless finds that the play oversimplifies two complex characters, and disapproves of some of the techniques used to make the spectator expect that what occurs on stage is close to what happened in reality, an expectation not delivered in the production.[9] United States:"
Source of income discrimination,/wiki/Source_of_income_discrimination,"Source of Income Discrimination describes when landlords refuse to rent to tenants using housing vouchers or other government assistance. Housing advocates argue the practice keeps vulnerable communities from accessing housing, although landlords point to lack of protections for tenants as their right to refuse service. In the United States, housing vouchers fall under Section 8 of the Housing Act of 1937. Section 8 housing vouchers provide housing assistance for low-income, elderly, and disabled individuals or families. [1] The term “source of income discrimination” is used by housing advocates [2] to describe a phenomenon that is legal nationwide in the United States but is increasingly being banned on the state [3] and city level. [4][5] Participation in the Section 8 Housing Voucher Program is largely voluntary for landlords.[6] The Biden Administration acknowledged the practice is currently legal federally but promised to address the issue. [7] State laws banning source of income discrimination vary widely with some including protections for tenants using section 8 housing vouchers and some not. [8] Advocates, such as the NAACP, argue renters have been unfairly denied usage of their housing voucher and that acceptance of housing vouchers leads to more diverse communities.[9] 22 states, California, Colorado, Connecticut, Delaware, Hawaii, Illinois, Iowa, Maine, Maryland, Massachusetts, Minnesota*, New Jersey, New York, North Dakota, Oklahoma*, Oregon, Rhode Island, Utah, Vermont, Virginia, Washington, and Wisconsin*, have statewide policies banning source of income discrimination. [10] Arizona, Indiana, and Texas preempt the passage of source of income discrimination laws; however, city ordinances may differ. Phoenix, Arizona passed an ordinance in March 2023 banning source of income discrimination.[11] *Excludes section 8 housing There are more than two million households in the United States that participate in the Section 8 Housing Choice Voucher Program (the Section 8 voucher program) to afford privately owned rental housing.[12] When a Section 8 voucher participant rents from a participating landlord, the local PHA “pays the difference between the household’s contribution (set at 30 percent of income) and the total monthly rent.” [13] The Section 8 voucher program does not set a maximum rent, but participants must pay the difference between the calculated subsidy and actual rent.[13] Landlords receive the subsidy directly from the PHAs. In the first study, in the early 1980s, 50 percent of the Section 8 Housing Voucher participants were able to find housing. This number increased to 68 percent from 1985 to 1987.[14] There was a rise to 81 percent by 1993. However, the figures dropped to 69% success in 2000.[14] The low success rates can be attributed to landlords declining to accept the vouchers either because of discrimination against the participants in the program or because of the burdens the program places on housing providers. In a study conducted by U. S. Department of Housing and Urban Development, researchers examined the landlord denial rate of individuals with a housing voucher in neighborhoods with varying poverty rates. The researchers found that in areas where people with vouchers are considered a protected class, the denial rates are significantly lower. The areas with less outright denial of people with vouchers often tend to be the neighborhoods with higher poverty rates. Specifically, denial rates were 11 to 27 percent higher in low-poverty tracts compared to high-poverty tracts. [15] In addition, while the refusal to accept the vouchers appears racially neutral on its face, many housing advocates contend that the acceptability and legality of Section 8 discrimination enable landlords to use it as a proxy for other legally prohibited kinds of discrimination, such as that based on race, ethnicity, national origin, gender, family status, or disability.[16] For example, studies show that the discrimination against Section 8 voucher holders increases if the recipient is African American or Latino.[16] Because of discrimination against voucher holders, many subsidy recipients can only find housing in neighborhoods where they already are in the racial majority.[17] One way in which discriminated parties have dealt with discrimination is by bringing disparate impact claims. In disparate impact claims, a prima facie case of discrimination is established by showing that the challenged practice of the defendant actually or predictably results in racial discrimination.[18] This analysis focuses on facially neutral policies that may have a discriminatory effect. Federal courts will allow claims to be made under the FHA on a disparate impact theory by analogizing the FHA to Title VII because they both share a goal of reducing discrimination.[18] However, courts are dividing on how they rule when it comes to allowing disparate impact claims under the FHA for voucher discrimination. A few federal courts have allowed plaintiffs who were denied housing because of their vouchers to assert these claims. Other courts have limited or prohibited them. Thus, the courts are not uniform when it comes to addressing disparate impact claims for voucher discrimination. Congress has recognized that refusing to rent to families with children violated the FHA, and it should extend that protection to people who use vouchers. Without more legal protections, voucher discrimination can continue, and the Section 8 Housing Voucher Program can be in danger of not meeting its intended goal of increasing the quantity of options and quality of housing for low-income individuals and families.[19]"
Southern Legal Resource Center,/wiki/Southern_Legal_Resource_Center,"The Southern Legal Resource Center, Inc. (SLRC) is a South Carolina non-profit public law corporation[1] which offers legal support to defend what they see as First Amendment violations, violation of civil rights, or “discrimination against advocates of southern heritage”. The SLRC was founded in 1995 by a group of four attorneys: Carl A. Barrington (deceased), Kirk David Lyons, Larry Norman, and Lourie A. Salley III. Lyons was appointed Chief Trial Counsel, a position he still holds, and Salley became the firm's first board chairman. The organization is a registered South Carolina corporation with its executive offices in Black Mountain, North Carolina. In 1996, the SLRC successfully defended the ""Blacksburg (SC) 7"" and in 1999 sued a Greenville, South Carolina, private academy on behalf of Dr. Winston McCuen, a teacher at the school who had been fired for refusing to remove a Confederate flag that was part of a classroom historical display, and for refusing to salute the US in protest. In 2004 the SLRC hired advertising executive and Southern activist Roger McCredie as its full-time executive director. Under McCredie the organization doubled the size of its board of directors, increased its advertising program and undertook an ambitious five-year growth and development plan.[2] The SLRC, was the first law firm to promote a legal theory developed by Lyons that combined First Amendment protection with an interpretation of the ""National Origin"" provision of the Civil Rights Act of 1964 that would afford federal legal protection for what Lyons termed ""Confederate Southern Americans."" Using this interpretation of Civil Rights law, the SLRC undertook cases on behalf of Federal Aviation Administration workers in Florida, utility company employees in South Carolina, workers at a DuPont plant in Virginia (the ""DuPont Seven""), and Cherokee students in Alabama. Federal judges have shown almost universal hostility to legal protection for ""Confederate Southern Americans,"" but Lyons and the SLRC still advocate for clients claiming an increase in the number of job-related persecution of ""Confederate Southern Americans"" and a paucity of legal protection in the workplace save for cases that come under the Civil Rights Act of 1964. Lyons has noted that ""Republican judges are adamantly opposed to any extension of the Civil Rights Act of 1964 and Democratic Judges are hostile to almost all things Confederate.""[citation needed] The SLRC has supported the rights of students to use and display Confederate symbols. Its most significant victory was Castorina v. Madison County Schools, in which the United States Court of Appeals for the Sixth Circuit in 2001 overturned a federal district court's ruling in favor of a Kentucky school board's ban on student displays of Confederate symbols, and remanded the case for further proceedings.[3] In 2006 the Castorina decision led to an out-of-court award of damages for Jacqueline Duty, an SLRC client who had sued her own school board after she was barred from attending her high school prom in a Confederate flag-patterned evening gown. The SLRC has drawn fire from the Southern Poverty Law Center (SPLC), which has frequently attacked the SLRC by citing Lyons' pre-SLRC defense of some controversial right-wing figures such as Aryan Nations members, White Aryan Resistance founder Tom Metzger[4] and Lyons' 1990 marriage to the sister of jailed Order defendant David Tate. The SPLC also criticized Lyons tie to Deborah Davila's FBI espionage case.[5] In addition the SPLC has called Southern Legal's fundraising practices ""deceitful"" citing, for example, ""the SLRC Web site detailed two disputes under a headline that read 'Cases Pending,' implying that the SLRC represented the parties involved. In both cases, the plaintiff's families say Lyons actually did very little for them.""[6] In 1997 Kirk Lyons and SLRC director Neill Payne befriended Asheville NAACP president H. K. Edgerton. In response, the NAACP deprived Edgerton of his presidency. Edgerton became a ""born-again Confederate"" and for a time served as an SLRC director and chairman of the SLRC's board of advisors.[citation needed] The SLRC has called on ""Confederate Southern Americans"" to identify as a separate race on the 2010 United States Census form.[7]"
Speechless: Silencing the Christians,/wiki/Speechless:_Silencing_the_Christians,"Speechless: Silencing the Christians (also known as Silencing Christians) is a 2008 documentary series produced by the American Family Association (AFA) and hosted by commentator Janet Parshall; the 13-episode series was first carried by the Inspiration Network. The documentary series describes the AFA's opposition to what it claims to be ""political correctness"", and claims that various factors, such as separation of church and state, hate crime laws, the Fairness Doctrine, and the ""gay agenda"", are threatening the existence of Christianity (though the Fairness Doctrine had not been enforced for 21 years at the time of broadcast). The DVD release featured an exclusive fourteenth episode. The series was also edited into the form of a one-hour special in 2009, causing controversy for an airing on a Tampa, Florida television station. In 2009, the AFA released a one-hour special version of the program, which deals with their stance against what they call the ""homosexual agenda"", including interviews with ex-gay people. This special was produced mainly for commercial television stations under the purview of paid programming time, where the AFA purchases the airtime from the station in the same way as a regular infomercial broker would do. The special also served as a tie-in to the Donald Wildmon book of the same name, which was released in early 2009.[1][2] In the Tampa Bay television market, Media General's NBC affiliate WFLA-TV (channel 8) aired the special on June 27, 2009, on the same day that the annual St. Petersburg pride parade was held, on the weekend of the 40th anniversary of the Stonewall riots. Prior to the telecast, the station was swamped with numerous phone calls and e-mails against the station showing the program.[3] After the program ended, the station logged hundreds of phone calls and over 1000 e-mails, all in protest against the show. General Manager Mike Pumo refused to elaborate on the decision, other than saying that the show's content did not ""raise the red flag"" during pre-screening. Stratton Pollitzer, deputy director of homosexual rights organization Equality Florida, considered the show hate speech, saying, ""I think this program is a piece of homophobic propaganda and it has no place on a major network like NBC.""[3] NBC is merely the station's affiliated network, and holds no role in endorsing or being able to determine the station's content outside of network programming. Brian Winfield, Equality Florida's director of communications, said the special ""paints the entire gay community as being anti-Christian and that's just not true. On a day when tens of thousands of Tampa residents and their friends gathered together to celebrate diversity and pride, WFLA chose to profit from screening a show that was dehumanizing to gay people.""[4] On July 15, 2009, reports ranging from about 70 to almost 100 protesters gathered outside of WFLA's studios to protest the airing of the special; the protesters carried red flags (in a reference to General Manager Pumo's remark), as well as signs that parodied the station's moniker, reading News Channel H8 (hate). John Schueler, a Media General executive responsible for WFLA, wrote in a prepared statement: ""Our overriding mission is to provide platforms for the broadest points of view and be responsible to the community we serve. We understand that doing so can cause strong disagreement. We screened this program and ran a disclaimer before and after it ran noting that this does not reflect the views of WFLA.""[5][6] City of Tampa councilman John Dingfelder asked WFLA to apologize for carrying the program, saying that ""This community wouldn't accept a racist infomercial, it wouldn't accept an anti-Semitic infomercial and we shouldn't accept a homophobic infomercial.""[7]"
Donald Sterling,/wiki/Donald_Sterling," Donald T. Sterling (born Donald Samuel Tokowitz;[1] April 26, 1934) is an American attorney and businessman who was the owner of the San Diego / Los Angeles Clippers professional basketball franchise of the National Basketball Association (NBA) from 1981 to 2014. In April 2014, Sterling was banned from the NBA for life and fined $2.5 million by the league after private recordings of him making racist comments were made public.[2] In May, Sterling's wife Shelly reached an agreement for the Sterling Family Trust to sell the Clippers for $2 billion to Steve Ballmer, which Sterling contested in court. The NBA Board of Governors approved the sale of the Clippers to Ballmer in August 2014.[3][4] Sterling settled his lawsuit against the NBA in November 2016[5] and remains active in Los Angeles real estate.[6] Donald Sterling was born Donald Tokowitz on April 26, 1934, in Chicago.[7][8][9][10] His family moved to the Boyle Heights area of Los Angeles when he was two years old. His parents, Susan and Mickey, were Ashkenazi Jewish immigrants.[7][11][12][13] He attended Theodore Roosevelt High School in Los Angeles, where he was on the school's gymnastics team and served as class president; he graduated in 1952. He then attended California State University, Los Angeles (class of 1956) and Southwestern University School of Law (class of 1960) in Los Angeles.[14] When he was 25, he and his wife Shelly changed their surname to ""Sterling"", filing a formal petition to do so on December 9, 1959.[15] They cited the difficulty among his peers to pronounce ""Tokowitz"" and the belief that there would be financial benefits for the change.[15] In 1961, Sterling started his career as a divorce and personal injury attorney, building an independent practice when Jews had fewer opportunities at prestigious law firms.[16] His biggest ventures were in real estate, which he began when he purchased a 26-unit apartment building in Beverly Hills. In the 1960s, Sterling also purchased Lesser Towers, a pair of large apartment buildings in the Westwood area of Los Angeles,[17] and renamed them the Sterling Towers (now the Sterling International Towers). In 1976, he leased the California Bank Building on Wilshire Boulevard in Beverly Hills and renamed it Sterling Plaza. The Art Deco landmark was built in 1930 by MGM cofounder Louis B. Mayer. In 2000, Sports Illustrated senior writer Franz Lidz revealed that Sterling had a 99-year lease with the Mayer estate that required him to pay a relatively small annual fee and 15% of any rental income, which was why Sterling had remained the sole tenant. ""With no other tenant,"" Lidz reported, ""the Mayer estate faces another 75 years with virtually no income from its Sterling Plaza property. By sitting and waiting, Sterling may force a fire sale.""[7][18] As of April 2014, he owned 162 properties in Los Angeles.[19] Sterling and Los Angeles Lakers majority owner Jerry Buss were each indirectly responsible for the other owning his respective NBA franchise.[7] The first instance came in 1979, when Buss used the money he made from selling a portion of his apartment buildings to Sterling (worth $2.7 million), which covered the remaining balance in purchasing the Lakers, the Kings hockey team, and The Forum arena from Jack Kent Cooke for $67 million. Two years later, Buss suggested that Sterling purchase his own NBA franchise, and Sterling bought the San Diego Clippers for $12.5 million. At his introductory news conference in San Diego, Sterling vowed to ""spend unlimited sums"" to build the Clippers into a contender, and he embarked on a county-wide marketing campaign featuring his smiling face on billboards and the backs of buses.[7][20][21][22] The seminal ads read: ""My Promise: I will make you proud of the Clippers"".[21] Unlike Buss' instant success with the Lakers (including winning an NBA championship in his first season as owner, 1979–80), Sterling and his Clippers struggled through many lackluster seasons, and they did not have their first winning season until the 1991–92 season, 11 years into his ownership. In Sterling's 33 years of owning the Clippers through 2013–14, the Clippers lost 50 or more games 22 times, 60 or more on eight occasions, and 70 games once. Their 9–41 record in the lockout-shortened 1998–99 season projected to another 60-loss season.[23] The NBA in 1982 fined Sterling $10,000, the largest sum ever levied against an owner at the time, after he commented that he would accept the Clippers finishing in last place in order to draft an impact player like Ralph Sampson.[7][21][24] In June 1982, Sterling attempted to move the team to Los Angeles. This prompted an investigation of the Clippers by an NBA committee of six owners. In September, the group recommended that Sterling's ownership be terminated, having found that he was late in paying creditors and players.[21] Days before a league scheduled vote in October to remove Sterling, he agreed to sell the team, and the league sought buyers who would keep the franchise in San Diego. At the suggestion of David Stern, then the league's vice president, Sterling was able to maintain his position as owner, instead handing over operations duties of the franchise to Alan Rothenberg, who became the team's president. By February 1983, Stern called the Clippers a ""first-class"" franchise, and the ouster of Sterling was no longer pursued.[21][25] Encouraged by friend Al Davis' victory over the National Football League in an antitrust lawsuit that allowed him to move his Oakland Raiders to Los Angeles without league approval, Sterling moved the Clippers from San Diego to Los Angeles in 1984, despite again being denied permission from the NBA to do so.[26] The NBA subsequently fined him $25 million. He sued the league for $100 million, but dropped the suit when the league agreed to decrease the fine to $6 million.[27] Sterling was widely criticized for his frugal operation of the Clippers, due in part to a consistent history of losing seasons. The club was long considered the laughingstock of the NBA.[28][29][30][31] The Clippers moved into the Staples Center for the 1999–2000 NBA season, the same place that the Lakers were becoming perpetual contenders. In the 2005–06 season, eight years removed from their last playoff appearance, they won 47 games and finished 6th in the Western Conference to make the playoffs. This was a record for the most victories in a single season since the franchise moved to California. It was the fourth playoff appearance for the Clippers with Sterling as owner and it was also only the second winning season in his tenure. They beat the Denver Nuggets in the first round for the team's first playoff series win since 1976 before losing to the Phoenix Suns in a seven game semifinals. In the lockout-shortened 2011–2012 season they made the playoffs with the best winning percentage in their history (.606) with 40 wins in 66 games and they won their first round series against the Memphis Grizzlies, 4–3, before being swept by the San Antonio Spurs, 4–0, in the conference semi-finals. Led by Blake Griffin and Chris Paul, the Clippers posted two more winning campaigns in 2012–13 and 2013–14, setting new franchise records for regular-season wins with 56 and 57, respectively, but won just one playoff series combined. Sterling rebuffed numerous offers from other cities to relocate the Clippers and was steadfast in his refusal to move the team out of Los Angeles, let alone sell the team. While the team played a few games in Anaheim in the Arrowhead Pond (now the Honda Center) for a few years before the Staples Center opened, he was not willing to move the team there permanently. In later years, he showed an increased willingness to spend. In 2003, Sterling signed Elton Brand to a six-year, $82 million deal, the biggest contract in franchise history. He matched the contract the Utah Jazz offered restricted free agent Corey Maggette: a deal worth $45 million over six years. The Clippers signed higher-priced veteran free agents, such as Cuttino Mobley in 2005, Tim Thomas in 2006, and Los Angeles native Baron Davis in 2008. In another first during the Sterling tenure of Clippers ownership, the team gave a four-year contract extension to head coach Mike Dunleavy Sr., as well as a five-year extension to center Chris Kaman. Both extensions took effect starting in the 2007–08 NBA season. Under Sterling's ownership, only Dunleavy and Bill Fitch (1994–1998) lasted four seasons or more as Clippers head coach; as of the 2009–10 NBA season, Dunleavy entered his seventh season as Clippers head coach, by far the longest tenure in franchise history, but was relieved of his coaching duties on February 4, 2010. Dunleavy was also the club's general manager, but was fired from that position a month later.[32] The Clippers accused Dunleavy of defrauding the team, and he sued the club for money owed on the remainder of his contract; an arbitrator ordered the Clippers to pay Dunleavy $13 million in 2011.[33] The Clippers also went to court with former head coaches Fitch and Bob Weiss.[34] Weiss, who signed a three-year contract but was fired in 1994 after one season, had to sue to receive money that was still owed him.[34][35][36] In 2001, the Clippers sued the 63-year-old Fitch, whom the team had fired in 1998, after they stopped paying him for failing to seek employment to reduce the team's obligation for payment.[34][36][37] The suit reached an undisclosed settlement before going to court.[36] Sterling spent $50 million to build a state-of-the-art practice facility and team headquarters in Los Angeles' Playa Vista mixed-use development neighborhood. This followed the lead of several other NBA franchises, including the Lakers, Sacramento Kings, Cleveland Cavaliers, and Detroit Pistons, in having their own facility dedicated exclusively for team use. The facility was completed and opened in September 2008, in time for the start of the team's training camp. The team previously practiced at a local health club in suburban El Segundo, and before that at Los Angeles Southwest College.[citation needed] Sterling's ownership was viewed critically.[7]ESPN The Magazine in 2009 named the Clippers the worst franchise in professional sports.[38][39] Uncharacteristic for an owner, Sterling in 2010 heckled players on his own team—with Baron Davis receiving the harshest treatment—while the owner sat courtside during home games.[40][41] In late April 2014, following news of racial remarks, Sporting News described Sterling as ""one of the worst owners in basketball for decades"",[42] while The New York Times and Forbes called him the ""worst owner"" in sports,[38][43] and an analyst noted that under Sterling's ownership, from his purchasing the Clippers in 1981 through 2013–14, the Clippers achieved the worst winning percentage in all four major American sports leagues.[44] On April 25, 2014, TMZ Sports released a recording of a conversation between Sterling and his mistress, V. Stiviano (born María Vanessa Perez, also known as Monica Gallegos, Vanessa Perez, and Maria Valdez).[45][46] In the recording from September 2013, a man confirmed to be Sterling was irritated over a photo Stiviano had posted on Instagram, in which she posed with Basketball Hall of Fame player and Laker great Magic Johnson.[47][48] Sterling told Stiviano, who herself is part African-American: ""It bothers me a lot that you want to broadcast that you're associating with black people,"" and, ""You can sleep with [black people]. You can bring them in, you can do whatever you want,"" but ""the little I ask you is ... not to bring them to my games.""[49][50] The recording received national media coverage[51] and Sterling retained Newport Beach-based attorney Bobby Samini as his lead counsel in litigation with the NBA, TMZ, and Stiviano.[52] Clippers president Andy Roeser issued a statement the following day,[53] indicating that Stiviano was being sued by the Sterling family and had ""told Mr. Sterling that she would ""get even"" with him.[54] A month earlier, Sterling's wife had sued Stiviano for the return of a $1.8 million Los Angeles duplex, a Ferrari, two Bentleys, a Range Rover, and $200,000 cash she said her husband bought for Stiviano.[55][56] Sterling's comments affected the NBA, a league with predominantly black players.[57] On April 26, the team held a meeting to discuss the incident. Both coaches and players expressed anger toward the comments, and they briefly raised the possibility of boycotting Game 4 of their series against the Golden State Warriors on April 27 before deciding against it.[58] Instead, players protested Sterling's remarks by wearing their shirts inside-out in order ""to obscure any team logo"" during their pre-game huddle.[59] On April 28, players of the Miami Heat wore their uniform tops inside-out to show solidarity with the Clippers. LeBron James commented on the situation, ""There's no room for Donald Sterling in the NBA."" The owner of the Miami Heat, Micky Arison, also called the allegations ""appalling, offensive and very sad"".[60] NBA's Kevin Johnson, Kareem Abdul-Jabbar, Magic Johnson, Charles Barkley, Shaquille O'Neal, Kobe Bryant, and Michael Jordan also condemned Sterling's remarks.[61][62] The Los Angeles chapter of the National Association for the Advancement of Colored People (NAACP) cancelled its plans for the following month to award Sterling for a second time with its lifetime achievement award.[63] President Barack Obama characterized the recording of Sterling as ""incredibly offensive racist statements"".[64] Chumash Casino, the Clippers' most visible sponsor during the prior four seasons, ended its relationship with the team, as did sponsors CarMax, Virgin America, and others.[65] On April 29, 2014, UCLA announced that it was rejecting a $3 million gift from Sterling.[66] On April 29, NBA commissioner Adam Silver announced that Sterling had been banned from the league for life and fined $2.5 million, the maximum fine allowed by the NBA constitution.[47][67] Silver stripped Sterling of virtually all of his authority over the Clippers, and banned him from entering any Clippers facility. He was also banned from attending any NBA games.[47][68] The punishment was one of the most severe ever imposed on a professional sports team owner.[48] Moreover, Silver stated that he would move to force Sterling to sell the team, based on a willful violation of the rules, which would require the consent of three-quarters, or 22, of the other 29 NBA team owners.[69] In his first public comments in nearly two weeks after his ban from the NBA, Sterling appeared on CNN with Anderson Cooper on May 11 to apologize, saying he was ""not a racist"", and ask for forgiveness.[70] He said he was ""baited"" by Stiviano into making the offensive comments.[70] In the interview, Sterling criticized Magic Johnson's character and his battle with HIV.[71] In response to Sterling, Silver apologized for the NBA to Johnson ""that he continues to be dragged into this situation and be degraded by such a malicious and personal attack"".[72] Sterling's wife, Shelly, had co-owned the team with him since 1983, and she had served as one of the team's two alternate governors.[73] While she was not included in the NBA's ban on Sterling,[74] the league stated that ""if a controlling owner's interest is terminated by a .mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}3⁄4 vote, all other team owners' interests are automatically terminated as well"".[75] In response to the NBA's decision, Sterling's attorney Bobby Samini called the NBA a ""band of hypocrites,"" citing previous discriminatory conduct by the NBA, and suggested the organization ""take a close reflection at their own conduct.""[76] The NBA formally charged Sterling with damaging the league with his comments from both the TMZ recording and the CNN interview, and scheduled a hearing to begin on June 3, after which the league could vote to terminate the Sterlings' ownership.[77][78] On May 23, Shelly Sterling said her husband had authorized her to negotiate the sale of the team.[79] On May 29, she reached a deal, pending NBA approval, to sell 100% of the Clippers to former Microsoft CEO Steve Ballmer for $2 billion.[80] Shelly also agreed not to sue the NBA and to indemnify the league against other suits related to the case, including any initiated by her husband. The NBA responded by cancelling its hearing to consider stripping the Clippers from the Sterlings. Sterling disavowed having given his wife authorization to sell the team, denied all charges, and refused to sell the Clippers. He called the penalties ""draconian"" and referred to the process as a ""sham"".[81] He then sued the NBA for $1 billion, alleging it had violated both antitrust laws and his constitutional rights.[82] On June 4, 2014, attorney Maxwell Blecher announced that Sterling had decided to drop the lawsuit against the NBA, and had agreed to allow the proposed $2 billion sale of the Clippers to Ballmer. The sale would be approved pending a majority vote of league owners.[83] On June 9, Blecher said Sterling had withdrawn support for the sale, and would resume the lawsuit.[84] However, Shelly was granted a trial in probate court that began on July 7 to allow her to proceed with the sale as sole trustee; she contended that three doctors reported that Sterling was suffering from Alzheimer's disease and lacked the mental capacity to be a trustee.[85] Closing arguments were scheduled for July 28. The NBA was scheduled to vote on the sale to Ballmer on July 15, the same day the deal was set to expire unless Ballmer granted an extension.[86] On July 23, Sterling sued his wife, the NBA, and Silver for damages, alleging that they violated corporate law and defrauded him in order to sell the team to Ballmer. Sterling also sought an injunction to freeze the sale.[87] On July 28, the probate court ruled in Shelly's favor, and granted her request for an order to permit the sale to be completed regardless of any intervention by an appellate court.[88] Ballmer's $2 billion purchase of the team closed on August 12, and Shelly received the titles ""Clippers Number One Fan"" and ""owner emeritus"" as part of the sale agreement.[89] As of April 2015[update], half of the $2 billion paid by Ballmer was held in an escrow account controlled by the NBA pending the conclusion of Sterling's lawsuit over the sale of the team.[90] One of Sterling's critics among the team owners, Atlanta Hawks majority stockholder Bruce Levenson,[91] also left the NBA in September 2014, as an indirect result of the incidents and lawsuits. Levenson reported a racially insensitive e-mail (sent two years prior) to the NBA, as the Sterling case made Levenson feel remorseful for behaving in a manner he described as similar.[92] Nearly a decade later, another critic among the team owners, Phoenix Suns majority owner Robert Sarver,[93] faced a season-long suspension during the entire 2022–23 NBA season for behaviors as team owner that were discovered through an independent investigation during the 2021–22 NBA season that were compared to Sterling's behavior in terms of what went on behind closed doors during that time.[94] Many people and organizations relating to the Suns and NBA (including Chris Paul, who had played for both the Clippers under Sterling and the Suns under Sarver by then) thought the season-long suspension and $10 million fine from Sarver was too lenient by comparison to his actions and felt that Sarver deserved a lifetime ban from the NBA similar to Sterling based on the initial report on Sarver. Sarver eventually would sell the Suns and Phoenix Mercury (the partner team of the Suns in the WNBA) later on in the 2022–23 season due to the loud backlash against the initial ruling,[95] ultimately selling his majority shares of the Suns and Mercury to Mat Ishbia and Justin Ishbia for a record-high asking price of $4 billion during the period of time between December 2022 and February 2023.[96][97] U.S. District Judge Fernando M. Olguin dismissed Sterling's 2014 lawsuit over the sale of the team in March 2016. The judge assailed it as ""plainly insufficient"" and ""clearly implausible."" Sterling appealed the decision, but his attorneys did not file an opening brief by the deadline. The matter concluded with a three-page motion to voluntarily dismiss the case.[98] In 2017, a judge ruled that two law firms could move forward[needs update] with their lawsuit to collect more than $270,000 in legal fees allegedly owed by Donald and Shelly Sterling, stemming from the 2014 probate action that cleared the way for the sale of the Clippers.[99] In February 2003, the Housing Rights Center of Los Angeles (HRC) filed a housing discrimination case against Sterling on behalf of 18 tenants. The lawsuit featured several racist statements allegedly made by Sterling to employees, such as that ""black people smell and attract vermin"" and ""Mexicans just sit around and smoke and drink all day"",[100][101] as well as Sterling's alleged intent to rent only to Korean tenants because ""they will pay the rent and live in whatever conditions I give them"".[101] Part of the HRC case's resolution included U.S. District Judge Dale Fischer awarding the plaintiffs' attorney $4.9 million in attorneys fees. While the final terms for the plaintiffs were confidential, the judge said the fees were justified as the settlement obtained by the plaintiffs against Sterling was one of the largest of its kind and the public benefit terms were significant and wide-ranging.[citation needed] In 2006, the U.S. Department of Justice sued Sterling for housing discrimination for using race as a factor in filling some of his apartment buildings. The suit charged that Sterling refused to rent to non-Koreans in the Koreatown neighborhood and to African Americans in Beverly Hills. In November 2009, ESPN reported that Sterling agreed to pay a fine of $2.7 million to settle the lawsuit.[102] In February 2009, the Clippers were sued in L.A. Superior Court by former longtime Clippers executive Elgin Baylor for wrongful termination and employment discrimination on the basis of age and race.[103][104][105] The lawsuit alleged that co-defendant Sterling told Baylor that he wanted to fill his team with ""poor black boys from the South and a white head coach"".[105] The suit also alleged that Mike Dunleavy Sr., the head coach, ""was given a four-year, $22-million contract ... [as he was a] Caucasian"" while Baylor's salary had ""been frozen at a comparatively paltry $350,000 since 2003"".[103] Baylor later dropped the race accusation. The case went to trial in March 2011, with the jury ruling unanimously in favor of the Clippers and Sterling.[106] In 1996, Christine Jasky, a property management consultant for Sterling who also did work for the Clippers, sued Sterling for sexual harassment, claiming she quit her job after he repeatedly offered her money for sex, and asked her to recruit sexual partners for him.[107][108] Sterling countersued, and the two eventually reached a confidential settlement in 1998.[108] Sumner Davenport, a property supervisor for Sterling who was fired in 2002, sued him in 2003 for sexual harassment for ""unwanted and offensive physical conduct"".[109] She lost the case at a jury trial two years later.[108][110] Court documents indicate that Davenport was a property supervisor based in Sterling's Beverly Hills office, with the responsibilities of overseeing several of his apartment buildings. In her case, she asserted she was fired for her complaints against and refusing to comply with his racially discriminating and abusive behavior against tenants, his illegal eviction process, as well as his offensive physical conduct against her. Court records indicate Sterling's organization denies firing her.[111] In 1955, Sterling married Rochelle (""Shelly"") Stein, with whom he had three children: Scott, Chris, and Joanna. Joanna's husband, Eric Miller, served as the Clippers' director of basketball administration, voluntarily leaving after Sterling sold the team.[112] Sterling had an extra-marital relationship with a woman named Alexandra Castro. Seeking the return of a house she was living in, Sterling sued her in 2003 after their relationship ended.[43][113] Castro, in 1999, had signed a contract that gave Sterling protection from her seeking palimony, which divides assets between unmarried couples. Their agreement read that Sterling ""is happily married, has a family and has no intention of engaging in any activity inconsistent with his domestic relationship"".[114][115] In the proceedings, Castro stated that Sterling consulted her on Clippers personnel decisions.[43] Sterling and Castro reached a confidential settlement out of court in 2004.[113] Sterling and Shelly became estranged at the end of 2012, when he moved to a mansion in Beverly Hills, California, after she kicked him out of their beach house in Malibu, California, following a family dispute during which he was arguing with a mistress on the phone.[26] A week later, Sterling's son Scott was found dead on New Year's Eve, having died of an accidental narcotic drug overdose at the age of 32.[26][116] On August 5, 2015, Sterling's attorney Bobby Samini confirmed to KABC-TV that Sterling filed for divorce from his wife Shelly.[117][118] In March 2016, Samini informed the Los Angeles Times that ""notwithstanding all the difficult events of the last two years, the Sterlings have resolved their differences"" and decided not to proceed with their divorce.[119] In 2012, Sterling began treatment for prostate cancer.[26][120] By May 2014, according to multiple doctors Sterling was in the early stages of Alzheimer's disease.[121] He was deemed mentally unfit to continue to lead the financial affairs of the Sterling Family Trust, clearing the way for his wife Shelly to sell the Los Angeles Clippers on his behalf despite his protests.[122]"
Ugly law,/wiki/Ugly_law,"From 1867 to 1974, various cities of the United States had unsightly beggar ordinances, retroactively named ugly laws.[1] These laws targeted poor people and disabled people. For instance, in San Francisco a law of 1867 deemed it illegal for ""any person, who is diseased, maimed, mutilated or deformed in any way, so as to be an unsightly or disgusting object, to expose himself or herself to public view.""[2][1] Exceptions to public exposure were acceptable only if the people were subjects of demonstration, to illustrate the separation of disabled from nondisabled and their need for reformation.[3]: 47  The Charity Organization Society suggested that the best charity relief would be to investigate and counsel the people needing assistance instead of providing them with material relief.[4] This created conflict in people between their desire to be good Christians and good citizens when seeing people in need of assistance. It was suggested that the beggars imposed guilt upon people in this way.[3]: 37  ""Pauperism is a disease upon the community, a sore upon the body politic, and being a disease, it must be, as far as possible, removed, and the curative purpose must be behind all our thought and effort for the pauper class.""[4] Similar to what Slocum said, other authors suggested that giving charity to beggars without knowing what was to be done with the funds, was as ""culpable as one who fires a gun into a crowd"".[5] The term ""ugly laws"" was coined in the mid-1970s by detractors Marcia Pearce Burgdorf and Robert Burgdorf, Jr.[3]: 9  In 1729 England, punishment was sometimes suggested for people with physical disabilities, whether they were born with disabilities or acquired later in life, who appeared in public.[3]: 4  Ugly laws in the United States arose in the late nineteenth century. During this period, urban spaces underwent an influx of new residents, which placed strain on the existing communities. The new residents were sometimes impoverished. This meant large numbers of people who were strangers to each other now occupied closer quarters than they had in small towns, where such local institutions as schools, families, and churches helped moderate social relations.[6][7] As a reaction to this influx of people who were impoverished, ministers, charitable organizers, city planners, and city officials across the United States worked to create ugly laws for their community.[1] The language of the unsightly beggar ordinances pertained to hiding the parts of the person that may appear disabled or diseased. This includes any movements that would indicate a disability or disease, like limping.[3]: 9–10  The first American ordinance pertaining to preventing people with disabilities from appearing in public was passed in 1867 in San Francisco, California. This ordinance had to do with the broader topic of begging.[3]: 2  It is noted that people who were perhaps in need of money traveled to California to ""strike it rich"" during the California Gold Rush. When they did not find themselves wealthy, they remained in California. Letters and documents from the period just after the California Gold Rush note the large number of ""insane"" people wandering the streets.[3]: 25 [8][9] Helper (1948) even refers to the ""insane"" people as ""pitible nuisance"" and remarked that they were allowed in public with no one to care for them.[9] New Orleans, Louisiana had a similar law police were strictly enforcing in 1883. A New Orleans newspaper reported on the City adopting a tough stance on begging as other cities in the United States had.[3]: 3  Portland, Oregon enacted an ugly law in 1881.[10] The Chicago ordinance of 1881 read as follows: Any person who is diseased, maimed, mutilated, or in any way deformed, so as to be an unsightly or disgusting object, or an improper person to be allowed in or on the streets, highways, thoroughfares, or public places in the city, shall not therein or thereon expose himself or herself to public view, under the penalty of a fine of $1 for each offense (Chicago City Code 1881)[3]: 2  The fine of $1 equates to more than $30 in 2023. In most cities, punishments for violating an ugly law ranged from incarceration to fines of up to $50 for each offense. In May 1881, the unsightly beggar ordinance went into effect in Chicago, Illinois. It was created by Chicago alderman James Peevey.[3]: 1  Peevey is quoted in the Chicago Tribune from May 19, 1881, saying of the ordinance, ""Its object is to abolish all street obstructions.""[3]: 1  Ugly laws identified groups of people as disturbing the flow of public life and forbid them from public spaces. Such people, deemed ""unsightly"" or ""unseemly,"" were usually impoverished and often beggars. Thus ugly laws were methods by which lawmakers attempted to remove the poor from sight.[3]: 31–32  Laws similar to Chicago's followed in Denver, Colorado and Lincoln, Nebraska in 1889. At some time from 1881 to 1890 an ugly law was enacted in Omaha, Nebraska.[10] Additionally, ugly laws were sparked by the Panic of 1893. These included Columbus, Ohio in 1894, and in 1891 for the entire state of Pennsylvania. Pennsylvania's was different as it contained language applying to cognitive disability as well as physical disability.[3]: 3  An attempt was made at introducing ugly laws in New York, but it failed in 1895. Initial drafts in New York were similar to those in Pennsylvania as to include cognitive disabilities. Reno, Nevada instituted an ordinance before 1905.[3]: 3 Los Angeles, California attempted to pass an ordinance in 1913. In 1902, an ugly law similar to that of the United States was enacted in the City of Manila in the Philippines.[3]: 5  This law was similar to those of the United States, being written in English and during a time when Manila was under American control, and included the common phrasing ""no person who is diseased"". This was one of the first ordinances to be written under American control. Other ordinances dealt with hygiene reform and considered unsightly beggars part of this initiative.[3]: 5  The last ugly laws were repealed in 1974.[11] Omaha, Nebraska repealed its ugly law in 1967, yet had an arrest of a person for violating the unsightly beggar ordinance documented in 1974.[1] Columbus, Ohio repealed its law in 1972. Chicago was the last to repeal its ugly law in 1974.[12] People charged under the ugly laws were either charged a fine or held in jail until they could be sent to the poor house or work farm. The wording in the San Francisco ordinance indicates violators will be sent to the almshouse. This connects with the Victorian Era poor law policy.[3]: 2  Historian Brad Byrom noted ugly laws have been unevenly and rarely enforced, being disregarded by police.[1] The first recorded arrest pertaining to ugly laws was Martin Oates in San Francisco, California in July 1867. Oates was a former Union soldier during the American Civil War.[3]: 2  The ugly laws did not restrict performances of people with disabilities for the purpose of entertainment or eliciting disgust, but rather restricted people with disabilities from mingling with the general public.[3]: 101  Use of the ugly laws to control the use of public spaces by people with disabilities was still occurring after the signing of the Rehabilitation Act of 1973.[3]: 280  Racism also played a role in the establishment and enforcement of ugly laws. In 1860s San Francisco, Chinese immigrants and their descendants were unlawfully quarantined en masse to prevent spread of disease and epidemics.[3]: 28  The last recorded arrest related to an ugly law was in 1974, under an Omaha, Nebraska ordinance.[13][14][1] In this instance, the man arrested was homeless and the officer arresting him did so under the guise of the ugly law as the man had visible scars and marks on his body. The judge, Walter Cropper, and assistant prosecutor, Richard Epstein, in this case noted there was no legal definition for ugly and criminal prosecution would demand proving someone is ugly. The result was that the city prosecutor, Gary Bucchino, did not file charges. He noted that while the law was still active, this person did not meet the definition.[3]: 6  Ugly laws prevented some people with physical disabilities from going out in public at all.[3]: vii, 4  British scholar Stuart Murray argues that the ""civil contagion"" of proliferation of ugly laws is peculiarly American: ""Disability disturbs, and it disturbs the sense of self in U.S. contexts in special ways.""[3]: 4  Jacobus tenBroek (1966) argued that the limitations imposed on a person with a disability had little to do with actual disability, but rather ""society's imagined thoughts of disability difficulties and risks"".[15] In 1975, Marcia Pearce Burgdorf and Robert Burgdorf, Jr. wrote about the unsightly beggar ordinances in their newspaper article, ""A History of Unequal Treatment: The Qualifications of Handicapped Persons as a 'Suspect Class' under the Equal Protection Clause"".[1][11] In this article, the term ""ugly laws"" was created and used, having been inspired by the newspaper article title regarding the Omaha arrest in 1974. It was an act of advocacy. John Belluso's play The Body of Bourne has a scene in which Randolph Bourne was confronted in Chicago due to an ugly law.[3]: 12  While this is a fictional occurrence, the depiction of the law indicates the impact on disability history.[3]: 12  In 1980, while on tour in Europe, a performer with the San Francisco-based Lilith Women's Theatre, Victoria Ann Lewis, delivered a monologue about the difficulty of people with disabilities finding work due to the social idea that people with disabilities should hide or be in the circus. Lewis believed herself to be denied admission to a theater school in New York City due to her limp. She noted they tried to persuade her to take a position behind the scenes. She felt this was due to the ugly laws and that she would not be able to perform in some cities.[3]: 12–13  There was a connection between ugly laws and ""public hygiene management schemes"" such as segregation, eugenics, institutionalization.[3]: 15 [16][17][18]: 30  The ugly laws had an impact on what society considers rehabilitation. ""In the rehabilitationist program the aim is in one sense to make disability vanish,"" ... to ""cause the disabled to disappear and with them all that is lacking, in order to assimilate them, drown them, dissolve them in the greater and single social whole."" People with disabilities were not allowed to publicly beg for food or money to support their needs as a person, but it was acceptable to display themselves commercially in order to beg for a cure or salvation from their disability.[3]: 254  Relationships, reproductive rights and individual right to life were also impacted by the ugly laws and charitable philosophy during this period. Policy makers discussed preventing people with disabilities from marrying and having children. The policy makers suggested this was to prevent the children their union would produce from tainting society's heredity pool. Charity must ""do what it can to check the spreading curse of race degeneration"". People involved with charitable policy suggested that while euthanasia would be a release for the person struggling with their disabilities, it also went against the moral principles taught by religion.[3]: 48–50 [18]: 30  The repeals of ugly laws followed soon after the passage of the Rehabilitation Act of 1973 and its Section 504, and the 1990 passage of the Americans with Disabilities Act further stopped any possibility of a recreation of ugly laws. Fredman (2011) comments: ""Individuals with disabilities are a discrete and insular minority who have been faced with restrictions and limitations, subjected to a history of purposeful unequal treatment, and relegated to a position of political powerlessness in our society, based on characteristics that are beyond the control of such individuals and resulting from stereotypic assumptions not truly indicative of the individual ability of such individuals to participate in, and contribute to, society.""[19]"
George Wallace (Georgia politician),/wiki/George_Wallace_(Georgia_politician),"George Wallace was an African-American state senator from Georgia during the Reconstruction Era.[1] He represented Hancock County, Baldwin County, and Washington County. He was a Republican.[1] On September 12, 1868, the Georgia State Senate voted to exclude members with mixed heritage. The Georgia House had already kicked out their African American members.[1][2][3][4] During the American Civil War, Wallace was reported to have been a body servant for Captain Howard Tinsley, to have been at Appomatox when Robert E. Lee surrendered, and to have ridden General Philip Cook's war-horse ""Old Whitey"" back to family members of its owner.[5] 
 This article about a politician from the U.S. state of Georgia is a stub. You can help Wikipedia by expanding it."
Weathering hypothesis,/wiki/Weathering_hypothesis,"The weathering hypothesis was proposed to account for early health deterioration as a result of cumulative exposure to experiences of social, economic and political adversity. It is well documented that minority groups and marginalized communities suffer from poorer health outcomes.[1] This may be due to a multitude of stressors including prejudice, social alienation, institutional bias, political oppression, economic exclusion and racial discrimination.[2] The weathering hypothesis proposes that the cumulative burden of these stressors as individuals age is ""weathering,"" and the increased weathering experienced by minority groups compared to others can account for differences in health outcomes.[3] In recent years, the biological plausibility of the weathering hypothesis has been investigated in studies evaluating the physiological effects of social, environmental and political stressors among marginalized communities.[4] This has led to more widespread use of the weathering hypothesis as a framework for explaining health disparities on the basis of differential exposure to racially based stressors.[5] Researchers have also identified patterns connecting weathering to biological phenomena associated with stress and aging, such as allostatic load, epigenetics, telomere shortening, and accelerated brain aging.[6][7][8][9] The weathering hypothesis was initially formulated by Dr. Arline T. Geronimus to explain the poor maternal health and birth outcomes of African American women that she observed in correspondence with increased age. While working part-time at a school for pregnant teenagers in Trenton, New Jersey, Geronimus first noticed that the teens who came to the school tended to have far more health problems than her classmates at Princeton University. She thus began to wonder whether the health conditions of the teens at that clinic may have been caused by their environment.[10] Subsequent research on the disparity in maternal health between African American and white women led Geronimus to propose the weathering hypothesis. She proposed that the accumulation of cultural, social and economic disadvantages may lead to earlier deterioration of health among African American women compared to their non-Hispanic, white counterparts.[11] Geronimus specifically chose the term weathering as a metaphor for the effects she perceived that exposure to stress was having on the health of marginalized people.[10] While the weathering hypothesis was initially proposed based on observations of patterns in maternal health, academics have expanded its application as a framework to examine other health disparities as well.[2] While conducting research in the Department of Public Health Policy and Administration as a graduate student at the University of Michigan in 1992, Geronimus noticed a trend in disparities between the fertility of African American women versus their white counterparts.[12] She noted that while the average white woman experiences her point of highest fertility and lowest risk of pregnancy complications or neonatal mortality in her 20's and 30's, this generalization did not apply to African American women. Instead, among African American women, teen mothers are most likely to have healthy pregnancies and offspring. The data indicated a widening disparity in black-white infant mortality as maternal ages increase. Subsequently, Geronimus proposed the ""weathering hypothesis,"" which she initially conceived as a potential explanation for the patterns of racial variation in infant mortality with increasing maternal age.[11] In the context of the weathering hypothesis, individual health is dynamic and shaped over time by social, economic, and environmental influences. These social determinants dictate what different demographics are exposed to as they develop and age.[3]Racism and discrimination are two specific social determinants that lay the foundation for systemic inequality in access and upward mobility. This entrenchment of social inequities disproportionately impacts minorities and communities of color, who remain in environments of poverty that have significantly more stressors than those of wealthier, predominantly white communities.[3] These stressors—and the associated burden of coping with them—manifest as physiological responses that have detrimental effects on individual health, often leading to a disproportionately high occurrence of chronic illness and shorter life expectancy in minority communities.[13] Multiethnic studies have yielded significant data demonstrating that weathering—accumulated health risk due to social, economic and environmental stressors—is a manifestation of social stratification that systemically influences disparities in health and mortality between dominant and minority communities.[14] Maternal mortality is three to four times higher for Black mothers than white mothers in the United States.[15]Infant mortality is also twice as high for infants born to non-Hispanic Black mothers compared to infants born to non-Hispanic white mothers.[16] Additionally, there are racial disparities for negative birth outcomes like low birth weight, which has been found to influence risk of infant mortality and developmental outcomes after birth, and preterm birth.[15][17] Across all women, older maternal age is associated with higher rates of these negative outcomes during pregnancy, but studies have consistently found that rates rise more rapidly for Black women than white women.[17][18] The weathering hypothesis proposes that the accumulation of racial stress over Black women's lives contributes to this observed pattern of racial disparities in maternal health and birth outcomes that increase with maternal age.[18] Research has consistently identified an association between preterm birth and low birth weight in Black women and maternal stress caused by experiences of racism, systemic bias, socioeconomic disadvantage, segregated neighborhoods, and high rates of violent crime.[16] There is biological evidence of weathering, including the finding that Black women have shorter telomeres, a biological indicator of age, when compared with white women of the same chronological age.[16] Though increased socioeconomic status serves as a protective factor against negative birth outcomes for non-Hispanic white mothers, disproportionate rates of preterm birth and low birth weight for non-Hispanic Black mothers have been found at every education and income level.[16] The weathering hypothesis has also been used to explain this trend because upward socioeconomic mobility is associated with increased exposure to discrimination for women of color.[16] There is modest evidence supporting the effects of weathering on mothers from other minority groups, including for high birth weight outcomes among American Indian/Alaska Native women.[19] Research has started to explore whether the weathering hypothesis could also explain racial disparities in the outcomes of assisted reproductive technologies, but so far the findings are inconsistent.[20] Research shows that mental health disparities among marginalized communities exist. Daily discrimination faced by marginalized groups have been found to be associated with increased depressive symptoms and feelings of loneliness.[21] Low-income communities are more likely to have severe mental illnesses, which is frequently heightened by the inaccessibility to quality healthcare.[22] Researchers found that persisting epigenetic changes lead to increased risk of postpartum depression as a result of adverse life events and cumulative life stress among Black, Latinx, and low-income women.[23] In a study assessing African American men, experiences of racism were linked to a poorer mental health state.[24] Black Americans often show mean level differences in cognition across multiple cognitive domains compared to non-Hispanic Whites.[25][26][27] These cognitive disparities often are reduced or eliminated when factoring various social determinants of health such as stress, education quality, economic stability, or quality of healthcare.[25][26][27] Black Americans also have higher rates of Alzheimer's disease and related dementias than non-Hispanic Whites.[28] These higher rates of Alzheimer's disease might be due to the impact of more negative and pronounced social determinants of health,[29][30] including racial discrimination,[31][32] that might accelerate brain aging disproportionately in Black Americans.[9] Intersectionality is a term coined by Kimberlé Crenshaw to describe the interconnected nature of different systems of oppression, the layered effects of which can be seen in the healthcare system. Research indicates that lower class status and increased depressive symptoms are associated with higher levels of biological weathering among Black individuals in comparison to white individuals.[33] In a study exploring disparities in mental health, researchers found that Black sexual minority women reported higher frequencies of discrimination and decreased levels of social and psychological well-being than their white sexual minority women counterparts.[34] Black sexual minority women had decreased levels of social well-being and increased levels of depressive symptoms in comparison to Black sexual minority men.[34] African American women are also more likely to contract COVID-19 than African American men and white women.[35] The prevalence of medical racism and sexism (lack of quality healthcare, harmful experimentation, etc.) has led to negative relationships with healthcare systems and increased risk of negative sexual and reproductive health outcomes among African American women.[36] Existing research show how systems of oppression work together to oppress marginalized groups within the healthcare system and, as a result, these groups disproportionately experience negative health effects.[35] Aging adults experience further intersections with health, health care, and structural inequalities that exacerbates health in marginalized groups.[37] Arline Geronimus faced significant pushback for the weathering hypothesis, including from members of the medical community who believed there was a genetic or evolutionary explanation for racial differences in health outcomes.[10] There was some early criticism regarding the quality of her data, though the evidence of weathering and health disparities has grown since.[38] Others pushed back against the weathering hypothesis because its application to racial disparities in maternal health seemed to contradict what advocacy groups had been saying about the negative consequences of teen pregnancy on young mothers.[10] A further criticism of this theory believes that Geronimus and others have not sufficiently demonstrated a link between weathering and racial and gender disparities in life expectancy.[39] The weathering hypothesis was initially proposed as a sociological explanation for health disparities, but it is closely related to biological theories like the allostatic load model, which proposes that an individual's exposure to repeated or chronic stress over their lifetime has physiological consequences which can be measured through various biomarkers.[16] Research has tended to discuss allostasis and allostatic load as the molecular mechanism behind the weathering hypothesis, and Geronimus herself went on to study racial differences in allostatic load.[40] Another related theory is the life course approach, which emphasizes focus on cumulative life experiences rather than maternal risk factors as an explanation for birth outcome disparities.[41] Researchers have also been interested in studying the possibility of children inheriting the epigenetic changes which result from their mother's cumulative life stress, which could relate the weathering hypothesis with transgenerational trauma.[41][42]"
Whitecapping,/wiki/Whitecapping,"Whitecapping was a violent vigilante movement of farmers in the United States during the late 19th and early 20th centuries. It was originally a ritualized form of extralegal actions to enforce community standards, appropriate behavior, and traditional rights.[1] However, as it spread throughout the poorest areas of the rural South after the Civil War, white members operated from economically driven and anti-black biases. States passed laws against it, but whitecapping continued into the early 20th century.[2] After it was institutionalized in formal law, its legal definition became more general than the specific movement itself: ""Whitecapping is the crime of threatening a person with violence. Ordinarily, members of the minority groups are the victims of whitecapping.""[3] Whitecapping was associated historically with such insurgent groups as The Night Riders, Bald Knobbers, and the Ku Klux Klan. They were known for committing ""extralegal acts of violence targeting select groups, carried out by vigilantes under cover of night or disguise.""[4][5] The Whitecapping movement started in Indiana around 1837,[6] as white males began forming secret societies in order to attempt to deliver what they considered justice on the American frontier, independent from the state. These groups became known as the ""White Caps"". The first White Cap operations generally aimed at those who went against a community's values. Men who neglected or abused their family, people who showed excessive laziness, and women who had children out of wedlock all were likely targets.[7] As whitecapping spread into the Southern states during the 1890s after Reconstruction, a period of increasing racial violence against blacks by whites, the targets changed. In the South, White Cap societies were generally made up of poor-white farmers, frequently sharecroppers and small landowners, who operated to control black laborers and to prevent merchants from acquiring more land.[8] In the South, Whitecaps tried to force victims to abandon their home or property. Whitecapping in the South is thought to have been related to the stresses of the postbellum agricultural depression that occurred immediately after the Civil War. The South had issues with overproduction and falling crop prices.[9] With attention centered on producing cotton, the South's economy became very unbalanced. Many farmers went into debt and lost their lands to merchants through mortgage foreclosures.[10] The dispossessed turned on merchants, African-American laborers, and sometimes new white tenants. Racism contributed to the problem as well. Prosperous black men, or simply African Americans who acquired land in the South, frequently faced resentment that could be expressed violently.[11] White Caps were also part of the effort by whites to maintain white supremacy, particularly in the economy.[12]Mexican Americans were also victims of whitecapping, particularly in the state of Texas.[13] Many White Cap societies were disbanded by 1894 and their members were punished with fines. Some state governments were determined to disband the White Cap societies operating in their regions. While a segregationist, Mississippi Governor James K. Vardaman assembled an executive task force in 1904 in order to gather information about membership. He feared that the violence would drive too many black workers away from the state economy, as the number of lynchings was also high in the state.[9] Active members of the Whitecaps were found and punished by states in the early 1900s.[14] Though the negative economic effects of whitecapping violence were the main reason for state response to the lawlessness, political leaders often expressed values of Christianity as the main reason to end whitecapping.[15] In Oklahoma, both white and black settlers migrated to Oklahoma Territory after its creation and opening for settlement in 1890, with black migrant leaders such as Edward P. McCabe proclaiming Oklahoma as a new opportunity to escape racism from elsewhere in the South. After the initial wave of settlement, tensions eventually rose, especially in mixed-race towns. Whitecappers would threaten violence and encourage blacks in mixed-race areas to move out, as well as threaten black farmers who were seen as controlling too much land and competing with white farmers. As a result, Oklahoma became quite segregated, with some formerly mixed towns becoming all-white; black residents generally lived in all-black towns. Popular history and perception of the state generally omitted its black residents, especially during the 20th century, portraying only whites and American Indians due to the whitecappers' successful campaign to push black life away from view in most of the state.[16][17] Over many years, whitecapping not only affected individuals, but communities and counties as a whole. In the South, whitecapping discouraged many merchants and industrialists from doing business in the counties. Added to the thousands of murders committed by whites in lynchings of blacks, whitecapping threatened to drive away black laborers.[18] Beginning around the time of World War I, tens of thousands of rural blacks began to leave in the Great Migration, with 1.5 million leaving for northern and midwestern industrial cities by 1940. In the late twentieth century, whitecapping continued to be an issue in the South: Mississippi passed a 1972 statute criminalizing its practice. The statute reads as follows: Any person or persons who shall, by placards, or other writing, or verbally, attempt by threats, direct or implied, of injury to the person or property of another, to intimidate such other person into an abandonment or change of home or employment, shall, upon conviction, be fined not exceeding five hundred dollars, or imprisoned in the county jail not exceeding six months, or in the penitentiary not exceeding five years, as the court, in its discretion may determine.[2] Despite the different whitecapping targets, the White Caps used similar methods. Generally, the members of this society were disguised in a way that somewhat resembled that of the Ku Klux Klan (KKK), and always attacked at night. Physical attacks could include such things as whipping, drowning, firing shots into houses, arson, and other brutalities, with whipping and threats constituting the majority of the tactics used against victims.[19][12] The White Caps also used non-violent means of intimidation to force certain residents from their homes. These included posting signs on doors of blacks' and merchants' homes, as well as cornering a target and verbally threatening them. If a resident or witness to a crime did not abandon their homes after being terrorized, White Caps sometimes murdered them.[20] The victims of these attacks had little support from the legal authorities until 1893, when the threat from whitecapping began to be taken more seriously. But, even if suspects were prosecuted, local officials had difficulty clearing juries of White Cap members or sympathizers. In addition to white bias against black victims, part of the White Cap oath was to never help convict a fellow member.[9] Some members of the White Caps had elite connections to defense attorneys in their state, who aided them in avoiding harsher sentences in court. In the case of Hodges v. United States, the defendants' attorneys were the lieutenant governor and a candidate running for state prosecutor. Defendants were convicted, but sentenced only to one year and a day in prison, along with a $100 fine.[5] The penalty for violating the blood oath of the White Caps was death.[20] In some states, whitecapping societies were interconnected throughout the region. Members could call on members from another county to terrorize witnesses of crimes as a scare tactic.[20]"
Women in America: Indicators of Social and Economic Well-Being,/wiki/Women_in_America:_Indicators_of_Social_and_Economic_Well-Being,"Women in America: Indicators of Social and Economic Well-Being is a report issued in 2011 by the United States Department of Commerce Economics and Statistics Administration and the Executive Office of the President Office of Management and Budget for the White House Council on Women and Girls, during the administration of President Barack Obama.[1] The report, which pulls together data from federal sources to give a ""snapshot"" of the well-being of American women,[2] was released in March in observance of Women's History Month.[3] This was the first such report since The Presidential Report on American Women issued in 1963 by a commission headed by Eleanor Roosevelt under President John F. Kennedy.[1] More than 30 people from about 6 government agencies provided the data and contributed to the report.[4] ""I think it will inform a wide variety of different policy in programs that the federal government will either initiate or continue but it will be evidence-based,"" Valerie Jarrett, a senior advisor to President Obama who is chair of the White House Council on Women and Children, said in a conference phone call announcing the report's publication.[5] The Wall Street Journal summarized the report: ""women have met, and in some cases surpassed, men in educational achievement but still lag in pay and are more likely to be in poverty"".[6]Reuters said, ""More women than men have a high school education, more have university degrees, and more have graduate degrees, but at all levels of education, women earn about 75 percent as much as their male counterparts"".[7] The report has five main sections divided into major points (listed below[8]) each with an accompanying chart. According to the foreword, women have made ""enormous progress"" in education. Young women are now more likely than young men to earn a college or a master's degree. The number of employed women and men has become nearly equal in recent years. In income and employment, women are more likely to be in poverty than men, and women of color are more likely to be in poverty than others. In health, men suffer from heart disease and diabetes more than women do. Women suffer from mobility impairments, arthritis, asthma, depression, and obesity more than men do. In crime, women are less likely to be the target of violent crimes than in the past but they are more likely than men to be the victims of intimate partner violence and stalking.[9]"
