@inproceedings{bender_2021,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610â€“623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@book{chollet_2017  ,
  title     = "Deep Learning with R",
  author    = "FranÃ§ois Chollet and J.J. Allaire",
  year      = 2017,
  publisher = "Manning Publisher",
  address   = "USA"
}

@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{peters_elmo_2019,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}


@article{perez_2023,
  author={PÃ©rez, Juan Manuel and Luque, Franco M. and Zayat, Demian and Kondratzky, MartÃ­n and Moro, AgustÃ­n and Serrati, Pablo Santiago and Zajac, JoaquÃ­n and Miguel, Paula and Debandi, Natalia and Gravano, AgustÃ­n and Cotik, Viviana},
  journal={IEEE Access}, 
  title={Assessing the Impact of Contextual Information in Hate Speech Detection}, 
  year={2023},
  volume={11},
  number={},
  pages={30575-30590},
  keywords={Hate speech;Task analysis;Social networking (online);Fake news;Media;Context awareness;NLP;text classification;hate speech detection;contextual information;Spanish corpus;COVID-19 hate speech},
  doi={10.1109/ACCESS.2023.3258973}}


@article{kozlowski_2019,
author = {Austin C. Kozlowski and Matt Taddy and James A. Evans},
title ={The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings},
journal = {American Sociological Review},
volume = {84},
number = {5},
pages = {905-949},
year = {2019},
doi = {10.1177/0003122419877135},
URL = { https://doi.org/10.1177/0003122419877135},
eprint = { https://doi.org/10.1177/0003122419877135},
abstract = { We argue word embedding models are a useful tool for the study of culture using a historical analysis of shared understandings of social class as an empirical case. Word embeddings represent semantic relations between words as relationships between vectors in a high-dimensional space, specifying a relational model of meaning consistent with contemporary theories of culture. Dimensions induced by word differences (rich â€“ poor) in these spaces correspond to dimensions of cultural meaning, and the projection of words onto these dimensions reflects widely shared associations, which we validate with surveys. Analyzing text from millions of books published over 100 years, we show that the markers of class continuously shifted amidst the economic transformations of the twentieth century, yet the basic cultural dimensions of class remained remarkably stable. The notable exception is education, which became tightly linked to affluence independent of its association with cultivated taste. }
}

@article{kennedy2020constructing,
  title={Constructing interval variables via faceted Rasch measurement and multitask deep learning: a hate speech application},
  author={Kennedy, Chris J and Bacon, Geoff and Sahn, Alexander and von Vacano, Claudia},
  journal={arXiv preprint arXiv:2009.10277},
  year={2020}
}

@article{garg_2018,
author = {Nikhil Garg  and Londa Schiebinger  and Dan Jurafsky  and James Zou },
title = {Word embeddings quantify 100 years of gender and ethnic stereotypes},
journal = {Proceedings of the National Academy of Sciences},
volume = {115},
number = {16},
pages = {E3635-E3644},
year = {2018},
doi = {10.1073/pnas.1720347115},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1720347115},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1720347115},
abstract = {Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shiftsâ€”e.g., the womenâ€™s movement in the 1960s and Asian immigration into the United Statesâ€”and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.}}

@inproceedings{pennington2014glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}

@misc{mikolov2013efficient,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bojanowski2017enriching,
      title={Enriching Word Vectors with Subword Information}, 
      author={Piotr Bojanowski and Edouard Grave and Armand Joulin and Tomas Mikolov},
      year={2017},
      eprint={1607.04606},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{joulin2016bag,
      title={Bag of Tricks for Efficient Text Classification}, 
      author={Armand Joulin and Edouard Grave and Piotr Bojanowski and Tomas Mikolov},
      year={2016},
      eprint={1607.01759},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{mikolov2013distributed,
      title={Distributed Representations of Words and Phrases and their Compositionality}, 
      author={Tomas Mikolov and Ilya Sutskever and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1310.4546},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}